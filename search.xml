<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[KVM(Kernel-based Virtual Machine)源码分析]]></title>
    <url>%2F2019%2F02%2F20%2Fkvm-src-analysis%2F</url>
    <content type="text"><![CDATA[VMCS对于Intel的虚拟化技术(VT)而言，它的软件部分基本体现在VMCS结构中(Virtual Machine Control Block)。主要通过VMCS结构来控制VCPU的运转。 VMCS是个不超过4K的内存块。 VMCS通过下列的指令控制: VMCLEAR: 清空VMCS结构 VMREAD: 读取VMCS数据 VMWRITE: 数据写入VMCS 通过VMPTR指针指向VMCS结构，该指针包含VMCS的物理地址。 VMCS包含的信息可以分为六个部分: Guest state area：虚拟机状态域，保存非根模式的vcpu运行状态。当VM-Exit发生，vcpu的运行状态要写入这个区域，当VM-Entry发生时，cpu会把这个区域保存的信息加载到自身，从而进入非根模式。这个过程是硬件自动完成的。保存是自动的，加载也是自动的，软件只需要修改这个区域的信息就可以控制cpu的运转。 Host state area：宿主机状态域，保存根模式下cpu的运行状态。在vm-exit时需要将状态加载到CPU。大概包含如下寄存器： CR0, CR3, CR4, RSP, RIP (都是64bit的，不支持32位） 段选择器CS, SS, DS, ES, FS, GS, TR，不包含LDTR。 基址部分FS, GS, TR, GDTR 和IDTR 一些MSR: IA32_SYSENTER_CS, IA32_SYSENTER_ESP,IA32_SYSENTER_EIP, IA32_PERF_GLOBAL_CTRL, IA32_PAT, IA32_EFER。 VM-Execution control filelds：包括page fault控制，I/O位图地址，CR3目标控制，异常位图，pin-based运行控制(异步事件)，processor-based运行控制(同步事件)。这个域可以设置哪些指令触发VM-Exit。触发VM-Exit的指令分为无条件指令和有条件指令，这里设置的是有条件指令。（SDM 24.6） VM-entry contorl filelds：包括‘vm-entry控制’，‘vm-entry MSR控制’和‘VM-Entry事件注入’。（SDM 24.8） VM-exit control filelds：包括’VM-Exit控制’，’VM-Exit MSR控制’。(SDM 24.7) VM退出信息：这个域保存VM-Exit退出时的信息，并且描述原因。(SDM 24.9) 有了VMCS结构后，对虚拟机的控制就是读写VMCS结构。后面对VCPU设置中断，检查状态实际上都是在读写VMCS结构。在vmx.c文件给出了intel定义的VMCS结构的内容。struct __packed vmcs12 CPU 虚拟化VCPU创建12345678910111213141516171819virt/kvm/kvm_main.c:static int kvm_dev_ioctl_create_vm(void)&#123; int fd; struct kvm *kvm; kvm = kvm_create_vm(type); if (IS_ERR(kvm)) return PTR_ERR(kvm); r = kvm_coalesced_mmio_init(kvm); r = get_unused_fd_flags(O_CLOEXEC); /*生成kvm-vm控制文件*/ file = anon_inode_getfile(&quot;kvm-vm&quot;, &amp;kvm_vm_fops, kvm, O_RDWR); return fd;&#125; 调用了函数kvm_create_vm，然后是创建一个文件，这个文件的作用是提供对vm的io_ctl控制。 12345678910111213141516171819202122232425262728293031virt/kvm/kvm_main.c:static struct kvm *kvm_create_vm(void)&#123; int r, i; struct kvm *kvm = kvm_arch_create_vm(); /*设置kvm的mm结构为当前进程的mm,然后引用计数为1*/ kvm-&gt;mm = current-&gt;mm; kvm_eventfd_init(kvm); mutex_init(&amp;kvm-&gt;lock); mutex_init(&amp;kvm-&gt;irq_lock); mutex_init(&amp;kvm-&gt;slots_lock); refcount_set(&amp;kvm-&gt;users_count, 1); INIT_LIST_HEAD(&amp;kvm-&gt;devices); INIT_HLIST_HEAD(&amp;kvm-&gt;irq_ack_notifier_list); r = kvm_arch_init_vm(kvm, type); r = hardware_enable_all() for (i = 0; i &lt; KVM_NR_BUSES; i++) &#123; rcu_assign_pointer(kvm-&gt;buses[i], kzalloc(sizeof(struct kvm_io_bus), GFP_KERNEL)); &#125; kvm_init_mmu_notifier(kvm); /*把kvm链表加入总链表*/ list_add(&amp;kvm-&gt;vm_list, &amp;vm_list); return kvm;&#125; 可以看到，这个函数首先是申请一个kvm结构。然后执行初始化工作。初始化第一步是把kvm的mm结构设置为当前进程的mm。我们知道，mm结构反应了整个进程的内存使用情况，也包括进程使用的页目录信息。然后是初始化io bus和eventfd。这两者和设备io有关。最后把kvm加入到一个全局链表头。通过这个链表头，可以遍历所有的vm虚拟机。创建VM之后，就是创建VCPU。 12345678910111213141516171819202122232425virt/kvm/kvm_main.c:static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)&#123; int r; struct kvm_vcpu *vcpu, *v; /*调用相关cpu的vcpu_create 通过arch/x86/x86.c 进入vmx.c*/ vcpu = kvm_arch_vcpu_create(kvm, id); /*调用相关cpu的vcpu_setup*/ r = kvm_arch_vcpu_setup(vcpu); /*判断是否达到最大cpu个数*/ mutex_lock(&amp;kvm-&gt;lock); if (atomic_read(&amp;kvm-&gt;online_vcpus) == KVM_MAX_VCPUS) &#123; r = -EINVAL; goto vcpu_destroy; &#125; kvm-&gt;created_vcpus++; mutex_unlock(&amp;kvm-&gt;lock); /*生成kvm-vcpu控制文件*/ /* Now it&apos;s all set up, let userspace reach it */ kvm_get_kvm(kvm); r = create_vcpu_fd(vcpu);&#125; 从代码可见，分别调用相关cpu提供的vcpu_create和vcpu_setup来完成vcpu创建。 123456789101112131415161718192021222324252627282930313233343536373839static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)&#123; int err; /*申请一个vmx结构*/ struct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL); int cpu; err = kvm_vcpu_init(&amp;vmx-&gt;vcpu, kvm, id); /*申请guest的msrs,host的msrs*/ vmx-&gt;guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL); vmx-&gt;host_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL); /*申请一个vmcs结构*/ err = alloc_loaded_vmcs(&amp;vmx-&gt;vmcs01); cpu = get_cpu(); vmx_vcpu_load(&amp;vmx-&gt;vcpu, cpu); /*设置vcpu为实模式，设置各种寄存器*/ err = vmx_vcpu_setup(vmx); vmx_vcpu_put(&amp;vmx-&gt;vcpu); put_cpu(); /*为中断分配slot，当虚拟机访问中断的时候，会map到vmcs中的相应地址中*/ if (cpu_need_virtualize_apic_accesses(&amp;vmx-&gt;vcpu)) &#123; err = alloc_apic_access_page(kvm); if (err) goto free_vmcs; &#125; if (enable_ept &amp;&amp; !enable_unrestricted_guest) &#123; err = init_rmode_identity_map(kvm); &#125; if (nested) nested_vmx_setup_ctls_msrs(&amp;vmx-&gt;nested.msrs, kvm_vcpu_apicv_active(&amp;vmx-&gt;vcpu)); vmx-&gt;msr_ia32_feature_control_valid_bits = FEATURE_CONTROL_LOCKED;&#125; 首先申请一个vcpu_vmx结构，然后初始化vcpu_vmx包含的mmu，模拟中断芯片等等成员。MSR寄存器是cpu模式寄存器，所以要分别为guest 和host申请页面，这个页面要保存MSR寄存器的信息。然后申请一个vmcs结构。然后调用vmx_vcpu_setup设置vcpu工作在实模式。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394static int vmx_vcpu_setup(struct vcpu_vmx *vmx)&#123; /* Control */ vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx)) vmx-&gt;hv_deadline_tsc = -1; vmcs_write32(CPU_BASED_VM_EXEC_CONTROL, vmx_exec_control(vmx)); if (cpu_has_secondary_exec_ctrls()) &#123; vmx_compute_secondary_exec_control(vmx); vmcs_write32(SECONDARY_VM_EXEC_CONTROL, vmx-&gt;secondary_exec_control); &#125; if (kvm_vcpu_apicv_active(&amp;vmx-&gt;vcpu)) &#123; vmcs_write64(EOI_EXIT_BITMAP0, 0); vmcs_write64(EOI_EXIT_BITMAP1, 0); vmcs_write64(EOI_EXIT_BITMAP2, 0); vmcs_write64(EOI_EXIT_BITMAP3, 0); vmcs_write16(GUEST_INTR_STATUS, 0); vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR); vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&amp;vmx-&gt;pi_desc))); &#125; if (!kvm_pause_in_guest(vmx-&gt;vcpu.kvm)) &#123; vmcs_write32(PLE_GAP, ple_gap); vmx-&gt;ple_window = ple_window; vmx-&gt;ple_window_dirty = true; &#125; vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0); vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0); vmcs_write32(CR3_TARGET_COUNT, 0); /* 22.2.1 */ vmcs_write16(HOST_FS_SELECTOR, 0); /* 22.2.4 */ vmcs_write16(HOST_GS_SELECTOR, 0); /* 22.2.4 */ vmx_set_constant_host_state(vmx); vmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */ vmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */ if (cpu_has_vmx_vmfunc()) vmcs_write64(VM_FUNCTION_CONTROL, 0); vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0); vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0); vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx-&gt;msr_autoload.host.val)) ; vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0); vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx-&gt;msr_autoload.guest.val )); if (vmcs_config.vmentry_ctrl &amp; VM_ENTRY_LOAD_IA32_PAT) vmcs_write64(GUEST_IA32_PAT, vmx-&gt;vcpu.arch.pat); for (i = 0; i &lt; ARRAY_SIZE(vmx_msr_index); ++i) &#123; u32 index = vmx_msr_index[i]; u32 data_low, data_high; int j = vmx-&gt;nmsrs; if (rdmsr_safe(index, &amp;data_low, &amp;data_high) &lt; 0) continue; if (wrmsr_safe(index, data_low, data_high) &lt; 0) continue; vmx-&gt;guest_msrs[j].index = i; vmx-&gt;guest_msrs[j].data = 0; vmx-&gt;guest_msrs[j].mask = -1ull; ++vmx-&gt;nmsrs; &#125; vmx-&gt;arch_capabilities = kvm_get_arch_capabilities(); vm_exit_controls_init(vmx, vmcs_config.vmexit_ctrl); /* 22.2.1, 20.8.1 */ vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl); vmx-&gt;vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS; vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS); set_cr4_guest_host_mask(vmx); if (vmx_xsaves_supported()) vmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP); if (enable_pml) &#123; vmcs_write64(PML_ADDRESS, page_to_phys(vmx-&gt;pml_pg)); vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1); &#125; if (cpu_has_vmx_encls_vmexit()) vmcs_write64(ENCLS_EXITING_BITMAP, -1ull);&#125; 这个函数要写一堆的寄存器和控制信息，信息很多。所以只重点分析其中的几个地方：设置CPU_BASED控制器（VMCS的一部分）；GUEST中断状态寄存器；CR3,CR0 以及各种段选寄存器CS, DS, ES；之后，要保存host的MSR寄存器的值到前面分配的guest_msrs页面; Guest PML地址等等…… VCPU运行推动vcpu运行，启动虚拟机开始运行，主要在vcpu_run函数执行。 123456789101112131415161718192021222324252627282930313233343536373839static int vcpu_run(struct kvm_vcpu *vcpu)&#123; int r; struct kvm *kvm = vcpu-&gt;kvm; for (;;) &#123; /*vcpu进入guest模式*/ if (kvm_vcpu_running(vcpu)) &#123;----&gt; r = vcpu_enter_guest(vcpu); &#125; else &#123; r = vcpu_block(kvm, vcpu); &#125; kvm_clear_request(KVM_REQ_PENDING_TIMER, vcpu); /*检查是否有阻塞的时钟timer*/ if (kvm_cpu_has_pending_timer(vcpu)) kvm_inject_pending_timer_irqs(vcpu); /*检查是否有用户空间的中断注入*/ if (dm_request_for_irq_injection(vcpu) &amp;&amp; kvm_vcpu_ready_for_interrupt_injection(vcpu)) &#123; r = 0; vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN; ++vcpu-&gt;stat.request_irq_exits; break; &#125; kvm_check_async_pf_completion(vcpu); /*是否有阻塞的signal*/ if (signal_pending(current)) &#123; r = -EINTR; vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_INTR; ++vcpu-&gt;stat.signal_exits; break; &#125; /*执行一个调度*/ if (need_resched()) &#123; cond_resched(); &#125; &#125; 这里理解的关键是vcpu_enter_guest进入了Guest，然后一直是vcpu在运行，当退出这个函数的时候，虚拟机已经执行了VM-Exit指令，也就是说，已经退出了虚拟机，进入根模式了。退出之后，要检查退出的原因。如果有时钟中断发生，则插入一个时钟中断，如果是用户空间的中断发生，则退出原因要填写为KVM_EXIT_INTR。注意一点的是，对于导致退出的事件，vcpu_enter_guest函数里面已经处理了一部分，处理的是虚拟机本身运行导致退出的事件。虚拟机一旦退出后，执行vmx_handle_exit。比如虚拟机内部写磁盘io导致退出，就在vcpu_enter_guest里面处理（只是设置了退出的原因为io，并没有真正执行io）。KVM是如何知道退出的原因的？这个就是vmcs结构的作用了，vmcs结构里面有VM-Exit的信息。退出VM之后，如果内核没有完成处理，那么要退出内核到QEMU进程。然后是QEMU进程要处理.后面io处理时，我们再看下QEMU的处理过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117static int vcpu_enter_guest(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)&#123; r = kvm_mmu_reload(vcpu); if (unlikely(r)) &#123; goto cancel_injection; &#125; preempt_disable(); kvm_x86_ops-&gt;prepare_guest_switch(vcpu); /* * Disable IRQs before setting IN_GUEST_MODE. Posted interrupt * IPI are then delayed after guest entry, which ensures that they * result in virtual interrupt delivery. */ local_irq_disable(); vcpu-&gt;mode = IN_GUEST_MODE; /* * This handles the case where a posted interrupt was * notified with kvm_vcpu_kick. */ if (kvm_lapic_enabled(vcpu) &amp;&amp; vcpu-&gt;arch.apicv_active) kvm_x86_ops-&gt;sync_pir_to_irr(vcpu); if (vcpu-&gt;mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) || need_resched() || signal_pending(current)) &#123; vcpu-&gt;mode = OUTSIDE_GUEST_MODE; smp_wmb(); local_irq_enable(); preempt_enable(); vcpu-&gt;srcu_idx = srcu_read_lock(&amp;vcpu-&gt;kvm-&gt;srcu); r = 1; goto cancel_injection; &#125; kvm_load_guest_xcr0(vcpu); trace_kvm_entry(vcpu-&gt;vcpu_id); if (lapic_timer_advance_ns) wait_lapic_expire(vcpu); guest_enter_irqoff(); if (unlikely(vcpu-&gt;arch.switch_db_regs)) &#123; set_debugreg(0, 7); set_debugreg(vcpu-&gt;arch.eff_db[0], 0); set_debugreg(vcpu-&gt;arch.eff_db[1], 1); set_debugreg(vcpu-&gt;arch.eff_db[2], 2); set_debugreg(vcpu-&gt;arch.eff_db[3], 3); set_debugreg(vcpu-&gt;arch.dr6, 6); vcpu-&gt;arch.switch_db_regs &amp;= ~KVM_DEBUGREG_RELOAD; &#125; ----&gt; kvm_x86_ops-&gt;run(vcpu); /* * Do this here before restoring debug registers on the host. And * since we do this before handling the vmexit, a DR access vmexit * can (a) read the correct value of the debug registers, (b) set * KVM_DEBUGREG_WONT_EXIT again. */ if (unlikely(vcpu-&gt;arch.switch_db_regs &amp; KVM_DEBUGREG_WONT_EXIT)) &#123; WARN_ON(vcpu-&gt;guest_debug &amp; KVM_GUESTDBG_USE_HW_BP); kvm_x86_ops-&gt;sync_dirty_debug_regs(vcpu); kvm_update_dr0123(vcpu); kvm_update_dr6(vcpu); kvm_update_dr7(vcpu); vcpu-&gt;arch.switch_db_regs &amp;= ~KVM_DEBUGREG_RELOAD; &#125; /* * If the guest has used debug registers, at least dr7 * will be disabled while returning to the host. * If we don&apos;t have active breakpoints in the host, we don&apos;t * care about the messed up debug address registers. But if * we have some of them active, restore the old state. */ if (hw_breakpoint_active()) hw_breakpoint_restore(); vcpu-&gt;arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc()); vcpu-&gt;mode = OUTSIDE_GUEST_MODE; smp_wmb(); kvm_put_guest_xcr0(vcpu); kvm_before_interrupt(vcpu); kvm_x86_ops-&gt;handle_external_intr(vcpu); kvm_after_interrupt(vcpu); ++vcpu-&gt;stat.exits; guest_exit_irqoff(); local_irq_enable(); preempt_enable(); /* * Profile KVM exit RIPs: */ if (unlikely(prof_on == KVM_PROFILING)) &#123; unsigned long rip = kvm_rip_read(vcpu); profile_hit(KVM_PROFILING, (void *)rip); &#125; if (unlikely(vcpu-&gt;arch.tsc_always_catchup)) kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu); if (vcpu-&gt;arch.apic_attention) kvm_lapic_sync_from_vapic(vcpu); vcpu-&gt;arch.gpa_available = false;----&gt; r = kvm_x86_ops-&gt;handle_exit(vcpu);&#125; 首先要装载mmu，然后注入事件，像中断，异常什么的。然后调用cpu架构相关的run函数(vmx_vcpu_run)，这个函数里面有一堆汇编写的语句，用来进入虚拟机以及指定从虚拟机退出的执行地址。最后调用cpu的handle_exit，用来从vmcs读取退出的信息。将注入中断的函数简化一下。 12345678910111213141516171819202122232425arch/x86/kvm/vmx.c:static void vmx_inject_irq(struct kvm_vcpu *vcpu)&#123; ++vcpu-&gt;stat.irq_injections; if (vmx-&gt;rmode.vm86_active) &#123; int inc_eip = 0; if (vcpu-&gt;arch.interrupt.soft) inc_eip = vcpu-&gt;arch.event_exit_inst_len; if (kvm_inject_realmode_interrupt(vcpu, irq, inc_eip) != EMULATE_ kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu); return; &#125; intr = irq | INTR_INFO_VALID_MASK; if (vcpu-&gt;arch.interrupt.soft) &#123; intr |= INTR_TYPE_SOFT_INTR; vmcs_write32(VM_ENTRY_INSTRUCTION_LEN, vmx-&gt;vcpu.arch.event_exit_inst_len); &#125; else intr |= INTR_TYPE_EXT_INTR; vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr); vmx_clear_hlt(vcpu);&#125; 可以看到，实际上注入中断就是写vmcs里面的VM_ENTRY_INTR_INFO_FIELD这个域。然后在vcpu的run函数里面设置cpu进入非根模式，vcpu会自动检查vmcs结构，然后注入中断，这是硬件自动完成的工作。而处理中断，就是Guest os内核所完成的工作了。 调度KVM只是个内核模块，虚拟机实际上是运行在QEMU的进程上下文中。所以VCPU的调度实际上直接使用了LINUX自身的调度机制。也就是linux自身的进程调度机制。QEMU可以设置每个VCPU都运作在一个线程中。 123456789101112131415static void qemu_kvm_start_vcpu(CPUState *cpu)&#123; char thread_name[VCPU_THREAD_NAME_SIZE]; cpu-&gt;thread = g_malloc0(sizeof(QemuThread)); cpu-&gt;halt_cond = g_malloc0(sizeof(QemuCond)); qemu_cond_init(cpu-&gt;halt_cond); snprintf(thread_name, VCPU_THREAD_NAME_SIZE, &quot;CPU %d/KVM&quot;, cpu-&gt;cpu_index); qemu_thread_create(cpu-&gt;thread, thread_name, qemu_kvm_cpu_thread_fn, cpu, QEMU_THREAD_JOINABLE); while (!cpu-&gt;created) &#123; qemu_cond_wait(&amp;qemu_cpu_cond, &amp;qemu_global_mutex); &#125;&#125; 从Qemu的代码，看到Qemu启动了一个kvm_cpu_thread线程。其主线程函数qemu_kvm_cpu_thread_fn内循环调用kvm_cpu_exec函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667int kvm_cpu_exec(CPUState *env)&#123; do &#123; MemTxAttrs attrs; if (cpu-&gt;kvm_vcpu_dirty) &#123; kvm_arch_put_registers(cpu, KVM_PUT_RUNTIME_STATE); cpu-&gt;kvm_vcpu_dirty = false; &#125; kvm_arch_pre_run(cpu, run); if (cpu-&gt;exit_request) &#123; qemu_cpu_kick_self(); &#125;----&gt; run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0); attrs = kvm_arch_post_run(cpu, run);----&gt; switch (run-&gt;exit_reason) &#123; case KVM_EXIT_IO: DPRINTF(&quot;handle_io\n&quot;); /* Called outside BQL */ kvm_handle_io(run-&gt;io.port, attrs, (uint8_t *)run + run-&gt;io.data_offset, run-&gt;io.direction, run-&gt;io.size, run-&gt;io.count); ret = 0; break; case KVM_EXIT_MMIO: DPRINTF(&quot;handle_mmio\n&quot;); /* Called outside BQL */ address_space_rw(&amp;address_space_memory, run-&gt;mmio.phys_addr, attrs, run-&gt;mmio.data, run-&gt;mmio.len, run-&gt;mmio.is_write); ret = 0; break; case KVM_EXIT_IRQ_WINDOW_OPEN: DPRINTF(&quot;irq_window_open\n&quot;); ret = EXCP_INTERRUPT; break; case KVM_EXIT_SHUTDOWN: DPRINTF(&quot;shutdown\n&quot;); qemu_system_reset_request(); ret = EXCP_INTERRUPT; break; case KVM_EXIT_UNKNOWN: fprintf(stderr, &quot;KVM: unknown exit, hardware reason %&quot; PRIx64 &quot;\n&quot;, (uint64_t)run-&gt;hw.hardware_exit_reason); ret = -1; break; case KVM_EXIT_INTERNAL_ERROR: ret = kvm_handle_internal_error(cpu, run); break; case KVM_EXIT_SYSTEM_EVENT: switch (run-&gt;system_event.type) &#123; case KVM_SYSTEM_EVENT_SHUTDOWN: qemu_system_shutdown_request(); ret = EXCP_INTERRUPT; break; &#125; &#125; while (ret == 0);&#125; 这个函数就是调用了前面分析过的KVM_RUN。回顾一下前面的分析，KVM_RUN就进入了虚拟机，如果从虚拟化退出到这里，那么Qemu要处理退出的事件。这些事件，可能是因为io引起的KVM_EXIT_IO，也可能是内部错误引起的KVM_EXIT_INTERNAL_ERROR。如果事件没有被完善处理，那么要停止虚拟机。 中断如何向vcpu注入中断？是通过向真实CPU模拟注入NMI（非可屏蔽中断）中断来实现。KVM要模拟一个中断控制芯片，这个是通过KVM_CREATE_IRQCHIP来实现的。然后，如果Qemu想注入一个中断，就通过KVM_IRQ_LINE实现。这个所谓中断控制芯片只是在内存中存在的结构，kvm通过软件方式模拟了中断的机制。KVM_CREATE_IRQCHIP实际上调用了kvm_pic_init这个函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354qemu-2.5.1/kvm-all.c:static int kvm_init(MachineState *ms)&#123;...... if (machine_kernel_irqchip_allowed(ms)) &#123;----&gt; kvm_irqchip_create(ms, s); &#125; kvm_state = s; s-&gt;memory_listener.listener.eventfd_add = kvm_mem_ioeventfd_add; s-&gt;memory_listener.listener.eventfd_del = kvm_mem_ioeventfd_del; s-&gt;memory_listener.listener.coalesced_mmio_add = kvm_coalesce_mmio_region; s-&gt;memory_listener.listener.coalesced_mmio_del = kvm_uncoalesce_mmio_region; kvm_memory_listener_register(s, &amp;s-&gt;memory_listener, &amp;address_space_memory, 0); memory_listener_register(&amp;kvm_io_listener, &amp;address_space_io); s-&gt;many_ioeventfds = kvm_check_many_ioeventfds(); cpu_interrupt_handler = kvm_handle_interrupt;&#125;qemu-2.5.1/kvm-all.c:static void kvm_irqchip_create(MachineState *machine, KVMState *s)&#123; int ret; /* First probe and see if there&apos;s a arch-specific hook to create the * in-kernel irqchip for us */ ret = kvm_arch_irqchip_create(s); if (ret == 0) &#123; ret = kvm_vm_ioctl(s, KVM_CREATE_IRQCHIP); &#125; if (ret &lt; 0) &#123; fprintf(stderr, &quot;Create kernel irqchip failed: %s\n&quot;, strerror(-ret)); exit(1); &#125; kvm_kernel_irqchip = true; /* If we have an in-kernel IRQ chip then we must have asynchronous * interrupt delivery (though the reverse is not necessarily true) */ kvm_async_interrupts_allowed = true; kvm_halt_in_kernel_allowed = true; kvm_init_irq_routing(s); s-&gt;gsimap = g_hash_table_new(g_direct_hash, g_direct_equal);&#125; IRQ的初始化就在kvm_init中，通过调用KVM_CREATE_IRQCHIP就搞定。而KVM_IRQ_LINE实际上依旧是通过IOCTL在kernel中完成的。 1234567891011121314151617181920212223242526linux-stable/virt/kvm/irqchip.c:int kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level)&#123; struct kvm_kernel_irq_routing_entry irq_set[KVM_NR_IRQCHIPS]; int ret = -1, i, idx; trace_kvm_set_irq(irq, level, irq_source_id); /* Not possible to detect if the guest uses the PIC or the * IOAPIC. So set the bit in both. The guest will ignore * writes to the unused one. */ idx = srcu_read_lock(&amp;kvm-&gt;irq_srcu); i = kvm_irq_map_gsi(kvm, irq_set, irq); srcu_read_unlock(&amp;kvm-&gt;irq_srcu, idx); while (i--) &#123; int r; r = irq_set[i].set(&amp;irq_set[i], kvm, irq_source_id, level, line_status); if (r &lt; 0) continue; ret = r + ((ret &lt; 0) ? 0 : ret); &#125; return ret;&#125; 从注释中可以看到，因为不能判断Guest使用的是PIC还是APIC，所以为每一个中断路由都设置中断。PIC就是传统的中断控制器8259，x86体系最初使用的中断控制器。后来，又推出了APIC，也就是高级中断控制器。APIC为支持多核架构做了更多的设计。实际上，在kvm模拟中，既有PIC的模拟，也有APIC的模拟。 如果使用PIC的话，这里的这个set函数，其实就是kvm_pic_set_irq。 123456789101112131415int kvm_pic_set_irq(void *opaque, int irq, int level)&#123; struct kvm_pic *s = opaque; if (irq &gt;= 0 &amp;&amp; irq &lt; PIC_NUM_PINS) &#123; ret = pic_set_irq1(&amp;s-&gt;pics[irq &gt;&gt; 3], irq &amp; 7, level); pic_update_irq(s); &#125; irq_level = __kvm_irq_line_state(&amp;s-&gt;irq_states[irq], irq_source_id, level); ret = pic_set_irq1(&amp;s-&gt;pics[irq &gt;&gt; 3], irq &amp; 7, irq_level); pic_update_irq(s); trace_kvm_pic_set_irq(irq &gt;&gt; 3, irq &amp; 7, s-&gt;pics[irq &gt;&gt; 3].elcr, s-&gt;pics[irq &gt;&gt; 3].imr, ret == 0);&#125; 可以看到，前面申请的kvm_pic结构作为参数被引入。然后设置irq到这个结构的pic成员。123456789101112131415static void pic_update_irq(struct kvm_pic *s)&#123; int irq2, irq; irq2 = pic_get_irq(&amp;s-&gt;pics[1]); if (irq2 &gt;= 0) &#123; /* * if irq request by slave pic, signal master PIC */ pic_set_irq1(&amp;s-&gt;pics[0], 2, 1); pic_set_irq1(&amp;s-&gt;pics[0], 2, 0); &#125; irq = pic_get_irq(&amp;s-&gt;pics[0]); pic_irq_request(s-&gt;kvm, irq &gt;= 0);&#125; 此时调用irq_request，就是初始化中断芯片时候绑定的函数pic_irq_request。12345678910111213141516171819202122232425262728void kvm_pic_update_irq(struct kvm_pic *s)&#123; pic_lock(s); pic_update_irq(s); pic_unlock(s);&#125;static void pic_unlock(struct kvm_pic *s) __releases(&amp;s-&gt;lock)&#123; bool wakeup = s-&gt;wakeup_needed; struct kvm_vcpu *vcpu; int i; s-&gt;wakeup_needed = false; spin_unlock(&amp;s-&gt;lock); if (wakeup) &#123; kvm_for_each_vcpu(i, vcpu, s-&gt;kvm) &#123; if (kvm_apic_accept_pic_intr(vcpu)) &#123; kvm_make_request(KVM_REQ_EVENT, vcpu);----&gt; kvm_vcpu_kick(vcpu); return; &#125; &#125; &#125;&#125; 可以看到irq_request之后会调用kvm_vcpu_kick。我们知道，对一个注入的中断来说，需要vcpu立即响应，但是在多核的架构下（smp），目的cpu可能正在运行，所以要提供一种机制停止目的cpu的运行，让它立即处理注入的中断。kvm_vcpu_kick就是给目的cpu发送一个处理器间中断(IPI)，让目的cpu停止运行。 1234567arch/alpha/kernel/smp.c:voidsmp_send_reschedule(int cpu)&#123; send_ipi_message(cpumask_of(cpu), IPI_RESCHEDULE);&#125; 等VM-exit退出后，就接上了前文分析过的部分。VCPU再次进入虚拟机的时候，通过inject_pengding_event检查中断。这个检查的过程就发现了通过KVM_IRQ_LINE注入的中断，然后就是写vmcs结构了注入中断，已经分析过了。 VCPU的内存虚拟化在KMV初始化的时候，要检查是否支持vt里面的EPT扩展技术。如果支持，enable_ept这个变量置为1，然后设置tdp_enabled为1。TDP就是两维页表。为表述方便，给出kvm中下列名字的定义： GPA：guest机物理地址 GVA：guest机虚拟地址 HVA：host机虚拟地址 HPA：host机物理地址 虚拟机页表初始化在vcpu初始化的时候，要调用kvm_init_mmu来设置不同的内存虚拟化方式。123456789101112131415161718void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots)&#123; if (reset_roots) &#123; uint i; vcpu-&gt;arch.mmu-&gt;root_hpa = INVALID_PAGE; for (i = 0; i &lt; KVM_MMU_NUM_PREV_ROOTS; i++) vcpu-&gt;arch.mmu-&gt;prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID; &#125; if (mmu_is_nested(vcpu)) init_kvm_nested_mmu(vcpu); else if (tdp_enabled) init_kvm_tdp_mmu(vcpu); else init_kvm_softmmu(vcpu);&#125; 设置两种方式，一种是支持EPT的方式，另种是soft mmu，也就是影子页表的方式。 123456789101112131415static int init_kvm_softmmu(struct kvm_vcpu *vcpu)&#123; int r; /*无分页模式的设置*/ if (!is_paging(vcpu)) r = nonpaging_init_context(vcpu); else if (is_long_mode(vcpu)) /*64位cpu的设置*/ r = paging64_init_context(vcpu); else if (is_pae(vcpu))/*32位cpu的设置*/ r = paging32E_init_context(vcpu);/*PAE模式cpu的设置*/ else r = paging32_init_context(vcpu); vcpu-&gt;arch.mmu.base_role.glevels = vcpu-&gt;arch.mmu.root_level; return r;&#125; 第一种情况是嵌套虚拟化的，我们暂且不考虑，可以看到在支持EPT的情况下，会调用init_kvm_tdp_mmu函数初始化MMU。在该函数中123456789101112131415161718192021222324252627282930313233343536373839404142 struct kvm_mmu *context = vcpu-&gt;arch.mmu; union kvm_mmu_role new_role = kvm_calc_tdp_mmu_root_page_role(vcpu, false); new_role.base.word &amp;= mmu_base_role_mask.word; if (new_role.as_u64 == context-&gt;mmu_role.as_u64) return; context-&gt;mmu_role.as_u64 = new_role.as_u64;---&gt; context-&gt;page_fault = tdp_page_fault; context-&gt;sync_page = nonpaging_sync_page; context-&gt;invlpg = nonpaging_invlpg; context-&gt;update_pte = nonpaging_update_pte; context-&gt;shadow_root_level = kvm_x86_ops-&gt;get_tdp_level(vcpu); context-&gt;direct_map = true; context-&gt;set_cr3 = kvm_x86_ops-&gt;set_tdp_cr3; context-&gt;get_cr3 = get_cr3; context-&gt;get_pdptr = kvm_pdptr_read; context-&gt;inject_page_fault = kvm_inject_page_fault; if (!is_paging(vcpu)) &#123; context-&gt;nx = false; context-&gt;gva_to_gpa = nonpaging_gva_to_gpa; context-&gt;root_level = 0; &#125; else if (is_long_mode(vcpu)) &#123; context-&gt;nx = is_nx(vcpu); context-&gt;root_level = is_la57_mode(vcpu) ? PT64_ROOT_5LEVEL : PT64_ROOT_4LEVEL; reset_rsvds_bits_mask(vcpu, context); context-&gt;gva_to_gpa = paging64_gva_to_gpa; &#125; else if (is_pae(vcpu)) &#123; context-&gt;nx = is_nx(vcpu); context-&gt;root_level = PT32E_ROOT_LEVEL; reset_rsvds_bits_mask(vcpu, context); context-&gt;gva_to_gpa = paging64_gva_to_gpa; &#125; else &#123; context-&gt;nx = false; context-&gt;root_level = PT32_ROOT_LEVEL; reset_rsvds_bits_mask(vcpu, context); context-&gt;gva_to_gpa = paging32_gva_to_gpa; &#125; vcpu-&gt;arch.walk_mmu.pagefault被初始化成tdp_page_fault。所以我们的正式分析从tdp_page_fault函数开始。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code, bool prefault)&#123; kvm_pfn_t pfn; int r; int level; bool force_pt_level; /* 物理地址右移12位得到物理页框号(相对于虚拟机而言)*/ gfn_t gfn = gpa &gt;&gt; PAGE_SHIFT; unsigned long mmu_seq; int write = error_code &amp; PFERR_WRITE_MASK; bool map_writable; if (page_fault_handle_page_track(vcpu, error_code, gfn)) return RET_PF_EMULATE;----&gt; r = mmu_topup_memory_caches(vcpu); if (r) return r; force_pt_level = !check_hugepage_cache_consistency(vcpu, gfn, PT_DIRECTORY_LEVEL);----&gt; level = mapping_level(vcpu, gfn, &amp;force_pt_level); if (likely(!force_pt_level)) &#123; if (level &gt; PT_DIRECTORY_LEVEL &amp;&amp; !check_hugepage_cache_consistency(vcpu, gfn, level)) level = PT_DIRECTORY_LEVEL; gfn &amp;= ~(KVM_PAGES_PER_HPAGE(level) - 1); &#125;----&gt; if (fast_page_fault(vcpu, gpa, level, error_code)) return RET_PF_RETRY; mmu_seq = vcpu-&gt;kvm-&gt;mmu_notifier_seq; smp_rmb();----&gt; if (try_async_pf(vcpu, prefault, gfn, gpa, &amp;pfn, write, &amp;map_writable)) return RET_PF_RETRY; if (handle_abnormal_pfn(vcpu, 0, gfn, pfn, ACC_ALL, &amp;r)) return r; spin_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock); if (mmu_notifier_retry(vcpu-&gt;kvm, mmu_seq)) goto out_unlock; if (make_mmu_pages_available(vcpu) &lt; 0) goto out_unlock; if (likely(!force_pt_level)) transparent_hugepage_adjust(vcpu, &amp;gfn, &amp;pfn, &amp;level); r = __direct_map(vcpu, write, map_writable, level, gfn, pfn, prefault); spin_unlock(&amp;vcpu-&gt;kvm-&gt;mmu_lock); 该函调用mmu_topup_memory_caches函数进行缓存池的分配，解释是为了避免在运行时分配空间失败，这里提前分配浩足额的空间，便于运行时使用。然后调用mapping_level函数判断当前gfn对应的slot是否可用。为什么要进行这样的判断呢？在if内部可以看到是获取level，如果当前GPN对应的slot可用，我们就可以获取分配slot的pagesize，然后得到最低级的level，比如如果是2M的页，那么level就为2，为4K的页，level就为1.接着调用了fast_page_fault尝试快速处理violation，只有当GFN对应的物理页存在且violation是由读写操作引起的，才可以使用快速处理。 假设这里不能快速处理，那么到后面就调用try_async_pf函数根据GFN获取对应的PFN，这个过程具体来说需要首先获取GFN对应的slot，转化成HVA，接着就是正常的HOST地址翻译的过程了，如果HVA对应的地址并不在内存中，还需要HOST自己处理缺页中断。 接着调用transparent_hugepage_adjust对level和gfn、pfn做出调整。紧着着就调用了__direct_map函数，该函数是构建页表的核心函数： 123456789101112131415161718192021222324252627282930313233343536static int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write, int map_writable, int level, gfn_t gfn, pfn_t pfn, bool prefault)&#123; struct kvm_shadow_walk_iterator iterator; struct kvm_mmu_page *sp; int emulate = 0; gfn_t pseudo_gfn; for_each_shadow_entry(vcpu, (u64)gfn &lt;&lt; PAGE_SHIFT, iterator) &#123; /*如果需要映射的level正是iterator.level，那么*/ if (iterator.level == level) &#123; mmu_set_spte(vcpu, iterator.sptep, ACC_ALL, write, &amp;emulate, level, gfn, pfn, prefault, map_writable); direct_pte_prefetch(vcpu, iterator.sptep); ++vcpu-&gt;stat.pf_fixed; break; &#125; /*判断当前entry指向的页表是否存在，不存在的话需要建立*/ if (!is_shadow_present_pte(*iterator.sptep)) &#123; /*iterator.addr是客户物理地址的物理页帧*/ u64 base_addr = iterator.addr; /*确保对应层级的偏移部分为0，如level=1，则baseaddr的低12位就清零*/ base_addr &amp;= PT64_LVL_ADDR_MASK(iterator.level);// /*得到物理页框号*/ pseudo_gfn = base_addr &gt;&gt; PAGE_SHIFT; sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr, iterator.level - 1, 1, ACC_ALL, iterator.sptep); /*设置页表项的sptep指针指向sp*/ link_shadow_page(iterator.sptep, sp); &#125; &#125; return emulate;&#125; 首先进入的便是for_each_shadow_entry，用于根据GFN遍历EPT页表的对应项，这点后面会详细解释。循环中首先判断entry的level和请求的level是否相等，相等说明该entry处引起的violation，即该entry对应的下级页或者页表不在内存中，或者直接为NULL。 如果level不相等，就进入后面的if判断，这是判断该entry对应的下一级页是否存在，如果不存在需要重新构建，存在就直接向后遍历，即对比二级页表中的entry。整个处理流程就是这样，根据GPA组逐层查找EPT，最终level相等的时候，就根据最后一层的索引定位一个PTE，该PTE应该指向的就是GFN对应的PFN，那么这时候set spite就可以了。最好的情况就是最后一级页表中的entry指向的物理页被换出外磁盘，这样只需要处理一次EPT violation，而如果在初始全部为空的状态下访问，每一级的页表都需要重新构建，则需要处理四次EPTviolation，发生4次VM-exit。 构建页表的过程即在level相等之前，发现需要的某一级的页表项为NULL，就调用kvm_mmu_get_page获取一个page，然后调用link_shadow_page设置页表项指向page， 看下kvm_mmu_get_page函数:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gaddr, unsigned level, int direct, unsigned access, u64 *parent_pte)&#123; union kvm_mmu_page_role role; unsigned quadrant; struct kvm_mmu_page *sp; bool need_sync = false; role = vcpu-&gt;arch.mmu.base_role; role.level = level; role.direct = direct; if (role.direct) role.cr4_pae = 0; role.access = access; /*quadrant 对应页表项的索引，来自于GPA*/ if (!vcpu-&gt;arch.mmu.direct_map &amp;&amp; vcpu-&gt;arch.mmu.root_level &lt;= PT32_ROOT_LEVEL) &#123; quadrant = gaddr &gt;&gt; (PAGE_SHIFT + (PT64_PT_BITS * level)); quadrant &amp;= (1 &lt;&lt; ((PT32_PT_BITS - PT64_PT_BITS) * level)) - 1; role.quadrant = quadrant; &#125; /*根据gfn遍历KVM维护的mmu_page_hash哈希链表*/ for_each_gfn_sp(vcpu-&gt;kvm, sp, gfn) &#123; /**/ if (is_obsolete_sp(vcpu-&gt;kvm, sp)) continue; if (!need_sync &amp;&amp; sp-&gt;unsync) need_sync = true; if (sp-&gt;role.word != role.word) continue; if (sp-&gt;unsync &amp;&amp; kvm_sync_page_transient(vcpu, sp)) break; /*设置sp-&gt;parent_pte=parent_pte*/ mmu_page_add_parent_pte(vcpu, sp, parent_pte); if (sp-&gt;unsync_children) &#123; kvm_make_request(KVM_REQ_MMU_SYNC, vcpu); kvm_mmu_mark_parents_unsync(sp); &#125; else if (sp-&gt;unsync) kvm_mmu_mark_parents_unsync(sp); __clear_sp_write_flooding_count(sp); trace_kvm_mmu_get_page(sp, false); return sp; &#125; /*如果根据页框号没有遍历到合适的page，就需要重新创建一个页*/ ++vcpu-&gt;kvm-&gt;stat.mmu_cache_miss; sp = kvm_mmu_alloc_page(vcpu, parent_pte, direct); if (!sp) return sp; /*设置其对应的客户机物理页框号*/ sp-&gt;gfn = gfn; sp-&gt;role = role; /*把该也作为一个节点加入到哈希表相应的链表汇总*/ hlist_add_head(&amp;sp-&gt;hash_link, &amp;vcpu-&gt;kvm-&gt;arch.mmu_page_hash[kvm_page_table_hashfn(gfn)]); if (!direct) &#123; if (rmap_write_protect(vcpu-&gt;kvm, gfn)) kvm_flush_remote_tlbs(vcpu-&gt;kvm); if (level &gt; PT_PAGE_TABLE_LEVEL &amp;&amp; need_sync) kvm_sync_pages(vcpu, gfn); account_shadowed(vcpu-&gt;kvm, gfn); &#125; sp-&gt;mmu_valid_gen = vcpu-&gt;kvm-&gt;arch.mmu_valid_gen; /*暂时对所有表项清零*/ init_shadow_page_table(sp); trace_kvm_mmu_get_page(sp, true); return sp;&#125; 具体的细节方面后面单独讲述，比如kvm_mmu_page_role结构，目前我们只需要知道一个kvm_mmu_page对应于一个kvm_mmu_page_role，kvm_mmu_page_role记录对应page的各种属性。下面for_each_gfn_sp是一个遍历链表的宏定义，KVM为了根据GFN查找对应的kvm_mmu_page，用一个HASH数组记录所有的kvm_mmu_page，每一个表项都是一个链表头，即根据GFN获取到的HASH值相同的，位于一个链表中。这也是HASH表处理冲突常见方法。 如果在对应链表中找到一个合适的页（怎么算是合适暂且不清楚），就直接利用该页，否则需要调用kvm_mmu_alloc_page函数重新申请一个页，主要是申请一个kvm_mmu_page结构和一个存放表项的page，这就用到了之前我们说过的三种缓存，不错这里只用到了两个，分别是mmu_page_header_cache和mmu_page_cache。这样分配好后，把对应的kvm_mmu_page作为一个节点加入到全局的HASH链表中，然后对数组项清零，最后返回sp. 我们知道，linux为不同的cpu提供不同的页表层级。64位cpu使用了四级页表。这里指定页表是两级，也就是PT32_ROOT_LEVEL，同时设定页表根地址为无效。此时页表尚未分配。何时真正分配vcpu的页表？是在vcpu_enter_guest的开始位置，通过调用kvm_mmu_reload实现。123456static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)&#123; /*页表根地址不是无效的，则退出，不用分配。*/ if (likely(vcpu-&gt;arch.mmu.root_hpa != INVALID_PAGE)) return 0; return kvm_mmu_load(vcpu);&#125; 首先检查页表根地址是否无效，如果无效，则调用kvm_mmu_load。 1234567891011int kvm_mmu_load(struct kvm_vcpu *vcpu)&#123; int r; r = mmu_alloc_roots(vcpu); /*同步页表*/ mmu_sync_roots(vcpu); /* set_cr3() should ensure TLB has been flushed */ kvm_x86_ops-&gt;set_cr3(vcpu, vcpu-&gt;arch.mmu.root_hpa); kvm_x86_ops-&gt;tlb_flush(vcpu, true);&#125; mmu_alloc_roots这个函数要申请内存，作为根页表使用，同时root_hpa指向根页表的物理地址。然后可以看到，vcpu中cr3寄存器的地址要指向这个根页表的物理地址。 虚拟机物理地址我们已经分析过，kvm的虚拟机实际上运行在Qemu的进程上下文中。于是，虚拟机的物理内存实际上是Qemu进程的虚拟地址。Kvm要把虚拟机的物理内存分成几个slot。这是因为，对计算机系统来说，物理地址是不连续的，除了bios和显存要编入内存地址，IO设备的内存也可能映射到内存，所以内存实际上是分为一段段的。Qemu通过KVM_SET_USER_MEMORY_REGION来为虚拟机设置内存。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576virt/kvm/kvm_main.c:int __kvm_set_memory_region(struct kvm *kvm, struct kvm_userspace_memory_region *mem, int user_alloc)&#123; /*找到现在的memslot*/ slot = id_to_memslot(__kvm_memslots(kvm, as_id), id); base_gfn = mem-&gt;guest_phys_addr &gt;&gt; PAGE_SHIFT; npages = mem-&gt;memory_size &gt;&gt; PAGE_SHIFT; new = old = *slot; /*new是新的slots,old保持老的数值不变*/ new.id = id; new.base_gfn = base_gfn; new.npages = npages; new.flags = mem-&gt;flags; /*用户已经分配了内存，slot的用户空间地址就等于用户分配的地址*/ if (change == KVM_MR_CREATE) &#123;----&gt; new.userspace_addr = mem-&gt;userspace_addr; if (kvm_arch_create_memslot(kvm, &amp;new, npages)) goto out_free; &#125; /* Allocate page dirty bitmap if needed */ if ((new.flags &amp; KVM_MEM_LOG_DIRTY_PAGES) &amp;&amp; !new.dirty_bitmap) &#123; if (kvm_create_dirty_bitmap(&amp;new) &lt; 0) goto out_free; &#125; slots = kvzalloc(sizeof(struct kvm_memslots), GFP_KERNEL); if (!slots) goto out_free; memcpy(slots, __kvm_memslots(kvm, as_id), sizeof(struct kvm_memslots)); /*内存地址页的检查和内存overlap的检查*/ if ((change == KVM_MR_DELETE) || (change == KVM_MR_MOVE)) &#123; slot = id_to_memslot(slots, id); slot-&gt;flags |= KVM_MEMSLOT_INVALID; old_memslots = install_new_memslots(kvm, as_id, slots); /* From this point no new shadow pages pointing to a deleted, * or moved, memslot will be created. * * validation of sp-&gt;gfn happens in: * - gfn_to_hva (kvm_read_guest, gfn_to_pfn) * - kvm_is_visible_gfn (mmu_check_roots) */ kvm_arch_flush_shadow_memslot(kvm, slot); /* * We can re-use the old_memslots from above, the only difference * from the currently installed memslots is the invalid flag. Thi s * will get overwritten by update_memslots anyway. */ slots = old_memslots; &#125; r = kvm_arch_prepare_memory_region(kvm, &amp;new, mem, change); if (r) goto out_slots; /* actual memory is freed via old in kvm_free_memslot below */ if (change == KVM_MR_DELETE) &#123; new.dirty_bitmap = NULL; memset(&amp;new.arch, 0, sizeof(new.arch)); &#125; update_memslots(slots, &amp;new, change); old_memslots = install_new_memslots(kvm, as_id, slots); kvm_arch_commit_memory_region(kvm, mem, &amp;old, &amp;new, change);&#125; 就是创建一个新的memslot，代替原来的memslot。一个内存slot，最重要部分是指定了vm的物理地址，使用函数gfn_to_hva可以把gfn转换为hva。可见，一个memslot就是建立了GPA到HVA的映射关系。 内存虚拟化过程这里，有必要描述一下内存虚拟化的过程：VM要访问GVA 地址x，那么首先查询VM的页表得到PTE（页表项），通过PTE将GVA x映射到物理地址GPA y.GPA y此时不存在，发生页缺失。KVM接管。从memslot，可以知道GPA对应的其实是HVA x’，然后从HVA x’，可以查找得到HPA y’，然后将HPA y’这个映射写入到页表。VM再次存取GVA x，这是从页表项已经可以查到HPA y’了，内存可正常访问。 首先，从page_fault处理开始。从前文的分析，知道VM里面的异常产生VM-Exit，然后由各自cpu提供的处理函数处理。对intel的vt技术，就是handle_exception这个函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162static int handle_exception(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)&#123; u32 intr_info, ex_no, error_code; /*读vmcs，获得VM-exit的信息*/ intr_info = vmx-&gt;exit_intr_info; /*发现是page_fault引起*/ if (is_page_fault(intr_info)) &#123; cr2 = vmcs_readl(EXIT_QUALIFICATION); /* EPT won&apos;t cause page fault directly */ /*如果支持EPT，不会因为page_fault退出，所以是bug*/ WARN_ON_ONCE(!vcpu-&gt;arch.apf.host_apf_reason &amp;&amp; enable_ept); return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0); &#125; ex_no = intr_info &amp; INTR_INFO_VECTOR_MASK; if (vmx-&gt;rmode.vm86_active &amp;&amp; rmode_exception(vcpu, ex_no)) return handle_rmode_exception(vcpu, ex_no, error_code); switch (ex_no) &#123; case AC_VECTOR: kvm_queue_exception_e(vcpu, AC_VECTOR, error_code); return 1; case DB_VECTOR: dr6 = vmcs_readl(EXIT_QUALIFICATION); if (!(vcpu-&gt;guest_debug &amp; (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) &#123; vcpu-&gt;arch.dr6 &amp;= ~15; vcpu-&gt;arch.dr6 |= dr6 | DR6_RTM; if (is_icebp(intr_info)) skip_emulated_instruction(vcpu); kvm_queue_exception(vcpu, DB_VECTOR); return 1; &#125; kvm_run-&gt;debug.arch.dr6 = dr6 | DR6_FIXED_1; kvm_run-&gt;debug.arch.dr7 = vmcs_readl(GUEST_DR7); /* fall through */ case BP_VECTOR: /* * Update instruction length as we may reinject #BP from * user space while in guest debugging mode. Reading it for * #DB as well causes no harm, it is not used in that case. */ vmx-&gt;vcpu.arch.event_exit_inst_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN); kvm_run-&gt;exit_reason = KVM_EXIT_DEBUG; rip = kvm_rip_read(vcpu); kvm_run-&gt;debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip; kvm_run-&gt;debug.arch.exception = ex_no; break; default: kvm_run-&gt;exit_reason = KVM_EXIT_EXCEPTION; kvm_run-&gt;ex.exception = ex_no; kvm_run-&gt;ex.error_code = error_code; break; &#125; return 0;&#125; 从这个函数，可以看到对vmcs的使用。通过读vmcs的域，可以获得退出vm的原因。如果是page_fault引起，则调用kvm_mmu_page_fault去处理。 12345678910111213141516int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u32 error_code)&#123; int r; enum emulation_result er; /*调用mmu的page_fault*/ r = vcpu-&gt;arch.mmu.page_fault(vcpu, cr2, error_code); if (r &lt; 0) goto out; if (!r) &#123; r = 1; goto out; &#125; /*模拟指令*/ er = emulate_instruction(vcpu, vcpu-&gt;run, cr2, error_code, 0); ..................................&#125; 这里调用了MMU的page_fault处理函数。这个函数就是前面初始化时候设置的paging32_page_fault。也就是通过FNAME宏展开的FNAME(page_fault)。1234567891011121314151617181920212223242526272829303132static int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code)&#123; /*查guest页表，物理地址是否存在 */ r = FNAME(walk_addr)(&amp;walker, vcpu, addr, write_fault, user_fault, fetch_fault); /*页还没映射，交Guest OS处理 */ if (!r) &#123; pgprintk(&quot;%s: guest page fault\n&quot;, __func__); inject_page_fault(vcpu, addr, walker.error_code); vcpu-&gt;arch.last_pt_write_count = 0; /* reset fork detector */ return 0; &#125; if (walker.level &gt;= PT_DIRECTORY_LEVEL) &#123; level = min(walker.level, mapping_level(vcpu, walker.gfn)); walker.gfn = walker.gfn &amp; ~(KVM_PAGES_PER_HPAGE(level) - 1); &#125; /*通过gfn找pfn*/ pfn = gfn_to_pfn(vcpu-&gt;kvm, walker.gfn); /* mmio ,如果是mmio，是io访问，不是内存，返回*/ if (is_error_pfn(pfn)) &#123; pgprintk(&quot;gfn %lx is mmio\n&quot;, walker.gfn); kvm_release_pfn_clean(pfn); return 1; &#125; /*写入HVA到页表*/ sptep = FNAME(fetch)(vcpu, addr, &amp;walker, user_fault, write_fault, level, &amp;write_pt, pfn);&#125; 对照前面的分析，比较容易理解这个函数了。首先是查guest机的页表，如果从GVA到GPA的映射都没建立，那么返回，让Guest OS做这个工作。然后，如果映射已经建立，GPA存在，那么从Guest的页面号，查找Host的页面号。如何执行这个查找？从memslot可以知道user space首地址，就可以把物理地址GPA转为HVA，通过HVA就可以查到HPA，然后找到所在页的页号。最后，写HVA到页表里面。页表在那里？回顾一下前面kvm_mmu_load的过程，页表是host申请的，host知道页表的真实物理地址。通过页表一层层的搜索，就可以找到要写入的页表项。已知虚拟地址，一级级查找页表找到要写的页表项位置，是经常用的一种操作，这个函数可以认真分析一下实现过程。 IO虚拟化IO虚拟化有两种方案，一种是半虚拟化方案，一种是全虚拟化方案。全虚拟化方案不需要修改Guest的代码，那么Guest里面的io操作最终都变成io指令。在前面的分析中，其实已经涉及了io虚拟化的流程。在VM-exit的时候，前文分析过page fault导致的退出。那么io指令，同样会导致VM-exit退出，然后kvm会把io交给Qemu进程处理。而半虚拟化方案，基本都是把io变成了消息处理，从guest机器发消息出来，然后由host机器处理。此时，在guest机器的驱动都被接管，已经不能被称为驱动（因为已经不再处理io指令，不和具体设备打交道），称为消息代理更合适。 Vmm对io的处理当guest因为执行io执行退出后，由handle_io函数处理.1234567891011static int handle_io(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)&#123; ++vcpu-&gt;stat.io_exits; exit_qualification = vmcs_readl(EXIT_QUALIFICATION); ................................... size = (exit_qualification &amp; 7) + 1; in = (exit_qualification &amp; 8) != 0; port = exit_qualification &gt;&gt; 16; ................................................. return kvm_emulate_pio(vcpu, kvm_run, in, size, port);&#125; 要从vmcs读退出的信息，然后调用kvm_emulate_pio处理。1234567891011121314151617181920212223int kvm_emulate_pio(struct kvm_vcpu *vcpu, struct kvm_run *run, int in, int size, unsigned port)&#123; unsigned long val; /*要赋值退出的种种参数*/ vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_IO; vcpu-&gt;run-&gt;io.direction = in ? KVM_EXIT_IO_IN : KVM_EXIT_IO_OUT; vcpu-&gt;run-&gt;io.size = vcpu-&gt;arch.pio.size = size; vcpu-&gt;run-&gt;io.data_offset = KVM_PIO_PAGE_OFFSET * PAGE_SIZE; vcpu-&gt;run-&gt;io.count = vcpu-&gt;arch.pio.count = vcpu-&gt;arch.pio.cur_count = 1; vcpu-&gt;run-&gt;io.port = vcpu-&gt;arch.pio.port = port; vcpu-&gt;arch.pio.in = in; vcpu-&gt;arch.pio.string = 0; vcpu-&gt;arch.pio.down = 0; vcpu-&gt;arch.pio.rep = 0; ................................. /*内核能不能处理？*/ if (!kernel_pio(vcpu, vcpu-&gt;arch.pio_data)) &#123; complete_pio(vcpu); return 1; &#125; return 0;&#125; 这里要为io处理赋值各种参数，然后看内核能否处理这个io，如果内核能处理，就不用Qemu进程处理，否则退出内核态，返回用户态。从前文的分析中，我们知道返回是到Qemu的线程上下文中。实际上就是kvm_handle_io这个函数里面。 虚拟化io流程用户态的Qemu如何处理io指令？首先，每种设备都需要注册自己的io指令处理函数到Qemu。这是通过register_ioport_write和register_ioport_read是实现的。 1234567891011121314int register_ioport_read(pio_addr_t start, int length, int size, IOPortReadFunc *func, void *opaque)&#123; int i, bsize; /*把处理函数写入ioport_read_table这个全局数据*/ for(i = start; i &lt; start + length; i += size) &#123; ioport_read_table[bsize][i] = func; if (ioport_opaque[i] != NULL &amp;&amp; ioport_opaque[i] != opaque) hw_error(&quot;register_ioport_read: invalid opaque for address 0x%x&quot;, i); ioport_opaque[i] = opaque; &#125; return 0;&#125; 通过这个函数，实际上把io指令处理函数登记到一个全局的数组。每种支持的设备都登记在这个数组中。再分析kvm_handle_io的流程。 1234567891011121314static void kvm_handle_io(uint16_t port, void *data, int direction, int size, uint32_t count)&#123; ............................. for (i = 0; i &lt; count; i++) &#123; if (direction == KVM_EXIT_IO_IN) &#123; switch (size) &#123; case 1: stb_p(ptr, cpu_inb(port)); break; &#125; ptr += size; &#125;&#125; 对于退出原因是KVM_EXIT_IO_IN的情况，调用cpu_inb处理。Cpu_inb是个封装函数，它的作用就是调用ioport_read.12345678910111213static uint32_t ioport_read(int index, uint32_t address)&#123; static IOPortReadFunc * const default_func[3] = &#123; default_ioport_readb, default_ioport_readw, default_ioport_readl &#125;; /*从全局数组读入处理函数*/ IOPortReadFunc *func = ioport_read_table[index][address]; if (!func) func = default_func[index]; return func(ioport_opaque[address], address);&#125; 这里代码很清晰，就是从登记io指令函数的数组中读出处理函数，然后调用每种设备所登记的指令处理函数处理，完成io。各种设备都有自己的处理函数，所以Qemu需要支持各种不同的设备，Qemu的大部分代码都是各种各样设备的驱动代码（注意这里驱动的意义和传统驱动程序的含义有所不同）。 设备注册和设备模拟QEMU设备注册可以移步qemu-qom详解 虚拟化概述脑图脑图有待进一步完善 # 虚拟化概述 ## CPU虚拟化 ### 指令的模拟 #### 陷入（利用处理器的保护机制，中断和异常） 1，基于处理器保护机制出发的异常 2，虚拟机主动触发的异常 3，异步zhognduan ##### 虚拟处理器 ##### 虚拟寄存器 ##### 上下文 ### 中断和异常的虚拟化 ### 对称对处理器技术的虚拟化（SMP） #### VMM选择第一个虚拟处理器，BSP #### 其他虚拟处理器，AP ## Memory虚拟化 ### 物理地址从0开始 ### 内存地址连续 ## I/O虚拟化 ### 设备发现 #### 总线类型的设备 ##### 总线类型不可枚举 ###### ISA设备 ###### PS/2键盘、鼠标、RTC ###### 传统IDE控制器 ##### 总线类型可枚举、资源可配置 ###### PCI #### 完全模拟的设备 ##### Frontend / backend 模型 ### 访问截获 #### I/O端口的访问 ##### I/O位图来决定 #### MMIO访问 ##### 页表项设置为无效 ### 设备模拟]]></content>
      <categories>
        <category>kvm</category>
      </categories>
      <tags>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英特尔®64和IA-32架构软件开发人员手册(Intel SDM)]]></title>
    <url>%2F2019%2F01%2F29%2FIntel-SDM%2F</url>
    <content type="text"><![CDATA[23.1 概述本章介绍虚拟机体系结构的基础知识和虚拟机扩展的概述(VMX),支持多个软件环境的处理器硬件虚拟化。关于VMX指令的信息参考英特尔®64和IA-32架构软件开发人员手册中的第2B卷。其他关于VMX和系统编程参考SDM 第3B卷 23.2虚拟机器结构虚拟机扩展为IA-32处理器上的虚拟机定义了处理器级支持。两个重要支持的软件类别： 虚拟机监视器(VMM)VMM充当主机，可以完全控制处理器和其他处理器平台硬件。 VMM使用虚拟的抽象来呈现客户软件(参见下一段)处理器并允许它直接在逻辑处理器上执行。 VMM能够保留选择性控制处理器资源，物理内存，中断管理和I / O。 Guest- 每个虚拟机(VM)是一个支持堆栈组成的客户软件环境,操作系统(OS)和应用程序软件。每个都独立于其他虚拟机运行在物理的处理器上，内存，存储器，图形和I / O的相同接口上使用平台。软件堆栈就像在没有VMM的平台上运行一样。软件执行中虚拟机必须以降低的权限运行，以便VMM可以保留对平台资源的控制。 23.3 VMX操作简介虚拟化的处理器支持由称为VMX操作的处理器操作形式提供。有两种VMX操作：VMX root和VMX non-root操作。通常，VMM运行在VMX root模式下，同时将guest 软件运行在non-root模式下。VMX root操作和VMX non-root操作的转换称为VMX转换。有两种VMX转换: 从VMX root过渡到VMX non-root模式称为VM entry； 从VMX not-root操作到VMX root的转换成为VM exit。VMX root操作中的处理器行为与VMX操作之外的处理器行为非常相似。主要区别是一组新指令(VMX指令)（见第23.8节）。VMX non-root操作中的处理器行为受到限制和修改，以便于虚拟化。那些指令（包括新的VMCALL指令）和事件会导致VM EXIT的发生，而不仅仅是他们原来的操作。由于这些VM Exit取代了普通行为，因此VMX non-root操作中的软件功能是有限制的。正是这种限制允许VMM保持对处理器资源的控制。并不存在用于通知guest“是否处于VMX non-root”的寄存器。这一事实可以防止guest软件察觉它正在虚拟机中运行。因为VMX操作对(CPL)Level 0做了限制，guest的软件可以在最初设计的权限级别运行。这样就可以简化VMM的开发。 23.4 VMM软件的生命周期图23-1说明了VMM及其客户软件的生命周期以及它们之间的交互。以下项目总结了生命周期： 软件通过执行VMXON指令进入VMX操作 使用VM entry，VMM可以执行guest，一次一个。 VMM通过使用VMLAUNCH和VMRESUME指令干预虚拟机。通过VM EXIT，VMM重新获得控制权。 VM移交控制权到VMM指定的入口点。VMM可以采取适当的措施使得VM exit发生，然后可以使用VM entry返回到虚拟机。 最终，VMM可以通过执行VMXOFF自行决定关闭并离开VMX操作。 23.5虚拟机控制结构VMX non-root的操作以及VMX转换由名为“虚拟机控制”(VMCS)的数据结构控制。通过VMCS指针(每个逻辑CPU一个)来管理对VMCS的访问。VMCS指针的值是64位地址。读取和写入VMCS指针使用指令VMPTRST和VMPTRLD。并且VMM使用VMREAD，VMWRITE和VMCLEAR指令来配置VMCS。VMM可以为其支持的每个虚拟机使用不同的VMCS。对于具有多个的虚拟机在逻辑处理器（虚拟处理器）中，VMM可以为每个虚拟处理器使用不同的VMCS。 23.6 发现对VMX的支持在系统软件进入VMX操作之前，它必须检查处理器中是否存在VMX支持。系统软件可以使用CPUID确定处理器是否支持VMX操作。如果CPUID.1：ECX.VMX [bit 5] = 1那么当前平台支持VMX操作。请参见第3章“指令集参考，A-L”英特尔®64和IA-32架构软件开发人员手册，第2A卷。VMX体系结构旨在实现可扩展性，以便VMX操作中的未来处理器可以支持VMX体系结构的第一代实现中不存在的其他功能。使用一组VMX功能MSR向软件报告可扩展VMX功能的可用性（参见附录A，“VMX功能”)。 23.7启用和进入VMX操作在系统软件进入VMX操作之前，它通过设置CR4.VMXE [bit 13] = 1 VMX操作来启用VMX。然后通过执行VMXON指令进入。当’CR4.VMXE = 0’时，如果执行指令VMXON会导致无效操作码异常（#UD)；一旦执行过VMX操作，就无法清除CR4.VMXE（参见第23.8节）。系统软件通过执行VMXOFF指令离开VMX操作。执行VMXOFF后，可以在VMX操作之外清除CR4.VMXE。VMXON也由IA32_FEATURE_CONTROL MSR（MSR地址3AH）控制。该MSR清零重置逻辑处理器时MSR的相关位是： 位0是锁定位。如果该位清零，则VMXON会导致general-protection异常。如果设置了锁定位，WRMSR到此MSR会导致general-protection异常;在上电复位之前，不能修改MSR。系统BIOS可以使用此位为BIOS提供设置选项以禁用对VMX的支持。在平台中启用VMX支持，BIOS必须设置位1或者位2或两者（见下文）以及锁定位。 位1在SMX操作中启用VMXON。如果该位清零，则在SMX操作中执行VMXON会导致general-protection expection。尝试在不支持两个VMX的逻辑处理器上设置此位操作（参见第23.6节）和SMX操作（参见英特尔®中的第6章“安全模式扩展参考”第2D卷）导致general-proction异常。 位2在SMX操作之外启用VMXON。如果该位清零，则在SMX外部执行VMXON操作会导致general-proction异常。尝试在没有的逻辑处理器上设置此位支持VMX操作（参见第23.6节）导致general-proction异常。 在执行VMXON之前，软件应该分配一个逻辑上自然对齐的4 KB内存区域,处理器可用于支持VMX操作.1该区域称为VMXON region。VMXON zone的地址区域（VMXON指针）在VMXON的操作数中提供。第24节 23.8 VMX操作限制（作者：限制还有很有一些的，暂且不一一列举了吧，这里挑1-2点） 当逻辑处理器在VMX root操作时，中断信号是被block的。但当在VMX non-root模式的时候，不会被block，相反中断信号会触发VM exit。 24.1 虚拟机控制结构(VMCS)逻辑处理器在VMX操作中使用”虚拟机控制结构”（VMCS）。这些管理VMX进出非root与root操作（VM entry和VM exit）以及处理器在VMX non-root时的行为。这个结构由新指令VMCLEAR，VMPTRLD，VMREAD, VMWRITE操纵。VMM可以为其支持的每个虚拟机使用不同的VMCS。对于具有多个的虚拟机在逻辑处理器（虚拟处理器）中，VMM可以为每个虚拟处理器使用不同的VMCS。逻辑处理器将存储器中的区域与每个VMCS相关联。该区域称为VMCS region。软件使用区域的64位物理地址（VMCS指针）引用特定VMCS。 VMCS指针必须在4 KB边界上对齐（位11：0必须为零）。这些指针不能设置超出理器的物理地址宽度。逻辑处理器可以维护多个活动的VMCS。处理器通过维护内存中活跃VMCS的状态来优化VMX操作。在任何给定时间，最多一个活动VMCS的数量是当前VMCS。 （本文档经常使用术语“VMCS”来指代当前VMCS。）VMLAUNCH，VMREAD，VMRESUME和VMWRITE指令仅对当前操作VMCS。以下各项描述了逻辑处理器如何确定哪些VMCS处于活动状态以及哪些VMCS处于当前状态： VMPTRLD指令的内存操作数是VMCS的地址。执行完指令后VMCS在逻辑处理器上既是活动的和也是当前的。任何其他活动的VMCS都不是当前VMCS。 VMCS中的VMCS链接指针字段（参见第24.4.2节）本身就是VMCS的地址。如果VM entry正确执行了，并且“VMCS shadow”VM执行控制（VMCS）的1设置成功，那么VMCS链接指针字段引用的字符在逻辑处理器上变为活动状态。当前VMCS不会改变。 VMCLEAR指令的内存操作数也是VMCS的地址。执行完毕后指令，VMCS在逻辑处理器上既不是活动的也不是当前的。如果VMCS已经开启逻辑处理器，逻辑处理器不再具有当前的VMCS。 VMPTRST指令将逻辑处理器的当前VMCS的地址存储到指定的存储器位置（如果没有当前的VMCS，则存储值FFFFFFFF_FFFFFFFFH）。VMCS的启动状态确定哪个VM-entry指令应该与该VMCS一起使用：VMLAUNCH指令需要VMCS，其启动状态为“清除”; VMRESUME指令需要VMCS，其发射状态是“发射”。逻辑处理器在相应的VMCS中维护VMCS的启动状态区域。以下各项描述了逻辑处理器如何管理VMCS的启动状态： 如果当前VMCS的启动状态为“clear”，则VMLAUNCH指令的成功执行会发生变化发射状态为“launched”。 VMCLEAR指令的内存操作数是VMCS的地址。执行完指令后VMCS的启动状态是“clear”。 没有其他方法可以修改VMCS的启动状态（无法使用VMWRITE进行修改）也没有直接的方法来发现它（它无法使用VMREAD读取） 图24-1说明了VMCS的不同状态。它使用“X”表示VMCS，使用“Y”表示任何其他VMCS。因此：“VMPTRLD X”总是使得VMCS变为当前和活动状态; “VMPTRLD Y”让X不再是当前状态（因为它使Y变为当前状态）;如果X是当前的并且其启动状态时，则VMLAUNCH的会将X变为“launched”; VMCLEAR X总是使X处于非活动状态而不是当前状态，并使其启动状态“clear”。该图未示出相对于这些参数不修改VMCS状态的操作（例如，当X已经是当前时执行VMPTRLD X）。请注意，VMCLEAR X使X“处于非活动状态，非当前状态，并且clear。即使X的当前状态未定义（例如，即使X尚未初始化）。见24.11.3节。由于影子VMCS（请参阅第24.10节）不能用于VM条目，因此影子VMCS的启动状态为没有意义。图24-1未说明可以使影子VMCS处于活动状态的所有方式。 24.2 VMCS Region的格式VMCS的格式包含了4K，VMCS的格式如表24-1 byte offside 内容 0 0~30位 VMCS保留，识别符31位: shadow-VMCS标识位 4 VMCS终止位 8 VMCS 数据 VMCS区域的前4个字节包含位30：0的VMCS修订标识符。维护的处理器不同格式的VMCS数据（见下文）使用不同的VMCS修订标识符。位31：shadow-VMCS指标（参见第24.10节） 在将该区域用于VMCS之前，软件应将VMCS标识符写入VMCS区域。该VMCS标识符永远不会被处理器写入;如果VMPTRLD的操作数引用VMCS区域的标识符与处理器正在使用的VMCS不同，则VMPTRLD会失败。 （如果是影子VMCS，并且处理器不支持shadow-VMCS，那么VMPTRLD也会失败）软件可以通过读取VMX相关MSR IA32_VMX_BASIC来检查处理器VMCS标识符。软件应根据VMCS是否为普通还是shadow-vmcs来设置或者清楚shadow-VMCS标识符.不支持“VMCS阴影”VM执行控件的1设置。软件可以通过读取VMX功能MSR IA32_VMX_PROCBASED_CTLS2来检查是否支持。VMCS区域的接下来的4个字节用于VMX中止指示符。这些位的内容没有控制处理器。当VMX中止发生时，逻辑处理器将非零值写入这些位。软件也可以写入此字段。VMCS区域的其余部分用于VMCS数据（控制VMX non-root操作以及VMX转换）。这些数据的格式是特定的。写回可缓存内存中的VMCS区域和相关结构（在第24.11.4节中列举）。未来实现可以允许或要求不同的存储器类型。软件应参考VMX功能MSRIA32_VMX_BASIC（见附录A.1） 未完待续……]]></content>
  </entry>
  <entry>
    <title><![CDATA[利用QOM(Qemu Object Model)创建虚拟设备]]></title>
    <url>%2F2018%2F12%2F26%2Fqemu-qom%2F</url>
    <content type="text"><![CDATA[什么是QOMQOM(Qemu Object Model)是QEMU最新的设备模型，将所有的模拟设备整合成了一种单根结点(系统总线)的树状形式，并具有热插拔功能。后来可能由于Device和Bus之间的复杂关系，又开发了QOM。QOM是QEMU在C的基础上自己实现的一套面向对象机制，负责将device、bus等设备都抽象成为对象。 QOM 的初始化对象的初始化分为四步： 将 TypeInfo 注册 TypeImpl 实例化 ObjectClass 实例化 Object 添加 Property 根据QEMU的wiki ，QOM没有构造和析构的概念。但矛盾的是根据代码，TypeInfo 中定义的 class_init 和 instance_init 无论从名字还是实现上都做了对象的初始化工作，比如设置对象成员的值。但为什么说它们最多只能算是初始化函数呢？Everything in QOM is a device根据实现，经过 class_init 和 instance_init 产生设备对应Object后，这个Object是不能直接使用的。其真正初始化逻辑的大头都放在 realize 中做，比如创建对应的memory region，挂载到对应bus上等等。只有在 realize 后，设备才算真正构造完成，可以拿来用了。因此QEMU认为，类似构造和析构的是realize和unrealize。而在设备的生命周期中，可以被realize和unrealize多次。为了保持习惯，本文会依然将 class_init 和 instance_init 当做构造函数，称前者为类构造函数，后者为类实例构造函数。 使用QOM添加设备源码分析下面我们就利用一个真实的案例，讲解一下利用QOM添加设备的具体实现步骤 TypeInfo =&gt; ModuleEntry设备相关代码的入口就是这里了, TypeInfo 定义了一种类型，并且使用函数type_register_static注册： 1234567891011121314static const TypeInfo caffee_agent_info = &#123; .name = &quot;caffee-agent&quot;, .parent = TYPE_ISA_DEVICE, .class_init = caffee_agent_class_init, .instance_size = sizeof(CaffeeAgentState), .instance_init = caffee_agent_initfn,&#125;; static void caffee_agent_register_types (void)&#123; type_register_static (&amp;caffee_agent_info);&#125;type_init(cafe_agent_register_types) 包含 类型的名称(name)、父类名称(parent)、Object实例的大小(instance_size)、是否抽象类(abstract)、初始化函数(class_init)。代码底部有 type_init ，由 C run-time(CRT)负责执行： 12345678910111213void register_module_init(void (*fn)(void), module_init_type type)&#123; ModuleEntry *e; ModuleTypeList *l; e = g_malloc0(sizeof(*e)); e-&gt;init = fn; e-&gt;type = type; l = find_type(type); QTAILQ_INSERT_TAIL(l, e, node);&#125; 创建了 type 为 MODULE_INIT_QOM ，init为 kvm_type_init 的 ModuleEntry ，并加入到 MODULE_INIT_QOM 的 ModuleTypeList 中。 ModuleEntry =&gt; TypeImpl在 main.c(vl.c) 的一开始执行了 module_call_init(MODULE_INIT_QOM) ，它从 init_type_list 中取出对应的 ModuleTypeList ，然后对里面的 ModuleEntry 成员都调用 init 函数。对于上文提到的 ModuleEntry ，调用的是： 12345678static TypeImpl *type_register_internal(const TypeInfo *info)&#123; TypeImpl *ti; ti = type_new(info); type_table_add(ti); return ti;&#125; 它根据 kvm_accel_type(TypeInfo) 创建一个名为TYPE_KVM_ACCEL的 TypeImpl 类型的结构。同时将该 TypeImpl 注册到全局 type_table 中，key为类型名称，即 TYPE_KVM_ACCEL ObjectClass12345678910111213struct ObjectClass&#123; /*&lt; private &gt;*/ Type type; // 用typedef定义的 TypeImpl 指针 GSList *interfaces; const char *object_cast_cache[OBJECT_CLASS_CAST_CACHE]; const char *class_cast_cache[OBJECT_CLASS_CAST_CACHE]; ObjectUnparent *unparent; GHashTable *properties;&#125;; ObjectClass 属于类对象，它是所有类对象的基类。 TypeImpl =&gt; ObjectClass有两种路径，一种是主动地调用： 比如 object_class_get_list(TYPE_DEVICE, false) 创建 TYPE_DEVICE 类型的 ObjectClass该过程用到glic的函数 g_hash_table_foreach ，见 https://developer.gnome.org/glib/stable/glib-Hash-Tables.html#g-hash-table-foreach另一种是被动调用，如: object_class_by_name object_class_get_parent object_new_with_type object_initialize_with_type 在获取 class、class的parent、创建type的object、初始化TypeImpl的object时，调用 type_initialize12345678910type_initialize=&gt; 如果 TypeImpl 已创建(class成员有值)，返回=&gt; ti-&gt;class = g_malloc0(ti-&gt;class_size) 根据class_size分配内存空间=&gt; type_get_parent(ti) 获取父类的TypeImpl=&gt; memcpy(ti-&gt;class, parent-&gt;class, parent-&gt;class_size) 将parent的class拷贝到自己class的最前面=&gt; ti-&gt;class-&gt;properties = g_hash_table_new_full 创建存放property的hash table=&gt; type_initialize_interface 初始化class的接口，包括父类和自己的=&gt; ti-&gt;class-&gt;type = ti 设置class的type为对应TypeImpl=&gt; parent-&gt;class_base_init 如果parent定义了 class_base_init ，调用之=&gt; ti-&gt;class_init(ti-&gt;class, ti-&gt;class_data) 调用class的 class_init 对于 kvm_accel_type 这个 TypeInfo 的 TypeImpl ，调用的class_init是 kvm_accel_class_init ，它将传入的 ObjectClass 强转为子类 AccelClass ，设置 init_machine 成员为 kvm_init这里的class是该类型的类实例，它的基类是 ObjectClass 。 继承从创建流程可以看出，在创建类对象时，会调用 type_initialize ，其会递归地对 TypeImpl 中的 parent 成员(TypeImpl)递归调用 type_initialize ，然后将创建出来的相应 ObjectClass 拷贝到自己class的最前面。类对象的第一个成员是 parent_class ，由于父类对象会拷到子类对象的最前面，因此可以认为其指向父类的对象，如此构成链状的继承链，最终指向基类对象 ObjectClass比如 kvm_accel_type 对应的类对象，该类对象作为叶子类型并没有定义，但其父类 AccelClass 在代码中有定义，其的第一个成员为 ObjectClass ，表示其继承自 ObjectClass 。为了能表示该叶子类型继承 AccelClass ，它修改了 AccelClass的一些对象成员，这样在某种程度上表示了继承关系。比如修改了函数指针成员的指向，相当于实现了虚函数。又如： register_info 对应的类对象 =&gt; PCIDeviceClass =&gt; DeviceClass =&gt; ObjectClass 构成继承链，最前端的叶子类型通过修改 PCIDeviceClass 成员进行定义。 强制类型转换将一个父类的指针转换为子类的指针是不安全的，为了实现这种转换，各类需要提供强制类型转换的宏，如：123456#define ACCEL_CLASS(klass) \ OBJECT_CLASS_CHECK(AccelClass, (klass), TYPE_ACCEL)#define OBJECT_CLASS_CHECK(class_type, class, name) \ ((class_type *)object_class_dynamic_cast_assert(OBJECT_CLASS(class), (name), \ __FILE__, __LINE__, __func__)) 如果类对象指针的name和目标子类的name一致，或类对象指针是目标子类的祖先，则执行转换，否则 abort反过来，从子类指针转换为父类指针是安全的，因为类的第一项就指向父类，访问时不会存在越界等问题。 ObjectObject 属于类实例对象，它是所有类实例对象的基类。123456789struct Object&#123; /*&lt; private &gt;*/ ObjectClass *class; // 指向类对象 ObjectFree *free; GHashTable *properties; // 维护属性的哈希表 uint32_t ref; // 引用计数 Object *parent; // 指向父类实例对象，实现继承&#125;; 可以看到其第一个成员指向类对象，同时维护有区别于类属性的类实例属性。 创建流程就流程而言，在C runtime 根据 TypeInfo 创建了 TypeImpl 后，此后主要根据 TypeImpl 创建 ObjectClass 和 Object以 TypeInfo(kvm_accel_type) 为例，其创建的 TypeImpl 在以下流程发挥作用：12345678910111213141516main =&gt; configure_accelerator =&gt; accel_init_machine(acc, ms)=&gt; ObjectClass *oc = OBJECT_CLASS(acc) 将AccelClass指针转换成父类(ObjectClass)指针=&gt; object_class_get_name 获取 ObjectClass-&gt;TypeImpl 的类名，如 kvm-accel=&gt; ACCEL(object_new(cname)) 利用名称创建 AccelState 对象=&gt; acc-&gt;init_machine(ms) 初始化machine，实际上是调用 kvm_initobject_new=&gt; type_get_by_name(typename) 根据类名查type_table获取 TypeImpl=&gt; object_new_with_type =&gt; type_initialize 创建 TypeImpl 对应的类对象，设置到对应 TypeImpl-&gt;class 中 =&gt; g_malloc(type-&gt;instance_size) 分配类实例对象的内存 =&gt; object_initialize_with_type 创建类实例对象 =&gt; type_initialize 会再次尝试实例化类对象 =&gt; obj-&gt;class = type-&gt;class 设置类实例对象的类对象为 TypeImpl-&gt;class =&gt; obj-&gt;properties = g_hash_table_new_full 创建存放类实例对象property的hash table =&gt; object_init_with_type =&gt; object_init_with_type 如果 TypeImpl 有父类，递归调用object_init_with_type =&gt; ti-&gt;instance_init(obj) 如果定义了类实例的构造函数，调用之 继承定义上的继承主要指类的继承，既然类对象已经通过包含的方式实现了继承，那么类实例对象就可以通过调用自己的class成员调用父类的函数，访问父类的class property。但在QEMU实现的这套面向对象模型中，类实例对象也拥有自己的构造函数，因此根据继承关系，需要对父类实例对象的构造函数进行调用。从创建流程可以看出，在创建类实例对象时，会调用 object_init_with_type ，其会递归地对 TypeImpl 中的 parent 成员递归调用 object_init_with_type ，从而让所有父类的 instance_init 都得到调用，在调用时传入的是当前对象的地址，相当于在当前对象上对父类实例对象进行构造。同理，类实例对象的第一个成员是 parent_obj ，指向父类的实例对象，如此构成链状的继承链，最终指向基类实例对象 Object如： kvm_accel_type的类实例Object =&gt; AccelState =&gt; Object又如： register_info的类实例Object =&gt; PCIDevice =&gt; DeviceState =&gt; Object 强制类型转换同理，将一个父类实例的指针转换为子类实例指针是不安全的。为了实现这种转换，各类需要提供强制类型转换的宏，如：123456#define ACCEL(obj) \ OBJECT_CHECK(AccelState, (obj), TYPE_ACCEL)#define OBJECT_CHECK(type, obj, name) \ ((type *)object_dynamic_cast_assert(OBJECT(obj), (name), \ __FILE__, __LINE__, __func__)) 如果类实例对象指针的name和目标子类实例的name一致，或类实例对象指针是目标子类的祖先，则执行转换，否则 abort。反过来，从子类实例指针转换为父类实例指针是安全的，因为类实例的第一项就指向父类实例，访问时不会存在越界等问题。 属性属性分为类对象(ObjectClass)属性和类实例对象(Object)属性，存储于 properties 成员中。properties 是一个 GHashTable ，存储了属性名到ObjectProperty的映射。 属性模版用于创建属性对象 ObjectProperty1234567891011struct Property &#123; const char *name; PropertyInfo *info; ptrdiff_t offset; uint8_t bitnr; QType qtype; int64_t defval; int arrayoffset; PropertyInfo *arrayinfo; int arrayfieldsize;&#125;; 属性对象属性对象包含属性名称、类型、描述，类型对应的属性结构，以及相应访问函数。1234567891011typedef struct ObjectProperty&#123; gchar *name; gchar *type; gchar *description; ObjectPropertyAccessor *get; ObjectPropertyAccessor *set; ObjectPropertyResolve *resolve; ObjectPropertyRelease *release; void *opaque;&#125; ObjectProperty; 如对于bool类型的属性，opaque为 BoolProperty ，set为 property_set_bool ，get为 property_get_bool 。12345typedef struct BoolProperty&#123; bool (*get)(Object *, Error **); void (*set)(Object *, bool, Error **);&#125; BoolProperty; 用于保存用户传入的 getter 和 setter 。 getter / setter (callback hook)定义了在设置/读取属性时触发的函数。比如 device 类型的 instance_init 即 device_initfn 中，定义了 realized 属性：1object_property_add_bool(obj, &quot;realized&quot;, device_get_realized, device_set_realized, NULL) 则 getter 为 device_get_realized ， setter 为 device_set_realized 静态属性凡是在代码中就已经定义好名称和类型的属性，都是静态属性。包括在初始化过程中添加 和 props 。 初始化过程中添加比如对于 TypeInfo x86_cpu_type_info ，类实例初始化函数 x86_cpu_initfn 定义好了属性：12345object_property_add(obj, &quot;family&quot;, &quot;int&quot;, x86_cpuid_version_get_family, x86_cpuid_version_set_family, NULL, NULL, NULL);object_property_add_alias(obj, &quot;kvm_steal_time&quot;, obj, &quot;kvm-steal-time&quot;, &amp;error_abort); 该属性会直接加到类实例对象的properties中。 props一些类对象会在 class_init 中设置 props 成员，比如 TypeInfo host_x86_cpu_type_info 在 host_x86_cpu_class_init 设置为 host_x86_cpu_properties：123456789101112131415161718192021static Property host_x86_cpu_properties[] = &#123; DEFINE_PROP_BOOL(&quot;migratable&quot;, X86CPU, migratable, true), DEFINE_PROP_BOOL(&quot;host-cache-info&quot;, X86CPU, cache_info_passthrough, false), DEFINE_PROP_END_OF_LIST()&#125;;#define DEFINE_PROP_BOOL(_name, _state, _field, _defval) &#123; \ .name = (_name), \ .info = &amp;(qdev_prop_bool), \ .offset = offsetof(_state, _field) \ + type_check(bool, typeof_field(_state, _field)), \ .qtype = QTYPE_QBOOL, \ .defval = (bool)_defval, \ &#125;// 闭包PropertyInfo qdev_prop_bool = &#123; .name = &quot;bool&quot;, .get = get_bool, .set = set_bool,&#125;; 而类实例 X86CPU 中定义了这些属性：123456struct X86CPU &#123; bool migratable; ... bool cache_info_passthrough; ...&#125;; 于是 X86CPU.migratable 和 X86CPU.cache_info_passthrough 两个成员被定义成属性。 在父类 device_type_info 的类实例初始化函数 device_initfn 中，对所有的props，有：1234567do &#123; for (prop = DEVICE_CLASS(class)-&gt;props; prop &amp;&amp; prop-&gt;name; prop++) &#123; qdev_property_add_legacy(dev, prop, &amp;error_abort); qdev_property_add_static(dev, prop, &amp;error_abort); &#125; class = object_class_get_parent(class);&#125; while (class != object_class_by_name(TYPE_DEVICE)); 而 qdev_property_add_static ： =&gt; object_property_add(obj, prop-&gt;name, prop-&gt;info-&gt;name, prop-&gt;info-&gt;get, prop-&gt;info-&gt;set, prop-&gt;info-&gt;release, prop, &amp;local_err) 根据Property中的数据，创建ObjectProperty，并将其加到类实例对象的 properties 中 关键是将闭包中的get和set取出，作为ObjectProperty的get和set=&gt; object_property_set_description 设置属性的描述字符串=&gt; 设置属性的默认值 查看可通过命令查看设备的静态属性，参数为设备 TypeInfo 的 name：1/home/binss/work/qemu/qemu-2.8.1.1/x86_64-softmmu/qemu-system-x86_64 -device Broadwell-x86_64-cpu,? 但是， x86_64-cpu 抽象设备无法打。 host-x86_64-cpu 无法列出。 动态属性指在运行时动态进行添加的属性。比如用户通过参数传入了一个设备，需要作为属性和其它设备关联起来。典型的动态属性就是 child&lt;&gt; 和 link&lt;&gt; (因为其类型就是这样构造的，后文简称child和link) 。 childchild实现了composition关系，表示一个设备(parent)创建了另外一个设备(child)，parent掌控child的生命周期，负责向其发送事件。一个device只能有一个parent，但能有多个child。这样就构成一棵组合树。通过 object_property_add_child 添加child：12=&gt; object_property_add 将 child 作为 obj 的属性，属性名name，类型为 &quot;child&lt;child的类名&gt;&quot;，同时getter为object_get_child_property，没有setter=&gt; child-&gt;parent = obj 例如 x86_cpu_realizefn =&gt; x86_cpu_apic_create =&gt; object_property_add_child(OBJECT(cpu), “lapic”, OBJECT(cpu-&gt;apic_state), &amp;error_abort) 将创建 APICCommonState ，并设置为 X86CPU 的child。可以在qemu hmp查询到：1234(qemu) info qom-tree/machine (pc-q35-2.8-machine) /unattached (container) /device[0] (host-x86_64-cpu) linklink实现了backlink关系，表示一个设备引用了另外一个设备，是一种松散的联系。两个设备之间能有多个link关系，可以进行修改。它完善了组合树，使其构成构成了一幅有向图。通过 object_property_add_link 添加link：12=&gt; 创建 LinkProperty ，填充目标(child)的信息=&gt; object_property_add 将 LinkProperty 作为 obj 的属性，属性名name，类型为 &quot;link&lt;child的类名&gt;&quot;，同时getter为 object_get_link_property 。如果传入了check函数，则需要回调，设置setter为 object_set_link_property 例如 q35 有以下link：123456789101112131415161718static void q35_host_initfn(Object *obj)&#123; object_property_add_link(obj, MCH_HOST_PROP_RAM_MEM, TYPE_MEMORY_REGION, (Object **) &amp;s-&gt;mch.ram_memory, qdev_prop_allow_set_link_before_realize, 0, NULL); object_property_add_link(obj, MCH_HOST_PROP_PCI_MEM, TYPE_MEMORY_REGION, (Object **) &amp;s-&gt;mch.pci_address_space, qdev_prop_allow_set_link_before_realize, 0, NULL); object_property_add_link(obj, MCH_HOST_PROP_SYSTEM_MEM, TYPE_MEMORY_REGION, (Object **) &amp;s-&gt;mch.system_memory, qdev_prop_allow_set_link_before_realize, 0, NULL); object_property_add_link(obj, MCH_HOST_PROP_IO_MEM, TYPE_MEMORY_REGION, (Object **) &amp;s-&gt;mch.address_space_io, qdev_prop_allow_set_link_before_realize, 0, NULL);&#125; 将 Q35PCIHost 和 ram_memory / pci_address_space / system_memory / address_space_io 链接起来。 API根据前面所述，属性有两种定义方式，一种是通过 DEFINE_PROP_* 定义，另一种是通过 object_property_add_&lt;type&gt; 进行定义。根据不同的定义方式，set会不同，设置值的方式也有所不同。 object_property_set_&lt;type&gt;用于设置某个属性的值。比如 object_property_set_bool ：12345=&gt; qbool_from_bool 将要设置的值包装成相应的 QObject ，这里是QBool=&gt; object_property_set_qobject =&gt; qobject_input_visitor_new 将传入的QObject包装成Visitor，其中含各类型的处理函数 =&gt; object_property_set =&gt; object_property_find 从props的hash table中找到对应的 ObjectProperty =&gt; prop-&gt;set 对于 DEFINE_PROP_BOOL 创建的属性来说，其闭包为qdev_prop_bool，因此在初始化时 set 被设置为 set_bool1234set_bool=&gt; qdev_get_prop_ptr 将设备指针加上属性值在其中的偏移量，得到属性值的地址=&gt; visit_type_bool =&gt; v-&gt;type_bool (qobject_input_type_bool) =&gt; qobject_input_get_object 从Visitor中取出QObject =&gt; qbool_get_bool 从QObject中取出值，设置到属性值的地址 对于 object_property_add_bool 创建的属性来说，它在 object_property_add 时设置 set 为 property_set_bool123property_set_bool=&gt; visit_type_bool =&gt; v-&gt;type_bool (qobject_input_type_bool) =&gt; qobject_input_get_object 找到QObject=&gt; (BoolProperty)prop-&gt;set 调用setter 比如 device 类型的 instance_init 即 device_initfn 中，定义了 realized 属性：1object_property_add_bool(obj, &quot;realized&quot;, device_get_realized, device_set_realized, NULL) 于是 setter 为 device_set_realized12=&gt; dc-&gt;realize 调用realize函数，其在 class_init 中定义=&gt; dev-&gt;realized = value 设置类实例对象的成员 一句话总结，前者的属性值的设置由 type_bool 负责设置，而后者由 setter 负责设置。 object_property_get_&lt;type&gt;用于读取某个属性的值。比如 object_property_get_bool ：1234567=&gt; object_property_get_qobject =&gt; 创建空的QObject指针 =&gt; qobject_output_visitor_new 将传入的QObject包装成Visitor，其中含各类型的处理函数 =&gt; object_property_get =&gt; object_property_find 从props的hash table中找到对应的 ObjectProperty =&gt; prop-&gt;get 调用get函数，设置QObject=&gt; qobject_to_qbool 将QObject转成QBool=&gt; qbool_get_bool 从QBool中取出值，返回 对于 DEFINE_PROP_BOOL 创建的属性来说，其闭包为qdev_prop_bool，因此在初始化时 get 被设置为 get_bool123get_bool=&gt; qdev_get_prop_ptr(dev, prop) 将设备指针加上属性值在其中的偏移量，得到属性值的地址=&gt; visit_type_bool =&gt; v-&gt;type_bool (qobject_output_type_bool) =&gt; qobject_input_get_object 将属性值包装成QObject 对于 object_property_add_bool 创建的属性来说，它在 object_property_add 时设置 get 为 property_get_bool ：123property_get_bool=&gt; prop-&gt;get 调用getter，得到属性值=&gt; visit_type_bool =&gt; v-&gt;type_bool (qobject_output_type_bool) =&gt; qobject_input_get_object 将属性值包装成QObject 个人的理解是，set 和 get 都需要通过 QObject 和 Visitor 两层包装。前者把要设置属性值包装成QObject再到Visitor，然后再取出设置到相应地址。后者根据属性值地址将属性值包装成QObject，设置为Visitor中QObject指针指向，然后再从QObject中取出值。 object_property_parse在用一个string设置不知道类型的属性的值时，使用 object_property_parse：1234567void object_property_parse(Object *obj, const char *string, const char *name, Error **errp)&#123; Visitor *v = string_input_visitor_new(string); object_property_set(obj, v, name, errp); visit_free(v);&#125; 它会创建一个 Visitor 并将值设置到里面，这里定义了string转其他类型属性的函数：1234567v-&gt;visitor.type = VISITOR_INPUT;v-&gt;visitor.type_int64 = parse_type_int64;v-&gt;visitor.type_uint64 = parse_type_uint64;v-&gt;visitor.type_size = parse_type_size;v-&gt;visitor.type_bool = parse_type_bool;v-&gt;visitor.type_str = parse_type_str;v-&gt;visitor.type_number = parse_type_number; 总结如此一来，根据 TypeInfo 创建了 TypeImpl ，然后根据 TypeImpl 创建了对应的 ObjectClass ，再根据 TypeImpl 创建了对应的 Object ， ObjectClass 和 Object 都有自己的 Property，关系如下：1234 TypeImpl class -&gt; ObjectClass(AccelClass) Object(AccelState) &lt;- type &lt;- classTypeImpl &lt;- parent_type properties(GHashTable) properties(GHashTable) op1=>operation: type_init(kvm_type_init) op2=>operation: module_init(function, MODULE_INIT_QOM) op3=>operation: register_module_init(function, type) op1(right)->op2(right)->op3{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);op1=>operation: kvm_type_init op2=>operation: type_register_static(&kvm_accel_type) op3=>operation: type_register op4=>operation: type_register_internal op1(right)->op2(right)->op3(right)->op4{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);op1=>operation: object_class_get_list op2=>operation: object_class_foreach op3=>operation: g_hash_table_foreach(object_class_foreach_tramp) op4=>operation: object_class_foreach_tramp op5=>operation: type_initialize op1(right)->op2(right)->op3(right)->op4(right)->op5{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-2-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-2-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-2", options);]]></content>
      <categories>
        <category>QEMU</category>
      </categories>
      <tags>
        <tag>QEMU QOM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debug QEMU with GDB]]></title>
    <url>%2F2018%2F12%2F24%2Fqemu-debug%2F</url>
    <content type="text"><![CDATA[学习Qemu-KVM虚拟机最重要的一步——调试QEMU，我们这里提前帮大家简单的总结归纳一下。Qemu的调试稍微有点特殊的地方就是，除了Qemu程序自身源代码的调试以外，我们可以通过Qemu+GDB来调试我们虚拟机程序。下面将两个不同方面的调试方法介绍一下。 1. 调试QEMU源码1gdb --args x86_64-softmmu/qemu-system-x86_64 --enable-kvm -m 1024 -drive file=test.qcow2 -append console=ttyS0 -kernel /boot/vmlinuz -initrd /boot/initrd.gz 当然以上参数中从–enable-kvm开始之后的参数因人而异，不尽相同。执行过之后，就会进入gdb界面，就可以跟其他普通应用程序一样，进行单步调试、设置断点、查看栈、寄存器内容等 2. 调试虚拟机这部分是本文的重点。跟调试应用程序不同，调试虚拟机时gdb和qemu分开执行，似乎并不能用gdb来调用qemu。长话短说，先来看如何启动qemu：1./x86_64-softmmu/qemu-system-x86_64 -s -S --enable-kvm -m 1024 -hda test.qcow2 同样，参数从–enable-kvm开始之后的参数也都不是必须的。着重了解下两个必须的参数：12-s shorthand for -gdb tcp::1234-S freeze CPU at startup (use &apos;c&apos; to start execution) 然后新开一个终端执行gdb，这样就跟调试应用程序一样，会看到同样的’(gdb)’ 提示符。在提示符中输入1target remote localhost:1234 1234是默认用于远程调试连接的端口号。然后设置断点”break *0x7c00”，这样就将一个断点设置在了bootloader被加载到的内存地址，接下来就任你玩了。 [root@ccd-sdv6 ~]# gdb GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type "show copying" and "show warranty" for details. This GDB was configured as "x86_64-redhat-linux-gnu". For bug reporting instructions, please see: . (gdb) target remote localhost:1234 Remote debugging using localhost:1234 0x0000fff0 in ?? () (gdb) c Continuing. Program received signal SIGINT, Interrupt. 0x00002bcb in ?? () (gdb) b *0x7c00 Breakpoint 1 at 0x7c00. (gdb) info breakpoints Num Type Disp Enb Address What 1 breakpoint keep y 0x00007c00 (gdb) 顺便附上一些用到的gdb的快捷键以及命令TUI 窗口123Ctrl + x, Ctrl + a好像等效Ctrl + x, a 一般也就按着Ctrl键，依次按下字母x 和a就可以再TUI和非TUI间切换 TUI 窗口概述在TUI模式中，可以显示以下几个窗口： 命令窗口用于GDB调试时的命令输入和命令结果输出显示，与普通GDB窗口无异。 源代码窗口用于显示程序源代码，包括当前运行行、中断以中断标识等。 汇编窗口显示当前程序的汇编代码。 寄存器窗口显示处理器的寄存器内容，当寄存器内容发生改变时会高亮显示。源代码窗口和汇编窗口会高亮显示程序运行位置并以’&gt;’符号标记。有两个特殊标记用于标识断点，第一个标记用于标识断点类型： B : 程序至少有一次运行到了该断点 b : 程序没有运行到过该断点 H : 程序至少有一次运行到了该硬件断点 h : 程序没有运行到过该硬件断点第二个标记用于标识断点使能与否: + : 断点使能Breakpointis enabled. - : 断点被禁用Breakpointis disabled. 三窗口模式1Ctrl + 2 使TUI的上半部分分割成两个窗口，连接按此快捷键可在三种组合中切换。寄存器窗口、代码窗口、汇编窗口 三个窗口只能同时显示两个，共3种组合。 更换激活窗口1Ctrl + o 之所以需要切换激活窗口，是因为有些快捷键，比如箭头上下左右，page up/down只有在当前窗口起作用 GDB commandc : continuer : runn : nexts : step TUI 特有命令info win ：显示正在显示的窗口大小信息layout next ：显示下一个窗口layout prev ：显示上一个窗口layout src ：显示源代码窗口layout asm ：显示汇编窗口layout split ：显示源代码和汇编窗口layout regs ：显示寄存器窗口focus next ： 将一个窗口置为激活状态focus prev ：将上一个窗口置为激活状态focus src : 将源代码窗口置为激活状态focus asm ：将汇编窗口置为激活状态focus regs ： 将寄存器窗口置为激活状态focus cmd ：将命令行窗口置为激活状态refresh ： 更新窗口，与C-L快捷键同 tuireg float ：寄存器窗口显示内容为浮点寄存器tuireg general ：寄存器窗口显示内容为普通寄存器tuireg next ：显示下一组寄存器，预定义的寄存器组: general, float,system, vector,all, save,restore.tuireg system ：显示上一组寄存器update ：更新源代码窗口到当前运行点winname + count ：增加指定窗口的高度winname + count ：减小指定窗口的高度tabset nchars : Set the width of tab stops to be nchars characters 条件断点：在gdb中可以watch一个寄存器，命令： 1watch $eax == 0x0000ffaa 另外，当我们想有条件的设置某一个断点的时候，命令如下：1break test.c:120 if $eax == 0x0000ffaa]]></content>
      <categories>
        <category>QEMU</category>
      </categories>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟化原理3--CPU]]></title>
    <url>%2F2018%2F12%2F10%2Fkvm-cpu%2F</url>
    <content type="text"><![CDATA[CPU 虚拟化简介上一篇文章笼统的介绍了一个虚拟机的诞生过程，从demo中也可以看到，运行一个虚拟机再也不需要像以前想象的那样，需要用软件来模拟硬件指令集了。虚拟机的指令集直接运行在宿主机物理CPU上，当虚拟机中的指令设计到IO操作或者一些特殊指令的时候，控制权转让给了宿主机（这里其实是转让给了vm monitor，下面检查VMM），也就是一个demo进程，他在宿主机上的表现形式也就是一个用户级进程。 用一张图来解释更为贴切。 VMM完成vCPU，内存的初始化后，通过ioctl调用KVM的接口，完成虚拟机的创建，并创建一个线程来运行VM，由于VM在前期初始化的时候会设置各种寄存器来帮助KVM查找到需要加载的指令的入口（main函数）。所以线程在调用了KVM接口后，物理CPU的控制权就交给了VM。VM运行在VMX non-root模式，这是Intel-V或者AMD-V提供的一种特殊的CPU执行模式。然后当VM执行了特殊指令的时候，CPU将当前VM的上下文保存到VMCS寄存器（这个寄存器是一个指针，保存了实际的上下文地址），然后执行权切换到VMM。VMM 获取 VM 返回原因，并做处理。如果是IO请求，VMM 可以直接读取VM的内存并将IO操作模拟出来，然后再调用VMRESUME指令，VM继续执行，此时在VM看来，IO操作的指令被CPU执行了。 Intel-V 技术Intel-V 技术是Intel为了支持虚拟化而提供的一套CPU特殊运行模式。 Intel-V虚拟化技术结构Intel-V 在IA-32处理器上扩展了处理器等级，原来的CPU支持ring0~ring3 4个等级，但是Linux只使用了其中的两个ring0,ring3。当CPU寄存器标示了当前CPU处于ring0级别的时候，表示此时CPU正在运行的是内核的代码。而当CPU处于ring3级别的时候，表示此时CPU正在运行的是用户级别的代码。当发生系统调用或者进程切换的时候，CPU会从ring3级别转到ring0级别。ring3级别是不允许执行硬件操作的，所有硬件操作都需要系统提供的API来完成。比如说一个IO操作： 1int nread = read(fd, buffer, 1024); 当执行到此段代码的时候，然后查找到系统调用号，保存到寄存器eax，然后会将对应的参数压栈后产生一个系统调用中断，对应的是 int $0x80。产生了系统调用中断后，此时CPU将切换到ring0模式，内核通过寄存器读取到参数，并完成最后的IO后续操作，操作完成后返回ring3模式。 12345movel $3,%eaxmovel fd,%ebxmovel buffer,%ecxmovel 1024,%edx int $0x80 Intel-V 在 ring0~ring3 的基础上，增加了VMX模式，VMX分为root和non-root。这里的VMX root模式是给VMM（前面有提到VM monitor)，在KVM体系中，就是qemu-kvm进程所运行的模式。VMX non-root模式就是运行的Guest，Guest也分ring0~ring3，不过他并不感知自己处于VMX non-root模式下。 Intel的虚拟架构基本上分两个部分: 虚拟机监视器 客户机（Guest VM) 虚拟机监视器（Virtual-machine monitors - VMM)虚拟机监视器在宿主机上表现为一个提供虚拟机CPU，内存以及一系列硬件虚拟的实体，这个实体在KVM体系中就是一个进程，如qemu-kvm。VMM负责管理虚拟机的资源，并拥有所有虚拟机资源的控制权，包括切换虚拟机的CPU上下文等。 Guest这个Guest在前面的Demo里面也提到，可能是一个操作系统（OS），也可能就是一个二进制程序，whatever，对于VMM来说，他就是一堆指令集，只需要知道入口（rip寄存器值）就可以加载。Guest运行需要虚拟CPU，当Guest代码运行的时候，处于VMX non-root模式，此模式下，该用什么指令还是用什么指令，该用寄存器该用cache还是用cache，但是在执行到特殊指令的时候（比如Demo中的out指令），把CPU控制权交给VMM，由VMM来处理特殊指令，完成硬件操作。 VMM 与 Guest 的切换 Guest与VMM之间的切换分两个部分：VM entry 和 VM exit。有几种情况会导致VM exit，比如说Guest执行了硬件访问操作，或者Guest调用了VMCALL指令或者调用了退出指令或者产生了一个page fault，或者访问了特殊设备的寄存器等。当Guest处于VMX模式的时候，没有提供获取是否处于此模式下的指令或者寄存器，也就是说，Guest不能判断当前CPU是否处于VMX模式。当产生VM exit的时候，CPU会将exit reason保存到MSRs（VMX模式的特殊寄存器组），对应到KVM就是vCPU-&gt;kvm_run-&gt;exit_reason。VMM根据exit_reason做相应的处理。 VMM 的生命周期如上图所示，VMM 开始于VMXON 指令，结束与VMXOFF指令。第一次启动Guest，通过VMLAUNCH指令加载Guest，这时候一切都是新的，比如说起始的rip寄存器等。后续Guest exit后再entry，是通过VMRESUME指令，此指令会将VMCS(后面会介绍到）所指向的内容加载到当前Guest的上下文，以便Guest继续执行。 VMCS （Virtual-Machine control structure)顾名思义，VMCS就是虚拟机控制结构，前面提到过很多次，Guest Exit的时候，会将当前Guest的上下文保存到VMCS中，Guest entry的时候把VMCS上下文恢复到VMM。VMCS是一个64位的指针，指向一个真实的内存地址，VMCS是以vCPU为单位的，就是说当前有多少个vCPU，就有多少个VMCS指针。VMCS的操作包括VMREAD，VMWRITE，VMCLEAR。 Guest exit Reason下面是qemu-kvm定义的exit reason。可以看到有很多可能会导致Guest转让控制权。选取几个解释一下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = &#123; [EXIT_REASON_EXCEPTION_NMI] = handle_exception, [EXIT_REASON_EXTERNAL_INTERRUPT] = handle_external_interrupt, [EXIT_REASON_TRIPLE_FAULT] = handle_triple_fault, [EXIT_REASON_NMI_WINDOW] = handle_nmi_window, // 访问了IO设备 [EXIT_REASON_IO_INSTRUCTION] = handle_io, // 访问了CR寄存器，地址寄存器，和DR寄存器（debug register)一样，用于调试 [EXIT_REASON_CR_ACCESS] = handle_cr, [EXIT_REASON_DR_ACCESS] = handle_dr, [EXIT_REASON_CPUID] = handle_cpuid, // 访问了MSR寄存器 [EXIT_REASON_MSR_READ] = handle_rdmsr, [EXIT_REASON_MSR_WRITE] = handle_wrmsr, [EXIT_REASON_PENDING_INTERRUPT] = handle_interrupt_window, // Guest执行了HLT指令，Demo开胃菜就是这个指令 [EXIT_REASON_HLT] = handle_halt, [EXIT_REASON_INVD] = handle_invd, [EXIT_REASON_INVLPG] = handle_invlpg, [EXIT_REASON_RDPMC] = handle_rdpmc, // 不太清楚以下VM系列的指令有什么用，猜测是递归VM（虚拟机里面运行虚拟机） [EXIT_REASON_VMCALL] = handle_vmcall, [EXIT_REASON_VMCLEAR] = handle_vmclear, [EXIT_REASON_VMLAUNCH] = handle_vmlaunch, [EXIT_REASON_VMPTRLD] = handle_vmptrld, [EXIT_REASON_VMPTRST] = handle_vmptrst, [EXIT_REASON_VMREAD] = handle_vmread, [EXIT_REASON_VMRESUME] = handle_vmresume, [EXIT_REASON_VMWRITE] = handle_vmwrite, [EXIT_REASON_VMOFF] = handle_vmoff, [EXIT_REASON_VMON] = handle_vmon, [EXIT_REASON_TPR_BELOW_THRESHOLD] = handle_tpr_below_threshold, // 访问了高级PCI设备 [EXIT_REASON_APIC_ACCESS] = handle_apic_access, [EXIT_REASON_APIC_WRITE] = handle_apic_write, [EXIT_REASON_EOI_INDUCED] = handle_apic_eoi_induced, [EXIT_REASON_WBINVD] = handle_wbinvd, [EXIT_REASON_XSETBV] = handle_xsetbv, // 进程切换 [EXIT_REASON_TASK_SWITCH] = handle_task_switch, [EXIT_REASON_MCE_DURING_VMENTRY] = handle_machine_check, // ept 是Intel的一个硬件内存虚拟化技术 [EXIT_REASON_EPT_VIOLATION] = handle_ept_violation, [EXIT_REASON_EPT_MISCONFIG] = handle_ept_misconfig, // 执行了暂停指令 [EXIT_REASON_PAUSE_INSTRUCTION] = handle_pause, [EXIT_REASON_MWAIT_INSTRUCTION] = handle_invalid_op, [EXIT_REASON_MONITOR_INSTRUCTION] = handle_invalid_op, [EXIT_REASON_INVEPT] = handle_invept,&#125;; 总结KVM的CPU虚拟化依托于Intel-V提供的虚拟化技术，将Guest运行于VMX模式，当执行了特殊操作的时候，将控制权返回给VMM。VMM处理完特殊操作后再把结果返回给Guest。CPU虚拟化可以说是KVM的最关键的核心，弄清楚了VM Exit和VM Entry。后续的IO虚拟化，内存虚拟化都是建立在此基础上。下一章介绍内存虚拟化。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟化原理2— QEMU启动过程]]></title>
    <url>%2F2018%2F12%2F10%2Fkvm-boot%2F</url>
    <content type="text"><![CDATA[虚拟机启动过程1234567891011121314151617181920212223第一步，获取到kvm句柄kvmfd = open(&quot;/dev/kvm&quot;, O_RDWR);第二步，创建虚拟机，获取到虚拟机句柄。vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0);第三步，为虚拟机映射内存，还有其他的PCI，信号处理的初始化。ioctl(vmfd, KVM_SET_USER_MEMORY_REGION, &amp;mem);第四步，创建vCPUvcpufd = ioctl(vmfd, KVM_CREATE_VCPU, vcpuio)第五步，为vCPU分配内存vcpu_size=ioctl(kvmfd, KVM_GET_VCPU_MMAP_SIZE, NULL)run = (struct kvm_run*)mmap(NULL, mmap_size, PROT_READ|PROT_WRITE, MAP_SHARED, vcpufd, 0)第六步，创建vCPU个数的线程并运行虚拟机。ioctl(vcpufd, KVM_RUN, 0);将汇编代码加载到用户内存中，并且设置vCPU的寄存器，例如RIP第七步，线程进入循环，并捕获虚拟机退出原因，做相应的处理。while(1) &#123; ioctl(kvm-&gt;vcpus-&gt;vcpu_fd, KVM_RUN, 0); &#125;;这里的退出并不一定是虚拟机关机，虚拟机如果遇到IO操作，访问硬件设备，缺页中断等都会退出执行，退出执行可以理解为将CPU执行上下文返回到QEMU。 12345678910open(&quot;/dev/kvm&quot;)ioctl(KVM_CREATE_VM)ioctl(KVM_CREATE_VCPU)for (;;) &#123; ioctl(KVM_RUN) switch (exit_reason) &#123; case KVM_EXIT_IO: /* ... */ case KVM_EXIT_HLT: /* ... */ &#125;&#125; 关于KVM_CREATE_VM参数的描述，创建的VM是没有cpu和内存的，需要QEMU进程利用mmap系统调用映射一块内存给VM的描述符，其实也就是给VM创建内存的过程。 KVM ioctl接口文档 先来一个KVM API开胃菜下面是一个KVM的简单demo，其目的在于加载 code 并使用KVM运行起来.这是一个at&amp;t的8086汇编，.code16表示他是一个16位的，当然直接运行是运行不起来的，为了让他运行起来，我们可以用KVM提供的API，将这个程序看做一个最简单的操作系统，让其运行起来。这个汇编的作用是输出al寄存器的值到0x3f8端口。对于x86架构来说，通过IN/OUT指令访问。PC架构一共有65536个8bit的I/O端口，组成64KI/O地址空间，编号从0~0xFFFF。连续两个8bit的端口可以组成一个16bit的端口，连续4个组成一个32bit的端口。I/O地址空间和CPU的物理地址空间是两个不同的概念，例如I/O地址空间为64K，一个32bit的CPU物理地址空间是4G。最终程序理想的输出应该是，al，bl的值后面KVM初始化的时候有赋值。4\n (并不直接输出\n，而是换了一行），hlt 指令表示虚拟机退出 12345678910.globl _start .code16_start: mov $0x3f8, %dx add %bl, %al add $&apos;0&apos;, %al out %al, (%dx) mov $&apos;\n&apos;, %al out %al, (%dx) hlt 我们编译一下这个汇编，得到一个 Bin.bin 的二进制文件 12as -32 bin.S -o bin.old -m elf_i386 --oformat binary -N -e _start -Ttext 0x10000 -o Bin.bin bin.o 查看一下二进制格式 12345678910111213➜ demo1 hexdump -C bin.bin00000000 ba f8 03 00 d8 04 30 ee b0 0a ee f4 |......0.....|0000000c对应了下面的code数组，这样直接加载字节码就不需要再从文件加载了 const uint8_t code[] = &#123; 0xba, 0xf8, 0x03, /* mov $0x3f8, %dx */ 0x00, 0xd8, /* add %bl, %al */ 0x04, &apos;0&apos;, /* add $&apos;0&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xb0, &apos;\n&apos;, /* mov $&apos;\n&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xf4, /* hlt */ &#125;; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#include &lt;err.h&gt;#include &lt;fcntl.h&gt;#include &lt;linux/kvm.h&gt;#include &lt;stdint.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/ioctl.h&gt;#include &lt;sys/mman.h&gt;#include &lt;sys/stat.h&gt;#include &lt;sys/types.h&gt;int main(void)&#123; int kvm, vmfd, vcpufd, ret; const uint8_t code[] = &#123; 0xba, 0xf8, 0x03, /* mov $0x3f8, %dx */ 0x00, 0xd8, /* add %bl, %al */ 0x04, &apos;0&apos;, /* add $&apos;0&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xb0, &apos;\n&apos;, /* mov $&apos;\n&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xf4, /* hlt */ &#125;; uint8_t *mem; struct kvm_sregs sregs; size_t mmap_size; struct kvm_run *run; // 获取 kvm 句柄 第一步 kvm = open(&quot;/dev/kvm&quot;, O_RDWR | O_CLOEXEC); if (kvm == -1) err(1, &quot;/dev/kvm&quot;); // 确保是正确的 API 版本 ret = ioctl(kvm, KVM_GET_API_VERSION, NULL); if (ret == -1) err(1, &quot;KVM_GET_API_VERSION&quot;); if (ret != 12) errx(1, &quot;KVM_GET_API_VERSION %d, expected 12&quot;, ret); // 创建一虚拟机 第二步 vmfd = ioctl(kvm, KVM_CREATE_VM, (unsigned long)0); if (vmfd == -1) err(1, &quot;KVM_CREATE_VM&quot;); // 为这个虚拟机申请内存，并将代码（镜像）加载到虚拟机内存中 mem = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0); if (!mem) err(1, &quot;allocating guest memory&quot;); memcpy(mem, code, sizeof(code)); // 为什么从 0x1000 开始呢，因为页表空间的前4K是留给页表目录 struct kvm_userspace_memory_region region = &#123; .slot = 0, .guest_phys_addr = 0x1000, .memory_size = 0x1000, .userspace_addr = (uint64_t)mem, &#125;; // 设置 KVM 的内存区域 第三部 ret = ioctl(vmfd, KVM_SET_USER_MEMORY_REGION, &amp;region); if (ret == -1) err(1, &quot;KVM_SET_USER_MEMORY_REGION&quot;); // 创建虚拟CPU 第四部 vcpufd = ioctl(vmfd, KVM_CREATE_VCPU, (unsigned long)0); if (vcpufd == -1) err(1, &quot;KVM_CREATE_VCPU&quot;); // 获取 KVM 运行时结构的大小 ret = ioctl(kvm, KVM_GET_VCPU_MMAP_SIZE, NULL); if (ret == -1) err(1, &quot;KVM_GET_VCPU_MMAP_SIZE&quot;); mmap_size = ret; if (mmap_size &lt; sizeof(*run)) errx(1, &quot;KVM_GET_VCPU_MMAP_SIZE unexpectedly small&quot;); // 将 kvm run 与 vcpu 做关联，这样能够获取到kvm的运行时信息 第五步 run = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED, vcpufd, 0); if (!run) err(1, &quot;mmap vcpu&quot;); // 获取特殊寄存器 第六步 ret = ioctl(vcpufd, KVM_GET_SREGS, &amp;sregs); if (ret == -1) err(1, &quot;KVM_GET_SREGS&quot;); // 设置代码段为从地址0处开始，我们的代码被加载到了0x0000的起始位置 sregs.cs.base = 0; sregs.cs.selector = 0; // KVM_SET_SREGS 设置特殊寄存器 ret = ioctl(vcpufd, KVM_SET_SREGS, &amp;sregs); if (ret == -1) err(1, &quot;KVM_SET_SREGS&quot;); // 设置代码的入口地址，相当于32位main函数的地址，这里16位汇编都是由0x1000处开始。 // 如果是正式的镜像，那么rip的值应该是类似引导扇区加载进来的指令 struct kvm_regs regs = &#123; .rip = 0x1000, .rax = 2, // 设置 ax 寄存器初始值为 2 .rbx = 2, // 同理 .rflags = 0x2, // 初始化flags寄存器，x86架构下需要设置，否则会粗错 &#125;; ret = ioctl(vcpufd, KVM_SET_REGS, &amp;regs); if (ret == -1) err(1, &quot;KVM_SET_REGS&quot;); // 开始运行虚拟机，如果是qemu-kvm，会用一个线程来执行这个vCPU，并加载指令 第七步 while (1) &#123; // 开始运行虚拟机 ret = ioctl(vcpufd, KVM_RUN, NULL); if (ret == -1) err(1, &quot;KVM_RUN&quot;); // 获取虚拟机退出原因 switch (run-&gt;exit_reason) &#123; case KVM_EXIT_HLT: puts(&quot;KVM_EXIT_HLT&quot;); return 0; // 汇编调用了 out 指令，vmx 模式下不允许执行这个操作，所以 // 将操作权切换到了宿主机，切换的时候会将上下文保存到VMCS寄存器 // 后面CPU虚拟化会讲到这部分 // 因为虚拟机的内存宿主机能够直接读取到，所以直接在宿主机上获取到 // 虚拟机的输出（out指令），这也是后面PCI设备虚拟化的一个基础，DMA模式的PCI设备 case KVM_EXIT_IO: if (run-&gt;io.direction == KVM_EXIT_IO_OUT &amp;&amp; run-&gt;io.size == 1 &amp;&amp; run-&gt;io.port == 0x3f8 &amp;&amp; run-&gt;io.count == 1) putchar(*(((char *)run) + run-&gt;io.data_offset)); else errx(1, &quot;unhandled KVM_EXIT_IO&quot;); break; case KVM_EXIT_FAIL_ENTRY: errx(1, &quot;KVM_EXIT_FAIL_ENTRY: hardware_entry_failure_reason = 0x%llx&quot;, (unsigned long long)run-&gt;fail_entry.hardware_entry_failure_reason); case KVM_EXIT_INTERNAL_ERROR: errx(1, &quot;KVM_EXIT_INTERNAL_ERROR: suberror = 0x%x&quot;, run-&gt;internal.suberror); default: errx(1, &quot;exit_reason = 0x%x&quot;, run-&gt;exit_reason); &#125; &#125;&#125; 编译并运行这个demo 1234gcc -g demo.c -o demo➜ demo1 ./demo4KVM_EXIT_HLT 另外一个简单的QEMU emulator demoIBM的徐同学有做过介绍，在此基础上我再详细介绍一下qemu-kvm的启动过程。 123456789.globl _start .code16_start: xorw %ax, %ax # 将 ax 寄存器清零loop1: out %ax, $0x10 # 像 0x10 的端口输出 ax 的内容，at&amp;t汇编的操作数和Intel的相反。 inc %ax # ax 值加一 jmp loop1 # 继续循环 这个汇编的作用就是一直不停的向0x10端口输出一字节的值。 从main函数开始说起 12345678910111213141516171819202122232425262728293031int main(int argc, char **argv) &#123; int ret = 0; // 初始化kvm结构体 struct kvm *kvm = kvm_init(); if (kvm == NULL) &#123; fprintf(stderr, &quot;kvm init fauilt\n&quot;); return -1; &#125; // 创建VM，并分配内存空间 if (kvm_create_vm(kvm, RAM_SIZE) &lt; 0) &#123; fprintf(stderr, &quot;create vm fault\n&quot;); return -1; &#125; // 加载镜像 load_binary(kvm); // only support one vcpu now kvm-&gt;vcpu_number = 1; // 创建执行现场 kvm-&gt;vcpus = kvm_init_vcpu(kvm, 0, kvm_cpu_thread); // 启动虚拟机 kvm_run_vm(kvm); kvm_clean_vm(kvm); kvm_clean_vcpu(kvm-&gt;vcpus); kvm_clean(kvm);&#125; 第一步，调用kvm_init() 初始化了 kvm 结构体。先来看看怎么定义一个简单的kvm。 1234567891011121314struct kvm &#123; int dev_fd; // /dev/kvm 的句柄 int vm_fd; // GUEST 的句柄 __u64 ram_size; // GUEST 的内存大小 __u64 ram_start; // GUEST 的内存起始地址， // 这个地址是qemu emulator通过mmap映射的地址 int kvm_version; struct kvm_userspace_memory_region mem; // slot 内存结构，由用户空间填充、 // 允许对guest的地址做分段。将多个slot组成线性地址 struct vcpu *vcpus; // vcpu 数组 int vcpu_number; // vcpu 个数&#125;; 初始化 kvm 结构体。 12345678910111213struct kvm *kvm_init(void) &#123; struct kvm *kvm = malloc(sizeof(struct kvm)); kvm-&gt;dev_fd = open(KVM_DEVICE, O_RDWR); // 打开 /dev/kvm 获取 kvm 句柄 if (kvm-&gt;dev_fd &lt; 0) &#123; perror(&quot;open kvm device fault: &quot;); return NULL; &#125; kvm-&gt;kvm_version = ioctl(kvm-&gt;dev_fd, KVM_GET_API_VERSION, 0); // 获取 kvm API 版本 return kvm;&#125; 第二步+第三步，创建虚拟机，获取到虚拟机句柄，并为其分配内存。 12345678910111213141516171819202122232425262728293031323334353637383940int kvm_create_vm(struct kvm *kvm, int ram_size) &#123; int ret = 0; // 调用 KVM_CREATE_KVM 接口获取 vm 句柄 kvm-&gt;vm_fd = ioctl(kvm-&gt;dev_fd, KVM_CREATE_VM, 0); if (kvm-&gt;vm_fd &lt; 0) &#123; perror(&quot;can not create vm&quot;); return -1; &#125; // 为 kvm 分配内存。通过系统调用. kvm-&gt;ram_size = ram_size; kvm-&gt;ram_start = (__u64)mmap(NULL, kvm-&gt;ram_size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_NORESERVE, -1, 0); if ((void *)kvm-&gt;ram_start == MAP_FAILED) &#123; perror(&quot;can not mmap ram&quot;); return -1; &#125; // kvm-&gt;mem 结构需要初始化后传递给 KVM_SET_USER_MEMORY_REGION 接口 // 只有一个内存槽 kvm-&gt;mem.slot = 0; // guest 物理内存起始地址 kvm-&gt;mem.guest_phys_addr = 0; // 虚拟机内存大小 kvm-&gt;mem.memory_size = kvm-&gt;ram_size; // 虚拟机内存在host上的用户空间地址，这里就是绑定内存给guest kvm-&gt;mem.userspace_addr = kvm-&gt;ram_start; // 调用 KVM_SET_USER_MEMORY_REGION 为虚拟机分配内存。 ret = ioctl(kvm-&gt;vm_fd, KVM_SET_USER_MEMORY_REGION, &amp;(kvm-&gt;mem)); if (ret &lt; 0) &#123; perror(&quot;can not set user memory region&quot;); return ret; &#125; return ret;&#125; 接下来就是load_binary把二进制文件load到虚拟机的内存中来，在第一个demo中我们是直接把字节码放到了内存中，这里模拟镜像加载步骤，把二进制文件加载到内存中。 1234567891011121314151617181920void load_binary(struct kvm *kvm) &#123; int fd = open(BINARY_FILE, O_RDONLY); // 打开这个二进制文件(镜像） if (fd &lt; 0) &#123; fprintf(stderr, &quot;can not open binary file\n&quot;); exit(1); &#125; int ret = 0; char *p = (char *)kvm-&gt;ram_start; while(1) &#123; ret = read(fd, p, 4096); // 将镜像内容加载到虚拟机的内存中 if (ret &lt;= 0) &#123; break; &#125; printf(&quot;read size: %d&quot;, ret); p += ret; &#125;&#125; 加载完镜像后，需要初始化vCPU，以便能够运行镜像内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct vcpu &#123; int vcpu_id; // vCPU id，vCPU int vcpu_fd; // vCPU 句柄 pthread_t vcpu_thread; // vCPU 线程句柄 struct kvm_run *kvm_run; // KVM 运行时结构，也可以看做是上下文 int kvm_run_mmap_size; // 运行时结构大小 struct kvm_regs regs; // vCPU的寄存器 struct kvm_sregs sregs; // vCPU的特殊寄存器 void *(*vcpu_thread_func)(void *); // 线程执行函数&#125;;struct vcpu *kvm_init_vcpu(struct kvm *kvm, int vcpu_id, void *(*fn)(void *)) &#123; // 申请vcpu结构 struct vcpu *vcpu = malloc(sizeof(struct vcpu)); // 只有一个 vCPU，所以这里只初始化一个 vcpu-&gt;vcpu_id = 0; // 调用 KVM_CREATE_VCPU 获取 vCPU 句柄，并关联到kvm-&gt;vm_fd（由KVM_CREATE_VM返回） vcpu-&gt;vcpu_fd = ioctl(kvm-&gt;vm_fd, KVM_CREATE_VCPU, vcpu-&gt;vcpu_id); if (vcpu-&gt;vcpu_fd &lt; 0) &#123; perror(&quot;can not create vcpu&quot;); return NULL; &#125; // 获取KVM运行时结构大小 vcpu-&gt;kvm_run_mmap_size = ioctl(kvm-&gt;dev_fd, KVM_GET_VCPU_MMAP_SIZE, 0); if (vcpu-&gt;kvm_run_mmap_size &lt; 0) &#123; perror(&quot;can not get vcpu mmsize&quot;); return NULL; &#125; printf(&quot;%d\n&quot;, vcpu-&gt;kvm_run_mmap_size); // 将 vcpu_fd 的内存映射给 vcpu-&gt;kvm_run结构。相当于一个关联操作 // 以便能够在虚拟机退出的时候获取到vCPU的返回值等信息 vcpu-&gt;kvm_run = mmap(NULL, vcpu-&gt;kvm_run_mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED, vcpu-&gt;vcpu_fd, 0); if (vcpu-&gt;kvm_run == MAP_FAILED) &#123; perror(&quot;can not mmap kvm_run&quot;); return NULL; &#125; // 设置线程执行函数 vcpu-&gt;vcpu_thread_func = fn; return vcpu;&#125; 最后一步，以上工作就绪后，启动虚拟机。 12345678910111213void kvm_run_vm(struct kvm *kvm) &#123; int i = 0; for (i = 0; i &lt; kvm-&gt;vcpu_number; i++) &#123; // 启动线程执行 vcpu_thread_func 并将 kvm 结构作为参数传递给线程 if (pthread_create(&amp;(kvm-&gt;vcpus-&gt;vcpu_thread), (const pthread_attr_t *)NULL, kvm-&gt;vcpus[i].vcpu_thread_func, kvm) != 0) &#123; perror(&quot;can not create kvm thread&quot;); exit(1); &#125; &#125; pthread_join(kvm-&gt;vcpus-&gt;vcpu_thread, NULL);&#125; 启动虚拟机其实就是创建线程，并执行相应的线程回调函数。线程回调函数在kvm_init_vcpu的时候传入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114void *kvm_cpu_thread(void *data) &#123; // 获取参数 struct kvm *kvm = (struct kvm *)data; int ret = 0; // 设置KVM的参数 kvm_reset_vcpu(kvm-&gt;vcpus); while (1) &#123; printf(&quot;KVM start run\n&quot;); // 启动虚拟机，此时的虚拟机已经有内存和CPU了，可以运行起来了。 ret = ioctl(kvm-&gt;vcpus-&gt;vcpu_fd, KVM_RUN, 0); if (ret &lt; 0) &#123; fprintf(stderr, &quot;KVM_RUN failed\n&quot;); exit(1); &#125; // 前文 kvm_init_vcpu 函数中，将 kvm_run 关联了 vCPU 结构的内存 // 所以这里虚拟机退出的时候，可以获取到 exit_reason，虚拟机退出原因 switch (kvm-&gt;vcpus-&gt;kvm_run-&gt;exit_reason) &#123; case KVM_EXIT_UNKNOWN: printf(&quot;KVM_EXIT_UNKNOWN\n&quot;); break; case KVM_EXIT_DEBUG: printf(&quot;KVM_EXIT_DEBUG\n&quot;); break; // 虚拟机执行了IO操作，虚拟机模式下的CPU会暂停虚拟机并 // 把执行权交给emulator case KVM_EXIT_IO: printf(&quot;KVM_EXIT_IO\n&quot;); printf(&quot;out port: %d, data: %d\n&quot;, kvm-&gt;vcpus-&gt;kvm_run-&gt;io.port, *(int *)((char *)(kvm-&gt;vcpus-&gt;kvm_run) + kvm-&gt;vcpus-&gt;kvm_run-&gt;io.data_offset) ); sleep(1); break; // 虚拟机执行了memory map IO操作 case KVM_EXIT_MMIO: printf(&quot;KVM_EXIT_MMIO\n&quot;); break; case KVM_EXIT_INTR: printf(&quot;KVM_EXIT_INTR\n&quot;); break; case KVM_EXIT_SHUTDOWN: printf(&quot;KVM_EXIT_SHUTDOWN\n&quot;); goto exit_kvm; break; default: printf(&quot;KVM PANIC\n&quot;); goto exit_kvm; &#125; &#125;exit_kvm: return 0;&#125;void kvm_reset_vcpu (struct vcpu *vcpu) &#123; if (ioctl(vcpu-&gt;vcpu_fd, KVM_GET_SREGS, &amp;(vcpu-&gt;sregs)) &lt; 0) &#123; perror(&quot;can not get sregs\n&quot;); exit(1); &#125; // #define CODE_START 0x1000 /* sregs 结构体 x86 struct kvm_sregs &#123; struct kvm_segment cs, ds, es, fs, gs, ss; struct kvm_segment tr, ldt; struct kvm_dtable gdt, idt; __u64 cr0, cr2, cr3, cr4, cr8; __u64 efer; __u64 apic_base; __u64 interrupt_bitmap[(KVM_NR_INTERRUPTS + 63) / 64]; &#125;; */ // cs 为code start寄存器，存放了程序的起始地址 vcpu-&gt;sregs.cs.selector = CODE_START; vcpu-&gt;sregs.cs.base = CODE_START * 16; // ss 为堆栈寄存器，存放了堆栈的起始位置 vcpu-&gt;sregs.ss.selector = CODE_START; vcpu-&gt;sregs.ss.base = CODE_START * 16; // ds 为数据段寄存器，存放了数据开始地址 vcpu-&gt;sregs.ds.selector = CODE_START; vcpu-&gt;sregs.ds.base = CODE_START *16; // es 为附加段寄存器 vcpu-&gt;sregs.es.selector = CODE_START; vcpu-&gt;sregs.es.base = CODE_START * 16; // fs, gs 同样为段寄存器 vcpu-&gt;sregs.fs.selector = CODE_START; vcpu-&gt;sregs.fs.base = CODE_START * 16; vcpu-&gt;sregs.gs.selector = CODE_START; // 为vCPU设置以上寄存器的值 if (ioctl(vcpu-&gt;vcpu_fd, KVM_SET_SREGS, &amp;vcpu-&gt;sregs) &lt; 0) &#123; perror(&quot;can not set sregs&quot;); exit(1); &#125; // 设置寄存器标志位 vcpu-&gt;regs.rflags = 0x0000000000000002ULL; // rip 表示了程序的起始指针，地址为 0x0000000 // 在加载镜像的时候，我们直接将binary读取到了虚拟机的内存起始位 // 所以虚拟机开始的时候会直接运行binary vcpu-&gt;regs.rip = 0; // rsp 为堆栈顶 vcpu-&gt;regs.rsp = 0xffffffff; // rbp 为堆栈底部 vcpu-&gt;regs.rbp= 0; if (ioctl(vcpu-&gt;vcpu_fd, KVM_SET_REGS, &amp;(vcpu-&gt;regs)) &lt; 0) &#123; perror(&quot;KVM SET REGS\n&quot;); exit(1); &#125;&#125; 运行一下结果，可以看到当虚拟机执行了指令 out %ax, $0x10 的时候，会引起虚拟机的退出，这是CPU虚拟化里面将要介绍的特殊机制。宿主机获取到虚拟机退出的原因后，获取相应的输出。这里的步骤就类似于IO虚拟化，直接读取IO模块的内存，并输出结果。 123456789101112131415161718➜ kvmsample git:(master) ✗ ./kvmsampleread size: 712288KVM start runKVM_EXIT_IOout port: 16, data: 0KVM start runKVM_EXIT_IOout port: 16, data: 1KVM start runKVM_EXIT_IOout port: 16, data: 2KVM start runKVM_EXIT_IOout port: 16, data: 3KVM start runKVM_EXIT_IOout port: 16, data: 4... 总结虚拟机的启动过程基本上可以这么总结：创建kvm句柄-&gt;创建vm-&gt;分配内存-&gt;加载镜像到内存-&gt;启动线程执行KVM_RUN。从这个虚拟机的demo可以看出，虚拟机的内存是由宿主机通过mmap调用映射给虚拟机的，而vCPU是宿主机的一个线程，这个线程通过设置相应的vCPU的寄存器指定了虚拟机的程序加载地址后，开始运行虚拟机的指令，当虚拟机执行了IO操作后，CPU捕获到中断并把执行权又交回给宿主机。 当然真实的qemu-kvm比这个复杂的多，包括设置很多IO设备的MMIO，设置信号处理等。 源代码本文中提到的所有源代码都可以从这里下载到，仅供大家学习交流使用github|kvm-cheat]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟化原理1 -- 概述]]></title>
    <url>%2F2018%2F12%2F10%2Fkvm-overview%2F</url>
    <content type="text"><![CDATA[KVM虚拟化简介KVM 全称 kernel-based virtual machine，由Qumranet公司发起，2008年被RedHat收购。KVM实现主要基于Intel-V或者AMD-V提供的虚拟化平台，利用Linux进程模拟虚拟机CPU和内存等。KVM不提供硬件虚拟化操作，其IO操作等都借助QEMU来完成。 Qemu 是纯软件实现的虚拟化模拟器，几乎可以模拟任何硬件设备，我们最熟悉的就是能够模拟一台能够独立运行操作系统的虚拟机，虚拟机认为自己和硬件打交道，但其实是和 Qemu 模拟出来的硬件打交道，Qemu 将这些指令转译给真正的硬件。 正因为 Qemu 是纯软件实现的，所有的指令都要经 Qemu 过一手，性能非常低，所以，在生产环境中，大多数的做法都是配合 KVM 来完成虚拟化工作，因为 KVM 是硬件辅助的虚拟化技术，主要负责 比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化，两者合作各自发挥自身的优势，相得益彰。 KVM有如下特点： guest作为一个普通进程运行于宿主机 guest的CPU(vCPU)作为进程的线程存在，并受到宿主机内核的调度 KVM整体架构 虚拟CPU虚拟机所有用户级别(user)的指令集，都会直接由宿主机线程执行，此线程会调用KVM的ioctl方式提供的接口加载guest的指令并在特殊的CPU模式下运行，不需要经过CPU指令集的软件模拟转换，大大的减少了虚拟化成本，这也是KVM优于其他虚拟化方式的点之一。 KVM向外提供了一个虚拟设备/dev/kvm，通过ioctl(IO设备带外管理接口）来对KVM进行操作，包括虚拟机的初始化，分配内存，指令加载等等。 虚拟IO设备guest作为一个进程存在，当然他的内核的所有驱动等都存在，只是硬件被QEMU所模拟。guest的所有虚拟的硬件操作都会有QEMU来接管，那些由host passthrough给guest的设备除外，QEMU负责与真实的宿主机硬件打交道。 虚拟内存guest的内存在host上由emulator提供，对emulator来说，guest访问的内存就是他的虚拟地址空间，guest上需要经过一次虚拟地址到物理地址的转换，转换到guest的物理地址其实也就是emulator的虚拟地址，emulator再次经过一次转换，转换为host的物理地址。 # 虚拟化概述 ## CPU虚拟化 ### 指令的模拟 #### 陷入（利用处理器的保护机制，中断和异常） 1，基于处理器保护机制出发的异常 2，虚拟机主动触发的异常 3，异步zhognduan ##### 虚拟处理器 ##### 虚拟寄存器 ##### 上下文 ### 中断和异常的虚拟化 ### 对称对处理器技术的虚拟化（SMP） #### VMM选择第一个虚拟处理器，BSP #### 其他虚拟处理器，AP ## Memory虚拟化 ### 物理地址从0开始 ### 内存地址连续 ## I/O虚拟化 ### 设备发现 #### 总线类型的设备 ##### 总线类型不可枚举 ###### ISA设备 ###### PS/2键盘、鼠标、RTC ###### 传统IDE控制器 ##### 总线类型可枚举、资源可配置 ###### PCI #### 完全模拟的设备 ##### Frontend / backend 模型 ### 访问截获 #### I/O端口的访问 ##### I/O位图来决定 #### MMIO访问 ##### 页表项设置为无效 ### 设备模拟]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMP 简介]]></title>
    <url>%2F2018%2F12%2F10%2Fqmp-introduction%2F</url>
    <content type="text"><![CDATA[什么是QMP协议QMP，即QEMU Machine Protocol，就是qemu虚拟机中的一种协议，是qemu的一部分。qmp是基于json格式的一整套协议，通过这套协议我们可以控制qemu虚拟机实例的整个生命周期，包括挂起、暂停、快照、查询、外设的热插拔等，以及最简单的查询，都可以通过qmp实现。 有多种方法使用qmp，这里简要介绍通过tcp和unix socket使用qmp。 QMP协议有哪些特征1）轻量、基于文本、指令格式易于解析，因为它是json格式的；2）支持异步消息，主要指通过qmp发送给虚拟机的指令支持异步；3）Capabilities Negotiation，主要指我们初次建立qmp连接时，进入了capabilities negotiation模式,这时我们不能发送任何指令，除了qmp_capabilities指令，发送了qmp_capabilitie指令，我们就退出了capabilities negotiation模式，进入了指令模式（command mode），这时我们可以发送qmp指令，如{ “execute”: “query-status” }，这样就可以查询虚拟机的状态。 QMP协议有哪些模式 有两种模式：Capabilities Negotiation模式和Command模式。 那么该如何建立qmp连接呢这里简要介绍通过tcp和unix socket使用qmp。 通过TCP使用QMP使用-qmp添加qmp相关参数： 1./qemu-system-x86_64 -m 2048 -hda /root/centos6.img -enable-kvm -qmp tcp:localhost:1234,server,nowait 新开一个终端使用telnet 链接localhost：1234 1telnet localhost 1234 之后就可以使用qmp的命令和虚拟机交互了 123456789[root@localhost ~]# telnet localhost 1234Trying ::1...Connected to localhost.Escape character is &apos;^]&apos;.&#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;micro&quot;: 0, &quot;minor&quot;: 6, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot;&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;&#123; &quot;execute&quot;: &quot;qmp_capabilities&quot; &#125;&#123;&quot;return&quot;: &#123;&#125;&#125;&#123; &quot;execute&quot;: &quot;query-status&quot; &#125;&#123;&quot;return&quot;: &#123;&quot;status&quot;: &quot;running&quot;, &quot;singlestep&quot;: false, &quot;running&quot;: true&#125;&#125; 通过unix socket使用QMP使用unix socket创建qmp： 1./qemu-system-x86_64 -m 2048 -hda /root/centos6.img -enable-kvm -qmp unix:/tmp/qmp-test,server,nowait 使用nc连接该socket: 1nc -U /tmp/qmp-test 之后就一样了。 123456[root@localhost qmp]# nc -U /tmp/qmp-test&#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;micro&quot;: 0, &quot;minor&quot;: 6, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot;&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;&#123; &quot;execute&quot;: &quot;qmp_capabilities&quot; &#125;&#123;&quot;return&quot;: &#123;&#125;&#125;&#123; &quot;execute&quot;: &quot;query-status&quot; &#125;&#123;&quot;return&quot;: &#123;&quot;status&quot;: &quot;running&quot;, &quot;singlestep&quot;: false, &quot;running&quot;: true&#125;&#125; QMP的详细命令格式可以在qemu的代码树主目录下面的qmp-commands.hx中找到。 自动批量发送QMP命令可以通过下面这个脚本给QEMU虚拟机发送命令。这对于测试虚拟机的一些功能是很有用的。试了一下，对于unix socket的方法能使用的，对于tcp连接的方法没有使用成功。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135# QEMU Monitor Protocol Python class## Copyright (C) 2009 Red Hat Inc.## This work is licensed under the terms of the GNU GPL, version 2. See# the COPYING file in the top-level directory.import socket, json, time, commandsfrom optparse import OptionParserclass QMPError(Exception): passclass QMPConnectError(QMPError): passclass QEMUMonitorProtocol: def connect(self): print self.filename self.sock.connect(self.filename) data = self.__json_read() if data == None: raise QMPConnectError if not data.has_key(&apos;QMP&apos;): raise QMPConnectError return data[&apos;QMP&apos;][&apos;capabilities&apos;] def close(self): self.sock.close() def send_raw(self, line): self.sock.send(str(line)) return self.__json_read() def send(self, cmdline, timeout=30, convert=True): end_time = time.time() + timeout if convert: cmd = self.__build_cmd(cmdline) else: cmd = cmdline print(&quot;*cmdline = %s&quot; % cmd) print cmd self.__json_send(cmd) while time.time() &lt; end_time: resp = self.__json_read() if resp == None: return (False, None) elif resp.has_key(&apos;error&apos;): return (False, resp[&apos;error&apos;]) elif resp.has_key(&apos;return&apos;): return (True, resp[&apos;return&apos;]) def read(self, timeout=30): o = &quot;&quot; end_time = time.time() + timeout while time.time() &lt; end_time: try: o += self.sock.recv(1024) if len(o) &gt; 0: break except: time.sleep(0.01) if len(o) &gt; 0: return json.loads(o) else: return None def __build_cmd(self, cmdline): cmdargs = cmdline.split() qmpcmd = &#123; &apos;execute&apos;: cmdargs[0], &apos;arguments&apos;: &#123;&#125; &#125; for arg in cmdargs[1:]: opt = arg.split(&apos;=&apos;) try: value = int(opt[1]) except ValueError: value = opt[1] qmpcmd[&apos;arguments&apos;][opt[0]] = value print(&quot;*cmdline = %s&quot; % cmdline) return qmpcmd def __json_send(self, cmd): # XXX: We have to send any additional char, otherwise # the Server won&apos;t read our input self.sock.send(json.dumps(cmd) + &apos; &apos;) def __json_read(self): try: return json.loads(self.sock.recv(1024)) except ValueError: return def __init__(self, filename, protocol=&quot;tcp&quot;): if protocol == &quot;tcp&quot;: self.filename = (&quot;localhost&quot;, int(filename)) self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) elif protocol == &quot;unix&quot;: self.filename = filename print self.filename self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) #self.sock.setblocking(0) self.sock.settimeout(5)if __name__ == &quot;__main__&quot;: parser = OptionParser() parser.add_option(&apos;-n&apos;, &apos;--num&apos;, dest=&apos;num&apos;, default=&apos;10&apos;, help=&apos;Times want to try&apos;) parser.add_option(&apos;-f&apos;, &apos;--file&apos;, dest=&apos;port&apos;, default=&apos;4444&apos;, help=&apos;QMP port/filename&apos;) parser.add_option(&apos;-p&apos;, &apos;--protocol&apos;, dest=&apos;protocol&apos;,default=&apos;tcp&apos;, help=&apos;QMP protocol&apos;) def usage(): parser.print_help() sys.exit(1) options, args = parser.parse_args() print options if len(args) &gt; 0: usage() num = int(options.num) qmp_filename = options.port qmp_protocol = options.protocol qmp_socket = QEMUMonitorProtocol(qmp_filename,qmp_protocol) qmp_socket.connect() qmp_socket.send(&quot;qmp_capabilities&quot;) qmp_socket.close()###########################################################Usage#Options:# -h, --help show this help message and exit# -n NUM, --num=NUM Times want to try# -f PORT, --file=PORT QMP port/filename# -p PROTOCOL, --protocol=PROTOCOL# QMP protocol# e.g: # python xxxxx.py -n $NUM -f $PORT########################################################## 参考文档关于QMP更详细的文档，可以参考其官方文档：https://wiki.qemu.org/Documentation/QMP]]></content>
      <categories>
        <category>QEMU</category>
      </categories>
      <tags>
        <tag>QEMU</tag>
        <tag>QMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+next主题]]></title>
    <url>%2F2018%2F11%2F26%2Fhexo-next%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[这篇内容详细记述了我在使用hexo搭载博客的过程中走过的路和跌过的坑。另外，我搭建了一个新的博客作为自己的技术博客，地址是xuquan.site，欢迎来逛逛~ 从印象笔记到简书到Hexo我一直有收集资料的习惯，最开始把资料都放在印象笔记里，然后自己平时处理消化之后会添加一个Learning Card作为资料开头，方便自己复习和记忆。但是时间一久，资料就特别多，加上处理过和没处理过的都积攒在一起就显得特别臃肿，于是我就考虑将消化过的内容发布到简书上，给自己做一个记录，也算是自己的技术博客。 但使用了简书3个月之后，我就发现了一些问题： 首先，我是用Typora来写内容的，简书虽然支持Markdown，但是自带的编辑器功能不是特别完善，有些时候还得反复切换Markdown和富文本模式，很麻烦； 其次，直接复制Typora的内容到简书是无法同步图片的，因为Typora的图片是放在文件夹内的asset文件夹内的，复制到简书需要图片挨个重新上传，图片多的时候简直想放弃这一篇内容； 另外，简书无法添加标签，只能分笔记本来写不同的内容，而且也不能添加置顶，功能比较单一； 在综合考虑之后，我决定搭建一个自己的博客。正好看到有人推荐hexo搭建，而且大家搭建的博客都挺赏心悦目的，我就开始动手搭建自己的了。 Hexo部署hexo有中文的文档，这一点非常方便，但是在安装过程中还是很容易有疏忽的地方，导致安装失败。 安装前提安装Hexo之前，必须保证自己的电脑中已经安装好了Node.js和Git。因为这两个软件我之前都安装过，这里就不重复安装过程了，检验方式如下： image-20180809141924679安装Hexo安装好node.js和git后，可以通过npm来安装Hexo。 npm install -g hexo-cli建站之后就可以在电脑里新建一个文件夹来作为存放博客全部内容的大本营了。我们直接用hexo命令来初始化博客文件夹： hexo init cd npm install 就是文件夹的名字，我们可以自己随意取这个名字，我的经验是，现在初始化应该不需要后面npm install这个步骤了，在创建的时候 ，文件夹初始化已经把需要的内容都下载进去了。 文件夹开始初始化了站内内容新建好的文件夹目录如下： .├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes这里解释一下各个文件夹的作用： config.yml博客的配置文件，博客的名称、关键词、作者、语言、博客主题…设置都在里面。 package.json应用程序信息，新添加的插件内容也会出现在这里面，我们可以不修改这里的内容。 scaffoldsscaffolds就是脚手架的意思，这里放了三个模板文件，分别是新添加博客文章（posts）、新添加博客页（page）和新添加草稿（draft）的目标样式。 这部分可以修改的内容是，我们可以在模板上添加比如categories等自定义内容 sourcesource是放置我们博客内容的地方，里面初始只有两个文件夹，一个是drafts（草稿），一个posts（文章），但之后我们通过命令新建tags（标签）还有categories（分类）页后，这里会相应地增加文件夹。 themes放置主题文件包的地方。Hexo会根据这个文件来生成静态页面。 初始状态下只有landscape一个文件夹，后续我们可以添加自己喜欢的。 Hexo命令init新建一个网站。 hexo init new新建文章或页面。 hexo new “title”这里的对应我们要添加的内容，如果是posts就是添加新的文章，如果是page就是添加新的页面。 默认是添加posts。 然后我们就可以在对应的posts或drafts文件夹里找到我们新建的文件，然后在文件里用Markdown的格式来写作了。 generate生成静态页面 hexo generate也可以简写成 hexo gdeploy将内容部署到网站 hexo deploy也可以简写成 hexo -dpublish发布内容，实际上是将内容从drafts（草稿）文件夹移到posts（文章）文件夹。 hexo publish server启动服务器，默认情况下，访问网站为http://localhost:4000/ hexo server也可以简写成 hexo s根据我的经验，除了第一次部署的时候，我们会重点用到hexo init这个命令外，在平时写博客和发布过程中最常用的就是： hexo n 新建文章hexo s 启动服务器，在本地查看内容hexo g 生成静态页面hexo deploy 部署到网站以上四个步骤。 其实以上命令我觉得就足够了，文档里还有很多功能，但我在实际使用的过程中都还没有遇到。 搭建好后我们在localhost:4000就可以看到这样的博客内容： image-20180809152743968实际操作我在新建博客之后，做了以下改动： 创建“分类”页面新建分类页面 hexo new page categories给分类页面添加类型 我们在source文件夹中的categories文件夹下找到index.md文件，并在它的头部加上type属性。 title: 文章分类date: 2017-05-27 13:47:40 type: “categories” #这部分是新添加的给模板添加分类属性 现在我们打开scarffolds文件夹里的post.md文件，给它的头部加上categories:，这样我们创建的所有新的文章都会自带这个属性，我们只需要往里填分类，就可以自动在网站上形成分类了。 title: hexo+next主题date: 1543200599000categories:tags:给文章添加分类 现在我们可以找到一篇文章，然后尝试给它添加分类 layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: [node.js, express] 创建“标签”页面创建”标签”页的方式和创建“分类”一样。 新建“标签”页面 hexo new page tags给标签页面添加类型 我们在source文件夹中的tags文件夹下找到index.md文件，并在它的头部加上type属性。 title: tagsdate: 2018-08-06 22:48:29type: “tags” #新添加的内容给文章添加标签 有两种写法都可以，第一种是类似数组的写法，把标签放在中括号[]里，用英文逗号隔开 layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: [node.js, express]第二种写法是用-短划线列出来 layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: node.js express部署域名紧接着我们就可以把这些内容添加到Github页面上，然后生成我们自己的博客了。 部署Github首先你必须有一个github账号 然后新建一个仓库，这一有第一个坑，我之前用了hexoblog来作为项目名称，一直没能搭建成功，后来看到其他大牛的经验，才发现项目名一定要是用户名.github.io的形式(README.md可选可不选) image-20180809153134467然后在setting里添加生成页面的选项 image-20180809153304980image-20180809153343362这个时候github页面其实就生成好了，但是我们的内容还需要同步到github上，所以打开hexo文件夹里的配置文件config.yml，添加部署路径 image-20180809153610047这里注意两小点： 属性和内容之间一定要有一个空格，配置文件有自己的格式规范如果你之前没有用git关联过自己的github库，需要配置SSH等参数，否则无法成功，这部分搜git就有很多相关教程我们再用hexo g &amp;&amp; hexo deploy就能将内容推送到github上了，在github页面上也能看到自己的内容了 image-20180809153933270部署自己的域名首先我们需要获取一个域名，我是在阿里云上购买了，上面可以根据自己想要的内容搜，比如我用了自己的名字，推荐给你的域名根据后缀不同会有价格上的区别，我选了一个不太贵的； 购买域名之后需要实名认证，这是另一个坑，我之前不知道实名认证审核完成前域名无法用，一直以为自己搭建失败了； 认证成功后需要解析域名 image-20180809154942783image-20180809155013659记录类型选CNAME，记录值是自己github生成页面的地址。 在博客的页面添加CNAME文件，并在里面记录自己域名的地址，将这个文件放在public文件夹下 这里还有一个小坑，CNAME文件经常被覆盖，导致我们重新部署博客后，链接就不可用了，这里可以下载一个叫hexo-generator-cname的插件，这样它会自动搞定CNAME的问题，只需要第一次手动将域名添加到文件里即可 npm i hexo-generator-cname –save最后hexo g &amp;&amp; hexo deploy就可以了 NexT主题hexo有很多开源的主题，我选了NexT，开始只是觉得很简洁清爽，后来发现它的功能挺齐全的，提前解决了很多搭建过程中会遇到的问题。这里强烈推荐一下。 首先，NexT也有中文文档，然后我们就可以开始了。 安装我是用的git clone的方法，文档中还有其他方法 $ git clone https://github.com/iissnan/hexo-theme-next themes/next设置主题在hexo根目录下的配置文件config.yml里设置主题 theme: next配置主题接下来我们就可以来按需配置主题内容了，所有内容都在themes/next文件夹下的config.yml文件里修改。 官方文档里写的是有些配置需要将一部分代码添加到配置文件中，但其实不用，我们逐行看配置文件就会发现，有很多功能都已经放在配置文件里了，只是注释掉了，我们只需要取消注释，把需要的相关信息补全即可使用 菜单栏 menu原生菜单栏有主页、关于、分类、标签等数个选项，但是在配置文件中是注释掉的状态，这里我们自行修改注释就行 menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive schedule: /schedule/ || calendarsitemap: /sitemap.xml || sitemapcommonweal: /404/ || heartbeat注意点： 如果事先没有通过hexo new page 来创建页面的话，即使在配置文件中取消注释，页面也没法显示我们也可以添加自己想要添加的页面，不用局限在配置文件里提供的选择里||后面是fontAwesome里的文件对应的名称menu_icons记得选enable: true（默认应该是true）我在这部分添加了两个自定义的页面，后面在第三方插件部分我会再提到。 menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th 读书: /books || book 电影: /movies || film archives: /archives/ || archive schedule: /schedule/ || calendarsitemap: /sitemap.xml || sitemapcommonweal: /404/ || heartbeat主题风格 schemes主题提供了4个，我们把想要选择的取消注释，其他三个保持注释掉的状态即可。 Muse image-20180809164700600Mist image-20180809164749052Pisces image-20180809164925685Gemini image-20180809165023401选择主题后也可以自定义，不过我还没摸清楚有哪些地方可以自定义，等弄清楚了我再来更新。 底部建站时间和图标修改修改主题的配置文件： footer: Specify the date when the site was setup.If not defined, current year will be used. since: 2018 Icon between year and copyright info. icon: snowflake-o If not defined, will be used author from Hexo main config. copyright: ————————————————————-Hexo link (Powered by Hexo). powered: false theme: # Theme &amp; scheme info link (Theme - NexT.scheme). enable: false # Version info of NexT after scheme info (vX.X.X). # version: false 我在这部分做了这样几件事： 把用户的图标从小人user改成了雪花snowflake-ocopyright留空，显示成页面author即我的名字powered: false把hexo的授权图片取消了theme: enable:false 把主题的内容也取消了这样底部信息比较简单。 image-20180809172835606个人社交信息 social在social里我们可以自定义自己想要在个人信息部分展现的账号，同时给他们加上图标。 social: GitHub: https://github.com/XuQuan-nikkkki || github E-Mail: mailto:xuquan1225@hotmail.com || envelope #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook注意点： ||后面对应的名称是fontAwesome里图标的名称，如果我们选择的账号没有对应的图标（如豆瓣、知乎），我们可以在fontAwesome库里去选择自己喜欢的图标建议不要找太新的fontAwesome图标，主题关联的库版本没有那么新，很可能显示不了或者显示一个地球网站动画效果为了网站响应速度我们可以把网站的动画关掉 motion: enable: false但我觉得页面比较素，所以开了动画，选择了canvas-nest这一个，主题自带四种效果，可以选自己喜欢的。 motion: enable: true async: true Canvas-nestcanvas_nest: true three_wavesthree_waves: false canvas_linescanvas_lines: false canvas_spherecanvas_sphere: false评论系统NexT原生支持多说、Disqus、hypercomments等多种评论系统。我选择了Disqus。 方法也非常简单。直接去Disqus注册，注册完了在配置的时候会给你一个名为shortname的ID，将这个ID填在配置文件里即可。 Disqusdisqus: enable: true shortname: xuquan count: true统计文章字数和阅读时间post_wordcount: item_text: true wordcount: true # 文章字数 min2read: true # 阅读时间 totalcount: true # 总共字数 separated_meta: true统计阅读次数这里我用的是leancloud的服务，具体方法参考NexT上的教程,添加完之后效果如下： image-20180809175133462第三方插件Hexo-adminHexo-admin插件允许我们直接在本地页面上修改文章内容。 下载 npm i hexo-admin –save登录http://localhost:4000/admin即可看到我们所有的文章内容，并且在可视化界面中操作文章内容 Hexo-doubanhexo-douban插件可以在博客中添加豆瓣电影、读书和游戏页面，关联我们自己的账号。 下载 npm install hexo-douban –save配置 在hexo根目录下的config.yml文件中添加如下内容 douban: user: builtin: false book: title: ‘This is my book title’ quote: ‘This is my book quote’ movie: title: ‘This is my movie title’ quote: ‘This is my movie quote’ game: title: ‘This is my game title’ quote: ‘This is my game quote’ timeout: 10000title和quote后面的内容会分别作为电影/读书/游戏页面的标题和副标题（引言）呈现在博客里。 user就写我们豆瓣的id，可以在“我的豆瓣”页面中找到，builtin指是否将生成页面功能嵌入hexo s和hexo g中，建议选false，因为true会导致页面每次启动本地服务器都需要很长时间生成豆瓣页面，长到怀疑人生。 生成页面 hexo douban #生成读书、电影、游戏三个页面hexo douban -b #生成读书页面hexo douban -m #生成电影页面hexo douban -g #生成游戏页面在博客中生成页面 这里就需要用到我们前面提过的hexo new命令了。 hexo new page bookshexo new page movieshexo new page games在博客中添加页面 在menu部分添加我们需要添加的页面名称和相对路径 menu: Home: / Archives: /archives Books: /books #This is your books page Movies: /movies #This is your movies page Games: /games #This is your games page部署到博客 hexo g &amp;&amp; hexo deploy我踩过的坑iPic图片上传hexo博客发布Typora写好的内容也会出现图片无法同步的问题，网上有大佬给出的解决方案是使用hexo-asset-image插件，这样在创建博客时会有一个与.md文件同名的文件夹，将图片同步到文件夹内即可。 但时间下来还是比较麻烦，因为Typora并没有自定义图片路径的功能，它会放在与文件相关的asset文件夹内。 我找到的最终方案是使用Typora自带的一个功能：图片上传iPic图床。这样在添加图片的时候，图片链接就自动更换成了图床的地址，这时同步到博客就没有问题了。 评论系统因为多说已经停止服务了，最开始看到有人说Disqus得翻墙，就选了一个韩国的评论服务，叫来必力，但事实证明墙外就没有稳定的服务，在我挂VPN的情况下也要加载好半天，后来就还是换成了Disqus，具体配置方法看前文。]]></content>
  </entry>
  <entry>
    <title><![CDATA[KVM源代码分析4:内存虚拟化]]></title>
    <url>%2F2014%2F12%2F11%2Fkvm-src-4-mem%2F</url>
    <content type="text"><![CDATA[代码版本：https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git v3.16.37 在虚拟机的创建与运行中pc_init_pci负责(“KVM源代码分析2:虚拟机的创建与运行”)，内存初始化也是在这里完成的，还是一步步从qemu说起，在vl.c的main函数中有ram_size参数，由qemu入参标识QEMU_OPTION_m设定，顾名思义就是虚拟机内存的大小，通过machine-&gt;init一步步传递给pc_init1函数。在这里分出了above_4g_mem_size和below_4g_mem_size，即高低端内存（也不一定是32bit机器..），然后开始初始化内存，即pc_memory_init，内存通过memory_region_init_ram下面的qemu_ram_alloc分配，使用qemu_ram_alloc_from_ptr。 插播qemu对内存条的模拟管理，是通过RAMBlock和ram_list管理的，RAMBlock就是每次申请的内存池，ram_list则是RAMBlock的链表，他们结构如下： typedefstruct RAMBlock { //对应宿主的内存地址 uint8_t *host; //block在ramlist中的偏移 ram_addr_t offset; //block长度 ram_addr_t length; uint32_t flags; //block名字 char idstr[256]; QLIST_ENTRY(RAMBlock) next; \#if defined(__linux__) && !defined(TARGET_S390X) int fd; \#endif } RAMBlock; typedef struct RAMList { //看代码理解就是list的head，但是不知道为啥叫dirty... uint8_t *phys_dirty; QLIST_HEAD(ram, RAMBlock) blocks; } RAMList; 下面再回到qemu_ram_alloc_from_ptr函数，使用find_ram_offset赋值给new block的offset，find_ram_offset具体工作模型已经在”KVM源代码分析2:虚拟机的创建与运行”，不赘述。然后是一串判断，在kvm_enabled的情况下使用new_block-&gt;host = kvm_vmalloc(size)，最终内存是qemu_vmalloc分配的，使用qemu_memalign干活。 void \*qemu_memalign(size_t alignment, size_t size){ void *ptr; //使用posix进行内存针对页大小对齐 \#if defined(_POSIX_C_SOURCE) && !defined(__sun__) int ret; ret = posix_memalign(&ptr, alignment, size); if (ret != 0) { fprintf(stderr, "Failed to allocate %zu B: %sn", size, strerror(ret)); abort(); } \#elif defined(CONFIG_BSD) ptr = qemu_oom_check(valloc(size)); \#else //所谓检查oom就是看memalign对应malloc申请内存是否成功 ptr = qemu_oom_check(memalign(alignment, size)); \#endif trace_qemu_memalign(alignment, size, ptr); return ptr; } 以上qemu_vmalloc进行内存申请就结束了。在qemu_ram_alloc_from_ptr函数末尾则是将block添加到链表，realloc整个ramlist，用memset初始化整个ramblock，madvise对内存使用限定。然后一层层的退回到pc_memory_init函数。 此时pc.ram已经分配完成，ram_addr已经拿到了分配的内存地址，MemoryRegion ram初始化完成。下面则是对已有的ram进行分段，即ram-below-4g和ram-above-4g，也就是高端内存和低端内存。用memory_region_init_alias初始化子MemoryRegion，然后将memory_region_add_subregion添加关联起来，memory_region_add_subregion具体细节“KVM源码分析2”中已经说了，参考对照着看吧，中间很多映射代码过程也只是qemu遗留的软件实现，没看到具体存在的意义，直接看到kvm_set_user_memory_region函数，内核真正需要kvm_vm_ioctl传递过去的参数是什么， struct kvm_userspace_memory_region mem而已，也就是 struct kvm_userspace_memory_region { __u32 slot; __u32 flags; __u64 guest_phys_addr; __u64 memory_size; /* bytes */ __u64 userspace_addr; /* start of the userspace allocated memory */ }; kvm_vm_ioctl进入到内核是在KVM_SET_USER_MEMORY_REGION参数中，即执行kvm_vm_ioctl_set_memory_region，然后一直向下，到kvm_set_memory_region函数，check_memory_region_flags检查mem-&gt;flags是否合法，而当前flag也就使用了两位，KVM_MEM_LOG_DIRTY_PAGES和KVM_MEM_READONLY，从qemu传递过来只能是KVM_MEM_LOG_DIRTY_PAGES,下面是对mem中各参数的合规检查，(mem-&gt;memory_size &amp; (PAGE_SIZE - 1))要求以页为单位，(mem-&gt;guest_phys_addr &amp; (PAGE_SIZE - 1))要求guest_phys_addr页对齐，而((mem-&gt;userspace_addr &amp; (PAGE_SIZE - 1)) || !access_ok(VERIFY_WRITE,(void user *)(unsigned long)mem-&gt;userspace_addr,mem-&gt;memory_size))则保证host的线性地址页对齐而且该地址域有写权限。id_to_memslot则是根据qemu的内存槽号得到kvm结构下的内存槽号，转换关系来自id_to_index数组，那映射关系怎么来的，映射关系是一一对应的，在kvm_create_vm “KVM源代码分析2:虚拟机的创建与运行”中，kvm_init_memslots_id初始化对应关系，即slots-&gt;id_to_index[i] = slots-&gt;memslots[i].id = i，当前映射是没有意义的，估计是为了后续扩展而存在的。扩充了new的kvm_memory_slot，下面直接在代码中注释更方便： //映射内存有大小，不是删除内存条if (npages) { //内存槽号没有虚拟内存条，意味内存新创建if (!old.npages) change = KVM_MR_CREATE; else { /* Modify an existing slot. */ //修改已存在的内存修改标志或者平移映射地址 //下面是不能处理的状态（内存条大小不能变，物理地址不能变，不能修改只读） if ((mem->userspace_addr != old.userspace_addr) || (npages != old.npages) || ((new.flags ^ old.flags) & KVM_MEM_READONLY)) goto out; //guest地址不同，内存条平移 if (base_gfn != old.base_gfn) change = KVM_MR_MOVE; else if (new.flags != old.flags) //修改属性 change = KVM_MR_FLAGS_ONLY; else { /* Nothing to change. */ r = 0; goto out; } } } else if (old.npages) { //申请插入的内存为0，而内存槽上有内存，意味删除 change = KVM_MR_DELETE; } else /* Modify a non-existent slot: disallowed. */ goto out; 另外看kvm_mr_change就知道memslot的变动值了： enum kvm_mr_change { KVM_MR_CREATE, KVM_MR_DELETE, KVM_MR_MOVE, KVM_MR_FLAGS_ONLY, }; 在往下是一段检查 if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) { /* Check for overlaps */ r = -EEXIST; kvm_for_each_memslot(slot, kvm->memslots) { if ((slot->id >= KVM_USER_MEM_SLOTS) || //下面排除掉准备操作的内存条，在KVM_MR_MOVE中是有交集的 (slot->id == mem->slot)) continue; //下面就是当前已有的slot与new在guest线性区间上有交集 if (!((base_gfn + npages base_gfn) || (base_gfn >= slot->base_gfn + slot->npages))) goto out; //out错误码就是EEXIST } } 如果是新插入内存条，代码则走入kvm_arch_create_memslot函数，里面主要是一个循环，KVM_NR_PAGE_SIZES是分页的级数，此处是3，第一次循环，lpages = gfn_to_index(slot-&gt;base_gfn + npages - 1,slot-&gt;base_gfn, level) + 1，lpages就是一级页表所需要的page数，大致是npages&gt;&gt;09,然后为slot-&gt;arch.rmap[i]申请了内存空间，此处可以猜想，rmap就是一级页表了，继续看，lpages约为npages&gt;&gt;19,此处又多为lpage_info申请了同等空间，然后对lpage_info初始化赋值，现在看不到lpage_info的具体作用，看到后再补上。整体上看kvm_arch_create_memslot做了一个3级的软件页表。如果有脏页,并且脏页位图为空,则分配脏页位图, kvm_create_dirty_bitmap实际就是”页数/8”. if ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) { if (kvm_create_dirty_bitmap(&new) < 0) goto out_free; } 当内存条的改变是KVM_MR_DELETE或者KVM_MR_MOVE,先申请一个slots,把kvm-&gt;memslots暂存到这里,首先通过id_to_memslot获取准备插入的内存条对应到kvm的插槽是slot,无论删除还是移动,将其先标记为KVM_MEMSLOT_INVALID,然后是install_new_memslots,其实就是更新了一下slots-&gt;generation的值。 内存的添加说完了，看一下EPT页表的映射，在kvm_arch_vcpu_setup中有kvm_mmu_setup，是mmu的初始化，EPT的初始化是init_kvm_tdp_mmu，所谓的初始化就是填充了vcpu-&gt;arch.mmu结构体，里面有很多回调函数都会用到，最终的是tdp_page_fault。 context->page_fault = tdp_page_fault; context->sync_page = nonpaging_sync_page; context->invlpg = nonpaging_invlpg; context->update_pte = nonpaging_update_pte; context->shadow_root_level = kvm_x86_ops->get_tdp_level(); context->root_hpa = INVALID_PAGE; context->direct_map = true; context->set_cr3 = kvm_x86_ops->set_tdp_cr3; context->get_cr3 = get_cr3; context->get_pdptr = kvm_pdptr_read; context->inject_page_fault = kvm_inject_page_fault; 当guest访问物理内存时发生vm-exit，进入vmx_handle_exit函数，根据EXIT_REASON_EPT_VIOLATION走到handle_ept_violation函数，exit_qualification = vmcs_readl(EXIT_QUALIFICATION)获取vm-exit的退出原因，进入kvm_mmu_page_fault函数：vcpu-&gt;arch.mmu.page_fault(vcpu, cr2, error_code, false)，即是tdp_page_fault，handle_mmio_page_fault的流程不提。 //填充kvm mmu专用的slab r = mmu_topup_memory_caches(vcpu); //获取gfn使用的level，即hugepage的问题 force_pt_level = mapping_level_dirty_bitmap(vcpu, gfn); if (likely(!force_pt_level)) { level = mapping_level(vcpu, gfn); gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1); } else level = PT_PAGE_TABLE_LEVEL; //顾名思义，快速处理一个简单的page fault //即present同时有写权限的非mmio page fault //参考page_fault_can_be_fast函数 //一部分处理没有写权限的page fault //一部分处理 TLB lazy //fast_pf_fix_direct_spte也就是将pte获取的写权限 if (fast_page_fault(vcpu, gpa, level, error_code)) return 0; //下面函数主要就一件事情，gfn_to_pfn if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable)) return 0; //direct map就是映射ept页表的过程 r = __direct_map(vcpu, gpa, write, map_writable, level, gfn, pfn, prefault); 在try_async_pf中就是gfn转换成hva，然后hva转换成pfn的过程，gfn转换到hva: static pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic, bool *async, bool write_fault, bool *writable) { unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault); if (addr == KVM_HVA_ERR_RO_BAD) return KVM_PFN_ERR_RO_FAULT; if (kvm_is_error_hva(addr)) return KVM_PFN_NOSLOT; /* Do not map writable pfn in the readonly memslot. */ if (writable && memslot_is_readonly(slot)) { *writable = false; writable = NULL; } return hva_to_pfn(addr, atomic, async, write_fault, writable); } gfn2hva本质就是 staticinline unsigned long __gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn) { return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE; } 而hva_to_pfn则就是host的线性区进行地址转换的问题了，不提。 static int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write, int map_writable, int level, gfn_t gfn, pfn_t pfn, bool prefault) { struct kvm_shadow_walk_iterator iterator; struct kvm_mmu_page *sp; int emulate = 0; gfn_t pseudo_gfn; if (!VALID_PAGE(vcpu->arch.mmu.root_hpa)) return0; //遍历ept四级页表 for_each_shadow_entry(vcpu, (u64)gfn < PAGE_SHIFT, iterator) { //如果是最后一级，level是hugepage下的level if (iterator.level == level) { //设置pte，页表下一级的page地址就是pfn写入到pte mmu_set_spte(vcpu, iterator.sptep, ACC_ALL, write, &emulate, level, gfn, pfn, prefault, map_writable); direct_pte_prefetch(vcpu, iterator.sptep); ++vcpu->stat.pf_fixed; break; } drop_large_spte(vcpu, iterator.sptep); //mmu page不在位的情况，也就是缺页 if (!is_shadow_present_pte(*iterator.sptep)) { u64 base_addr = iterator.addr; //获取指向的具体mmu page entry的index base_addr &= PT64_LVL_ADDR_MASK(iterator.level); pseudo_gfn = base_addr >> PAGE_SHIFT; //获取mmu page sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr, iterator.level - 1, 1, ACC_ALL, iterator.sptep); //将当前的mmu page的地址写入到上一级别mmu page的pte中 link_shadow_page(iterator.sptep, sp, true); } } return emulate; } static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gaddr, unsigned level, int direct, unsigned access, u64 *parent_pte) { union kvm_mmu_page_role role; unsigned quadrant; struct kvm_mmu_page *sp; bool need_sync = false; role = vcpu->arch.mmu.base_role; role.level = level; role.direct = direct; if (role.direct) role.cr4_pae = 0; role.access = access; if (!vcpu->arch.mmu.direct_map && vcpu->arch.mmu.root_level (PAGE_SHIFT + (PT64_PT_BITS * level)); quadrant &= (1 < ((PT32_PT_BITS - PT64_PT_BITS) * level)) - 1; role.quadrant = quadrant; } //根据一个hash索引来的 for_each_gfn_sp(vcpu->kvm, sp, gfn) { //检查整个mmu ept是否被失效了 if (is_obsolete_sp(vcpu->kvm, sp)) continue; if (!need_sync && sp->unsync) need_sync = true; if (sp->role.word != role.word) continue; if (sp->unsync && kvm_sync_page_transient(vcpu, sp)) break; mmu_page_add_parent_pte(vcpu, sp, parent_pte); if (sp->unsync_children) { kvm_make_request(KVM_REQ_MMU_SYNC, vcpu); kvm_mmu_mark_parents_unsync(sp); } else if (sp->unsync) kvm_mmu_mark_parents_unsync(sp); __clear_sp_write_flooding_count(sp); trace_kvm_mmu_get_page(sp, false); return sp; } ++vcpu->kvm->stat.mmu_cache_miss; sp = kvm_mmu_alloc_page(vcpu, parent_pte, direct); if (!sp) return sp; sp->gfn = gfn; sp->role = role; //新的mmu page加入hash索引，所以前面的for循环中才能知道gfn对应的mmu有没有 //被分配 hlist_add_head(&sp->hash_link, &vcpu->kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)]); if (!direct) { if (rmap_write_protect(vcpu->kvm, gfn)) kvm_flush_remote_tlbs(vcpu->kvm); if (level > PT_PAGE_TABLE_LEVEL && need_sync) kvm_sync_pages(vcpu, gfn); account_shadowed(vcpu->kvm, gfn); } sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen; init_shadow_page_table(sp); trace_kvm_mmu_get_page(sp, true); return sp; }]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM源代码分析3:CPU虚拟化]]></title>
    <url>%2F2014%2F12%2F11%2Fkvm-src-3-cpu%2F</url>
    <content type="text"><![CDATA[在虚拟机的创建与运行章节里面笼统的介绍了KVM在qemu中的创建和运行，基本的qemu代码流程已经梳理清楚，后续主要写一些硬件虚拟化的原理和代码流程，主要写原理和qemu控制KVM运行的的ioctl接口，后续对内核代码的梳理也从这些接口下手。 QEMU：git://git.qemu.org/qemu.git v2.4.0 KVM：https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git v4.2 1.VT-x 技术Intel处理器支持的虚拟化技术即是VT-x，之所以CPU支持硬件虚拟化是因为软件虚拟化的效率太低。 处理器虚拟化的本质是分时共享，主要体现在状态恢复和资源隔离，实际上每个VM对于VMM看就是一个task么，之前Intel处理器在虚拟化上没有提供默认的硬件支持，传统 x86 处理器有4个特权级，Linux使用了0,3级别，0即内核，3即用户态，（更多参考CPU的运行环、特权级与保护）而在虚拟化架构上，虚拟机监控器的运行级别需要内核态特权级，而CPU特权级被传统OS占用，所以Intel设计了VT-x，提出了VMX模式，即VMX root operation 和 VMX non-root operation，虚拟机监控器运行在VMX root operation，虚拟机运行在VMX non-root operation。每个模式下都有相对应的0~3特权级。 为什么引入这两种特殊模式，在传统x86的系统中，CPU有不同的特权级，是为了划分不同的权限指令，某些指令只能由系统软件操作，称为特权指令，这些指令只能在最高特权级上才能正确执行，反之则会触发异常，处理器会陷入到最高特权级，由系统软件处理。还有一种需要操作特权资源（如访问中断寄存器）的指令，称为敏感指令。OS运行在特权级上，屏蔽掉用户态直接执行的特权指令，达到控制所有的硬件资源目的；而在虚拟化环境中，VMM控制所有所有硬件资源，VM中的OS只能占用一部分资源，OS执行的很多特权指令是不能真正对硬件生效的，所以原特权级下有了root模式，OS指令不需要修改就可以正常执行在特权级上，但这个特权级的所有敏感指令都会传递到root模式处理，这样达到了VMM的目的。 在KVM源代码分析1:基本工作原理章节中也说了kvm分3个模式，对应到VT-x 中即是客户模式对应vmx非root模式，内核模式对应VMX root模式下的0特权级，用户模式对应vmx root模式下的3特权级。 如下图 在非根模式下敏感指令引发的陷入称为VM-Exit，VM-Exit发生后，CPU从非根模式切换到根模式；对应的，VM-Entry则是从根模式到非根模式，通常意味着调用VM进入运行态。VMLAUCH/VMRESUME命令则是用来发起VM-Entry。 2.VMCS寄存器VMCS保存虚拟机的相关CPU状态，每个VCPU都有一个VMCS（内存的），每个物理CPU都有VMCS对应的寄存器（物理的），当CPU发生VM-Entry时，CPU则从VCPU指定的内存中读取VMCS加载到物理CPU上执行，当发生VM-Exit时，CPU则将当前的CPU状态保存到VCPU指定的内存中，即VMCS，以备下次VMRESUME。 VMLAUCH指VM的第一次VM-Entry，VMRESUME则是VMLAUCH之后后续的VM-Entry。VMCS下有一些控制域： col 1 | col 2 | col 3———————- | ——————————————————– | —————————————————————– VM-execution controls | Determines what operations cause VM exits | CR0, CR3, CR4, Exceptions, IO Ports, Interrupts, Pin Events, etcGuest-state area | Saved on VM exits，Reloaded on VM entry | EIP, ESP, EFLAGS, IDTR, Segment Regs, Exit info, etc Host-state area | Loaded on VM exits | CR3, EIP set to monitor entry point, EFLAGS hardcoded, etc VM-exit controls | Determines which state to save, load, how to transition | Example: MSR save-load list VM-entry controls | Determines which state to load, how to transition | Including injecting events (interrupts, exceptions) on entry 关于具体控制域的细节，还是翻Intel手册吧。 3.VM-Entry/VM-ExitVM-Entry是从根模式切换到非根模式，即VMM切换到guest上，这个状态由VMM发起，发起之前先保存VMM中的关键寄存器内容到VMCS中，然后进入到VM-Entry，VM-Entry附带参数主要有3个：1.guest是否处于64bit模式，2.MSR VM-Entry控制，3.注入事件。1应该只在VMLAUCH有意义，3更多是在VMRESUME，而VMM发起VM-Entry更多是因为3，2主要用来每次更新MSR。 VM-Exit是CPU从非根模式切换到根模式，从guest切换到VMM的操作，VM-Exit触发的原因就很多了，执行敏感指令，发生中断，模拟特权资源等。 运行在非根模式下的敏感指令一般分为3个方面： 1.行为没有变化的，也就是说该指令能够正确执行。 2.行为有变化的，直接产生VM-Exit。 3.行为有变化的，但是是否产生VM-Exit受到VM-Execution控制域控制。 主要说一下”受到VM-Execution控制域控制”的敏感指令，这个就是针对性的硬件优化了，一般是1.产生VM-Exit；2.不产生VM-Exit，同时调用优化函数完成功能。典型的有“RDTSC指令”。除了大部分是优化性能的，还有一小部分是直接VM-Exit执行指令结果是异常的，或者说在虚拟化场景下是不适用的，典型的就是TSC offset了。 VM-Exit发生时退出的相关信息，如退出原因、触发中断等，这些内容保存在VM-Exit信息域中。 4.KVM_CREATE_VM创建VM就写这里吧，kvm_dev_ioctl_create_vm函数是主干，在kvm_create_vm中，主要有两个函数，kvm_arch_init_vm和hardware_enable_all，需要注意，但是更先一步的是KVM结构体，下面的struct是精简后的版本。 struct kvm { struct mm_struct *mm; /* userspace tied to this vm */ struct kvm_memslots *memslots; /*qemu模拟的内存条模型*/ struct kvm_vcpu *vcpus[KVM_MAX_VCPUS]; /* 模拟的CPU */ atomic_t online_vcpus; int last_boosted_vcpu; struct list_head vm_list; //HOST上VM管理链表， struct kvm_io_bus *buses[KVM_NR_BUSES]; struct kvm_vm_stat stat; struct kvm_arch arch; //这个是host的arch的一些参数 atomic_t users_count; long tlbs_dirty; struct list_head devices; }; kvm_arch_init_vm基本没有特别动作，初始化了KVM-&gt;arch，以及更新了kvmclock函数，这个另外再说。而hardware_enable_all，针对于每个CPU执行“on_each_cpu(hardware_enable_nolock, NULL, 1）”，在hardware_enable_nolock中先把cpus_hardware_enabled置位，进入到kvm_arch_hardware_enable中，有hardware_enable和TSC初始化规则，主要看hardware_enable，crash_enable_local_vmclear清理位图，判断MSR_IA32_FEATURE_CONTROL寄存器是否满足虚拟环境，不满足则将条件写入到寄存器内，CR4将X86_CR4_VMXE置位，另外还有kvm_cpu_vmxon打开VMX操作模式，外层包了vmm_exclusive的判断，它是kvm_intel.ko的外置参数，默认唯一，可以让用户强制不使用VMM硬件支持。 5.KVM_CREATE_VCPUkvm_vm_ioctl_create_vcpu主要有三部分，kvm_arch_vcpu_create，kvm_arch_vcpu_setup和kvm_arch_vcpu_postcreate，重点自然是kvm_arch_vcpu_create。老样子，在这之前先看一下VCPU的结构体。 struct kvm_vcpu { struct kvm *kvm; //归属的KVM\#ifdef CONFIG_PREEMPT_NOTIFIERSstruct preempt_notifier preempt_notifier; \#endif int cpu; int vcpu_id; int srcu_idx; int mode; unsigned long requests; unsigned long guest_debug; struct mutex mutex; struct kvm_run *run; //运行时的状态 int fpu_active; int guest_fpu_loaded, guest_xcr0_loaded; wait_queue_head_t wq; //队列 struct pid *pid; int sigset_active; sigset_t sigset; struct kvm_vcpu_stat stat; //一些数据 \#ifdef CONFIG_HAS_IOMEM int mmio_needed; int mmio_read_completed; int mmio_is_write; int mmio_cur_fragment; int mmio_nr_fragments; struct kvm_mmio_fragment mmio_fragments[KVM_MAX_MMIO_FRAGMENTS]; \#endif \#ifdef CONFIG_KVM_ASYNC_PF struct { u32 queued; struct list_head queue; struct list_head done; spinlock_t lock; } async_pf; \#endif \#ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT /* \* Cpu relax intercept or pause loop exit optimization \* in_spin_loop: set when a vcpu does a pause loop exit \* or cpu relax intercepted. \* dy_eligible: indicates whether vcpu is eligible for directed yield. \*/ struct { bool in_spin_loop; bool dy_eligible; } spin_loop; \#endif bool preempted; struct kvm_vcpu_arch arch; //当前VCPU虚拟的架构，默认介绍X86 }; 借着看kvm_arch_vcpu_create，它借助kvm_x86_ops-&gt;vcpu_create即vmx_create_vcpu完成任务，vmx是X86硬件虚拟化层，从代码看，qemu用户态是一层，kernel 中KVM通用代码是一层，类似kvm_x86_ops是一层，针对各个不同硬件架构，而vcpu_vmx则是具体架构的虚拟化方案一层。首先是kvm_vcpu_init初始化，主要是填充结构体，可以注意的是vcpu-&gt;run分派了一页内存，下面有kvm_arch_vcpu_init负责填充x86 CPU结构体，下面就是kvm_vcpu_arch： struct kvm_vcpu_arch { /* \* rip and regs accesses must go through \* kvm_{register,rip}_{read,write} functions. \*/unsignedlong regs[NR_VCPU_REGS]; u32 regs_avail; u32 regs_dirty; //类似这些寄存器就是就是用来缓存真正的CPU值的unsignedlong cr0; unsignedlong cr0_guest_owned_bits; unsignedlong cr2; unsignedlong cr3; unsigned long cr4; unsigned long cr4_guest_owned_bits; unsigned long cr8; u32 hflags; u64 efer; u64 apic_base; struct kvm_lapic *apic; /* kernel irqchip context */ unsigned long apic_attention; int32_t apic_arb_prio; int mp_state; u64 ia32_misc_enable_msr; bool tpr_access_reporting; u64 ia32_xss; /* \* Paging state of the vcpu \* \* If the vcpu runs in guest mode with two level paging this still saves \* the paging mode of the l1 guest. This context is always used to \* handle faults. \*/ struct kvm_mmu mmu; //内存管理，更多的是附带了直接操作函数 /* \* Paging state of an L2 guest (used for nested npt) \* \* This context will save all necessary information to walk page tables \* of the an L2 guest. This context is only initialized for page table \* walking and not for faulting since we never handle l2 page faults on \* the host. \*/ struct kvm_mmu nested_mmu; /* \* Pointer to the mmu context currently used for \* gva_to_gpa translations. \*/ struct kvm_mmu *walk_mmu; struct kvm_mmu_memory_cache mmu_pte_list_desc_cache; struct kvm_mmu_memory_cache mmu_page_cache; struct kvm_mmu_memory_cache mmu_page_header_cache; struct fpu guest_fpu; u64 xcr0; u64 guest_supported_xcr0; u32 guest_xstate_size; struct kvm_pio_request pio; void *pio_data; u8 event_exit_inst_len; struct kvm_queued_exception { bool pending; bool has_error_code; bool reinject; u8 nr; u32 error_code; } exception; struct kvm_queued_interrupt { bool pending; bool soft; u8 nr; } interrupt; int halt_request; /* real mode on Intel only */ int cpuid_nent; struct kvm_cpuid_entry2 cpuid_entries[KVM_MAX_CPUID_ENTRIES]; int maxphyaddr; /* emulate context */ //下面是KVM的软件模拟模式，也就是没有vmx的情况，估计也没人用这一套 struct x86_emulate_ctxt emulate_ctxt; bool emulate_regs_need_sync_to_vcpu; bool emulate_regs_need_sync_from_vcpu; int (*complete_userspace_io)(struct kvm_vcpu *vcpu); gpa_t time; struct pvclock_vcpu_time_info hv_clock; unsigned int hw_tsc_khz; struct gfn_to_hva_cache pv_time; bool pv_time_enabled; /* set guest stopped flag in pvclock flags field */ bool pvclock_set_guest_stopped_request; struct { u64 msr_val; u64 last_steal; u64 accum_steal; struct gfn_to_hva_cache stime; struct kvm_steal_time steal; } st; u64 last_guest_tsc; u64 last_host_tsc; u64 tsc_offset_adjustment; u64 this_tsc_nsec; u64 this_tsc_write; u64 this_tsc_generation; bool tsc_catchup; bool tsc_always_catchup; s8 virtual_tsc_shift; u32 virtual_tsc_mult; u32 virtual_tsc_khz; s64 ia32_tsc_adjust_msr; atomic_t nmi_queued; /* unprocessed asynchronous NMIs */ unsigned nmi_pending; /* NMI queued after currently running handler */ bool nmi_injected; /* Trying to inject an NMI this entry */ struct mtrr_state_type mtrr_state; u64 pat; unsigned switch_db_regs; unsigned long db[KVM_NR_DB_REGS]; unsigned long dr6; unsigned long dr7; unsigned long eff_db[KVM_NR_DB_REGS]; unsigned long guest_debug_dr7; u64 mcg_cap; u64 mcg_status; u64 mcg_ctl; u64 *mce_banks; /* Cache MMIO info */ u64 mmio_gva; unsigned access; gfn_t mmio_gfn; u64 mmio_gen; struct kvm_pmu pmu; /* used for guest single stepping over the given code position */ unsigned long singlestep_rip; /* fields used by HYPER-V emulation */ u64 hv_vapic; cpumask_var_t wbinvd_dirty_mask; unsigned long last_retry_eip; unsigned long last_retry_addr; struct { bool halted; gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)]; struct gfn_to_hva_cache data; u64 msr_val; u32 id; bool send_user_only; } apf; /* OSVW MSRs (AMD only) */ struct { u64 length; u64 status; } osvw; struct { u64 msr_val; struct gfn_to_hva_cache data; } pv_eoi; /* \* Indicate whether the access faults on its page table in guest \* which is set when fix page fault and used to detect unhandeable \* instruction. \*/ bool write_fault_to_shadow_pgtable; /* set at EPT violation at this point */ unsigned long exit_qualification; /* pv related host specific info */ struct { bool pv_unhalted; } pv; }; 整个arch结构真是长，很适合凑篇幅，很多结构其他过程涉及到的再提吧，反正我也不知道。kvm_arch_vcpu_init初始化了x86在虚拟化底层的实现函数，首先是pv和emulate_ctxt，这些不支持VMX下的模拟虚拟化，尤其是vcpu-&gt;arch.emulate_ctxt.ops = &amp;emulate_ops，emulate_ops初始化虚拟化模拟的对象函数。 static struct x86_emulate_ops emulate_ops = { .read_std = kvm_read_guest_virt_system, .write_std = kvm_write_guest_virt_system, .fetch = kvm_fetch_guest_virt, .read_emulated = emulator_read_emulated, .write_emulated = emulator_write_emulated, .cmpxchg_emulated = emulator_cmpxchg_emulated, .invlpg = emulator_invlpg, .pio_in_emulated = emulator_pio_in_emulated, .pio_out_emulated = emulator_pio_out_emulated, .get_segment = emulator_get_segment, .set_segment = emulator_set_segment, .get_cached_segment_base = emulator_get_cached_segment_base, .get_gdt = emulator_get_gdt, .get_idt = emulator_get_idt, .set_gdt = emulator_set_gdt, .set_idt = emulator_set_idt, .get_cr = emulator_get_cr, .set_cr = emulator_set_cr, .cpl = emulator_get_cpl, .get_dr = emulator_get_dr, .set_dr = emulator_set_dr, .set_msr = emulator_set_msr, .get_msr = emulator_get_msr, .halt = emulator_halt, .wbinvd = emulator_wbinvd, .fix_hypercall = emulator_fix_hypercall, .get_fpu = emulator_get_fpu, .put_fpu = emulator_put_fpu, .intercept = emulator_intercept, .get_cpuid = emulator_get_cpuid, }; x86_emulate_ops函数看看就好，实际上也很少有人放弃vmx直接软件模拟。后面又有mp_state，给pio_data分配了一个page，kvm_set_tsc_khz设置TSC，kvm_mmu_create则是初始化MMU的函数，里面的函数都是地址转换的重点，在内存虚拟化重点提到。kvm_create_lapic初始化lapic，初始化mce_banks结构，还有pv_time,xcr0,xstat,pmu等，类似x86硬件结构上需要存在的，OS底层需要看到的硬件名称都要有对应的软件结构。回到vmx_create_vcpu，vmx的guest_msrs分配得到一个page，后面是vmcs的分配，vmx-&gt;loaded_vmcs-&gt;vmcs = alloc_vmcs()，alloc_vmcs为当前cpu执行alloc_vmcs_cpu，alloc_vmcs_cpu中alloc_pages_exact_node分配给vmcs，alloc_pages_exact_node调用__alloc_pages实现，原来以为vmcs占用了一个page，但此处从伙伴系统申请了2^vmcs_config.order页，此处vmcs_config在setup_vmcs_config中初始化，vmcs_conf-&gt;order = get_order(vmcs_config.size)，而vmcs_conf-&gt;size = vmx_msr_high &amp; 0x1fff，又rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high)，此处size由于与0x1fff与运算，大小必然小于4k，order则为0，然来绕去还是一个page大小。这么做估计是为了兼容vmcs_config中的size计算。下面根据vmm_exclusive进行kvm_cpu_vmxon，进入vmx模式，初始化loaded_vmcs，然后用kvm_cpu_vmxoff退出vmx模式。vmx_vcpu_load加载VCPU的信息，切换到指定cpu，进入到vmx模式，将loaded_vmcs的vmcs和当前cpu的vmcs绑定到一起。vmx_vcpu_setup则是初始化vmcs内容，主要是赋值计算，下面的vmx_vcpu_put则是vmx_vcpu_load的反运算。下面还有一些apic，nested，pml就不说了。vmx_create_vcpu结束就直接回到kvm_vm_ioctl_create_vcpu函数，下面是kvm_arch_vcpu_setup，整个就一条线到kvm_arch_vcpu_load函数，主要有kvm_x86_ops-&gt;vcpu_load(vcpu, cpu)和tsc处理，vcpu_load就是vmx_vcpu_load，刚说了，就是进入vcpu模式下准备工作。kvm_arch_vcpu_setup后面是create_vcpu_fd为proc创建控制fd，让qemu使用。kvm_arch_vcpu_postcreate则是马后炮般，重新vcpu_load，写msr，tsc。如此整个vcpu就创建完成了。 6.KVM_RUN KVM run涉及内容也不少，先写完内存虚拟化之后再开篇专门写RUN流程。 下一篇： KVM源代码分析4:内存虚拟化 ———-完———- ——————–下面未编辑的留存————————————-给vmcs分配空间并初始化，在alloc_vmcs_cpu分配一个页大小内存，用来保存vm和vmm信息。 vmx->vmcs = alloc_vmcs(); if (!vmx->vmcs) goto free_msrs; vmcs_init(vmx->vmcs); 执行vm entry的时候将vmm状态保存到vmcs的host area，并加载对应vm的vmcs guest area信息到CPU中，vm exit的时候则反之，vmcs具体结构分配由硬件实现，程序员只需要通过VMWRITE和VMREAD指令去访问。 vmx执行完后，回到kvm_vm_ioctl_create_vcpu函数。kvm_arch_vcpu_reset对vcpu的结构进行初始化，后面一些就是检查vcpu的合法性，最后和kvm串接到一起。 vcpu的创建到此结束，下面说一下vcpu的运行。 VCPU一旦创建成功，后续的控制基本上从kvm_vcpu_ioctl开始，控制开关有KVM_RUN，KVM_GET_REGS，KVM_SET_REGS，KVM_GET_SREGS，KVM_SET_SREGS，KVM_GET_MP_STATE，KVM_SET_MP_STATE，KVM_TRANSLATE，KVM_SET_GUEST_DEBUG，KVM_SET_SIGNAL_MASK等，如果不清楚具体开关作用，可以直接到qemu搜索对应开关代码，一目了然。 KVM_RUN的实现函数是kvm_arch_vcpu_ioctl_run，进行安全检查之后进入__vcpu_run中，在while循环里面调用vcpu_enter_guest进入guest模式，首先处理vcpu-&gt;requests，对应的request做处理，kvm_mmu_reload加载mmu，通过kvm_x86_ops-&gt;prepare_guest_switch(vcpu)准备陷入到guest，prepare_guest_switch实现是vmx_save_host_state，顾名思义，就是保存host的当前状态。 kvm_x86_ops->prepare_guest_switch(vcpu); if (vcpu->fpu_active) kvm_load_guest_fpu(vcpu); kvm_load_guest_xcr0(vcpu); vcpu->mode = IN_GUEST_MODE; /* We should set ->mode before check ->requests, \* see the comment in make_all_cpus_request. \*/ smp_mb(); local_irq_disable(); 然后加载guest的寄存器等信息，fpu，xcr0,将vcpu模式设置为guest状态，屏蔽中断响应，准备进入guest。但仍进行一次检查，vcpu-&gt;mode和vcpu-&gt;requests等，如果有问题，则恢复host状态。 kvm_guest_enter做了两件事：account_system_vtime计算虚拟机系统时间；rcu_virt_note_context_switch对rcu锁数据进行保护，完成上下文切换。 准备工作搞定，kvm_x86_ops-&gt;run(vcpu)，开始运行guest，由vmx_vcpu_run实现。 if (vmx->emulation_required && emulate_invalid_guest_state) return; if (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty)) vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]); if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty)) vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]); 判断模拟器，RSP，RIP寄存器值。 主要功能在这段内联汇编上 asm( /* Store host registers */ "push %%"R"dx; push %%"R"bp;" "push %%"R"cx nt" /* placeholder for guest rcx */ "push %%"R"cx nt"//如果vcpu host rsp和环境不等，则将其拷贝到vpu上 "cmp %%"R"sp, %c[host_rsp](%0) nt""je 1f nt""mov %%"R"sp, %c[host_rsp](%0) nt" __ex(ASM_VMX_VMWRITE_RSP_RDX) "nt"//__kvm_handle_fault_on_reboot write host rsp"1: nt"/* Reload cr2 if changed */ "mov %c[cr2](%0), %%"R"ax nt" "mov %%cr2, %%"R"dx nt" //环境上cr2值和vpu上的值不同，则将vpu上值拷贝到环境上 "cmp %%"R"ax, %%"R"dx nt" "je 2f nt" "mov %%"R"ax, %%cr2 nt" "2: nt" /* Check if vmlaunch of vmresume is needed */ "cmpl $0, %c[launched](%0) nt" /* Load guest registers. Don't clobber flags. */ "mov %c[rax](%0), %%"R"ax nt" "mov %c[rbx](%0), %%"R"bx nt" "mov %c[rdx](%0), %%"R"dx nt" "mov %c[rsi](%0), %%"R"si nt" "mov %c[rdi](%0), %%"R"di nt" "mov %c[rbp](%0), %%"R"bp nt" \#ifdef CONFIG_X86_64 "mov %c[r8](%0), %%r8 nt" "mov %c[r9](%0), %%r9 nt" "mov %c[r10](%0), %%r10 nt" "mov %c[r11](%0), %%r11 nt" "mov %c[r12](%0), %%r12 nt" "mov %c[r13](%0), %%r13 nt" "mov %c[r14](%0), %%r14 nt" "mov %c[r15](%0), %%r15 nt" \#endif "mov %c[rcx](%0), %%"R"cx nt" /* kills %0 (ecx) */ /* Enter guest mode */ //此处和cmpl $0, %c[launched](%0)是对应的，此处选择进入guest的两种模式 //RESUME和LAUNCH，通过__ex __kvm_handle_fault_on_reboot执行 "jne .Llaunched nt" __ex(ASM_VMX_VMLAUNCH) "nt" "jmp .Lkvm_vmx_return nt" ".Llaunched: " __ex(ASM_VMX_VMRESUME) "nt" //退出vmx，保存guest信息，加载host信息 ".Lkvm_vmx_return: " /* Save guest registers, load host registers, keep flags */ "mov %0, %c[wordsize](%%"R"sp) nt" "pop %0 nt" "mov %%"R"ax, %c[rax](%0) nt" "mov %%"R"bx, %c[rbx](%0) nt" "pop"Q" %c[rcx](%0) nt" "mov %%"R"dx, %c[rdx](%0) nt" "mov %%"R"si, %c[rsi](%0) nt" "mov %%"R"di, %c[rdi](%0) nt" "mov %%"R"bp, %c[rbp](%0) nt" \#ifdef CONFIG_X86_64 "mov %%r8, %c[r8](%0) nt" "mov %%r9, %c[r9](%0) nt" "mov %%r10, %c[r10](%0) nt" "mov %%r11, %c[r11](%0) nt" "mov %%r12, %c[r12](%0) nt" "mov %%r13, %c[r13](%0) nt" "mov %%r14, %c[r14](%0) nt" "mov %%r15, %c[r15](%0) nt" \#endif "mov %%cr2, %%"R"ax nt" "mov %%"R"ax, %c[cr2](%0) nt" "pop %%"R"bp; pop %%"R"dx nt" "setbe %c[fail](%0) nt" : : "c"(vmx), "d"((unsigned long)HOST_RSP), //下面加了前面寄存器的指针值，对应具体结构的值 [launched]"i"(offsetof(struct vcpu_vmx, launched)), [fail]"i"(offsetof(struct vcpu_vmx, fail)), [host_rsp]"i"(offsetof(struct vcpu_vmx, host_rsp)), [rax]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])), [rbx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])), [rcx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])), [rdx]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])), [rsi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])), [rdi]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])), [rbp]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])), \#ifdef CONFIG_X86_64 [r8]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])), [r9]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])), [r10]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])), [r11]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])), [r12]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])), [r13]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])), [r14]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])), [r15]"i"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])), \#endif [cr2]"i"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)), [wordsize]"i"(sizeof(ulong)) : "cc", "memory" , R"ax", R"bx", R"di", R"si" \#ifdef CONFIG_X86_64 , "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15" \#endif 以上代码相对容易理解的，根据注释大致清楚了具体作用。 然后就是恢复系统NMI等中断: vmx_complete_atomic_exit(vmx); vmx_recover_nmi_blocking(vmx); vmx_complete_interrupts(vmx); 回到vcpu_enter_guest，通过hw_breakpoint_restore恢复硬件断点。 if (hw_breakpoint_active()) hw_breakpoint_restore(); kvm_get_msr(vcpu, MSR_IA32_TSC, &vcpu->arch.last_guest_tsc); //设置vcpu模式，恢复host相关内容 vcpu->mode = OUTSIDE_GUEST_MODE; smp_wmb(); local_irq_enable(); ++vcpu->stat.exits; /* \* We must have an instruction between local_irq_enable() and \* kvm_guest_exit(), so the timer interrupt isn't delayed by \* the interrupt shadow. The stat.exits increment will do nicely. \* But we need to prevent reordering, hence this barrier(): \*/ barrier(); //刷新系统时间 kvm_guest_exit(); preempt_enable(); vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu); /* \* Profile KVM exit RIPs: \*/ if (unlikely(prof_on == KVM_PROFILING)) { unsigned long rip = kvm_rip_read(vcpu); profile_hit(KVM_PROFILING, (void *)rip); } kvm_lapic_sync_from_vapic(vcpu); //处理vmx退出 r = kvm_x86_ops->handle_exit(vcpu); handle_exit退出函数由vmx_handle_exit实现，主要设置vcpu-&gt;run-&gt;exit_reason，让外部感知退出原因，并对应处理。对于vpu而言，handle_exit只是意味着一个传统linux一个时间片的结束，后续的工作都是由handle完成的，handle_exit对应的函数集如下： staticint (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = { [EXIT_REASON_EXCEPTION_NMI] = handle_exception, [EXIT_REASON_EXTERNAL_INTERRUPT] = handle_external_interrupt, [EXIT_REASON_TRIPLE_FAULT] = handle_triple_fault, [EXIT_REASON_NMI_WINDOW] = handle_nmi_window, [EXIT_REASON_IO_INSTRUCTION] = handle_io, [EXIT_REASON_CR_ACCESS] = handle_cr, [EXIT_REASON_DR_ACCESS] = handle_dr, [EXIT_REASON_CPUID] = handle_cpuid, [EXIT_REASON_MSR_READ] = handle_rdmsr, [EXIT_REASON_MSR_WRITE] = handle_wrmsr, [EXIT_REASON_PENDING_INTERRUPT] = handle_interrupt_window, [EXIT_REASON_HLT] = handle_halt, [EXIT_REASON_INVD] = handle_invd, [EXIT_REASON_INVLPG] = handle_invlpg, [EXIT_REASON_VMCALL] = handle_vmcall, [EXIT_REASON_VMCLEAR] = handle_vmx_insn, [EXIT_REASON_VMLAUNCH] = handle_vmx_insn, [EXIT_REASON_VMPTRLD] = handle_vmx_insn, [EXIT_REASON_VMPTRST] = handle_vmx_insn, [EXIT_REASON_VMREAD] = handle_vmx_insn, [EXIT_REASON_VMRESUME] = handle_vmx_insn, [EXIT_REASON_VMWRITE] = handle_vmx_insn, [EXIT_REASON_VMOFF] = handle_vmx_insn, [EXIT_REASON_VMON] = handle_vmx_insn, [EXIT_REASON_TPR_BELOW_THRESHOLD] = handle_tpr_below_threshold, [EXIT_REASON_APIC_ACCESS] = handle_apic_access, [EXIT_REASON_WBINVD] = handle_wbinvd, [EXIT_REASON_XSETBV] = handle_xsetbv, [EXIT_REASON_TASK_SWITCH] = handle_task_switch, [EXIT_REASON_MCE_DURING_VMENTRY] = handle_machine_check, [EXIT_REASON_EPT_VIOLATION] = handle_ept_violation, [EXIT_REASON_EPT_MISCONFIG] = handle_ept_misconfig, [EXIT_REASON_PAUSE_INSTRUCTION] = handle_pause, [EXIT_REASON_MWAIT_INSTRUCTION] = handle_invalid_op, [EXIT_REASON_MONITOR_INSTRUCTION] = handle_invalid_op, }; 有handle_task_switch进行任务切换，handle_io处理qemu的外部模拟IO等，具体处理内容后面在写。 再次退回到__vcpu_run函数，在while (r &gt; 0)中，循环受vcpu_enter_guest返回值控制，只有运行异常的时候才退出循环，否则通过kvm_resched一直运行下去。 if (need_resched()) { srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx); kvm_resched(vcpu); vcpu->srcu_idx = srcu_read_lock(&kvm->srcu); } 再退就到了kvm_arch_vcpu_ioctl_run函数，此时kvm run的执行也结束。 KVM cpu虚拟化的理解基本如上，涉及到的具体细节有时间后开篇另说。 KVM源代码分析未完待续]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM源代码分析2:虚拟机的创建与运行]]></title>
    <url>%2F2014%2F12%2F11%2Fkvm-src-2-vm-run%2F</url>
    <content type="text"><![CDATA[原文链接：http://oenhan.com/kvm-src-2-vm-run 前段时间挖了一个坑，KVM源代码分析1:基本工作原理，准备写一下kvm的代码机制，结果一直没时间填土，现在还一下旧账，争取能温故而知新。 基本原理里面提到kvm虚拟化由用户态程序Qemu和内核态驱动kvm配合完成，qemu负责HOST用户态层面进程管理，IO处理等，KVM负责把qemu的部分指令在硬件上直接实现，从虚拟机的创建和运行上看，qemu的代码占了流程上的主要部分。下面的代码主要主要针对与qemu，KVM部分另外开篇再说。 代码： QEMU：git://git.qemu.org/qemu.git v2.4.0 KVM：https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git v4.2 QEMU和KVM是通过IOCTL进行配合的，直接抓住这个线看有kvm_ioctl、kvm_vm_ioctl、kvm_vcpu_ioctl、kvm_device_ioctl等，他们还都在一个C文件里面。 使用kvm_ioctl很少了，直接看调用的代码，有KVM_GET_VCPU_MMAP_SIZE，KVM_CHECK_EXTENSION，KVM_GET_API_VERSION，KVM_CREATE_VM，KVM_GET_SUPPORTED_CPUID等等，需要记住只有KVM_CREATE_VM。 而调用kvm_vm_ioctl的函数真是海了去了，需要看的是KVM_SET_USER_MEMORY_REGION，KVM_CREATE_VCPU，KVM_CREATE_DEVICE。 所有寄存器的交换信息都是通过kvm_vcpu_ioctl，需要记住的操作只有，KVM_RUN。 所有看QEMU和KVM的配合流程如下： 接下来参考上图分析qemu代码流程： 从vl.c代码的main函数开始。 atexit(qemu_run_exit_notifiers)注册了qemu的退出处理函数，后面在具体看qemu_run_exit_notifiers函数。 module_call_init则开始初始化qemu的各个模块，陆陆续续的有以下参数： typedef enum { MODULE_INIT_BLOCK, MODULE_INIT_MACHINE, MODULE_INIT_QAPI, MODULE_INIT_QOM, MODULE_INIT_MAX } module_init_type; 最开始初始化的MODULE_INIT_QOM，QOM是qemu实现的一种模拟设备，具体可以参考http://wiki.qemu.org/Features/QOM，代码下面的不远处就MODULE_INIT_MACHINE的初始化，这两条语句放到一起看，直接说一下module_call_init的机制。 module_call_init实际设计的一个函数链表，ModuleTypeList ，链表关系如下图 它把相关的函数注册到对应的数组链表上，通过执行init项目完成所有设备的初始化。module_call_init就是执行e-&gt;init()完成功能的，而e-&gt;init是什么时候通过register_module_init注册到ModuleTypeList上的ModuleEntry，是module_init注册的，而调用module_init的有 \#define block_init(function) module_init(function, MODULE_INIT_BLOCK) \#define machine_init(function) module_init(function, MODULE_INIT_MACHINE) \#define qapi_init(function) module_init(function, MODULE_INIT_QAPI) \#define type_init(function) module_init(function, MODULE_INIT_QOM) 那么执行machine_init则是挂到了MODULE_INIT_MACHINE，type_init则将函数挂载了MODULE_INIT_QOM。那么排查一下是，我们只关注PC的注册，那么就是machine_init(pc_machine_init_##suffix)，源自DEFINE_PC_MACHINE(suffix, namestr, initfn, optsfn)宏，而DEFINE_I440FX_MACHINE有 \#define DEFINE_I440FX_MACHINE(suffix, name, compatfn, optionfn)staticvoid pc_init_\##suffix(MachineState *machine) { void (*compat)(MachineState *m) = (compatfn); if (compat) { compat(machine); } pc_init1(machine); } DEFINE_PC_MACHINE(suffix, name, pc_init_\##suffix, optionfn) \#define DEFINE_PC_MACHINE(suffix, namestr, initfn, optsfn) static void pc_machine_\##suffix##_class_init(ObjectClass *oc, void *data) { MachineClass *mc = MACHINE_CLASS(oc); optsfn(mc); mc->name = namestr; mc->init = initfn; } static const TypeInfo pc_machine_type_\##suffix = { .name = namestr TYPE_MACHINE_SUFFIX, .parent = TYPE_PC_MACHINE, .class_init = pc_machine_\##suffix##_class_init, }; static void pc_machine_init_\##suffix(void) { type_register(&pc_machine_type_\##suffix); } machine_init(pc_machine_init_\##suffix) DEFINE_PC_MACHINE注册的函数pc_init_##suffix在DEFINE_I440FX_MACHINE中定义，怎么组合都无关，pc_init1(machine)函数一定要执行，本质就是pc_init1赋值给了mc-&gt;init，其他爱看不看吧。而module_init的宏是 \#define module_init(function, type)static void __attribute__((constructor)) do_qemu_init_ \## function(void) { register_dso_module_init(function, type); } \#else /* This should not be used directly. Use block_init etc. instead. */ \#define module_init(function, type) static void __attribute__((constructor)) do_qemu_init_ \## function(void) { register_module_init(function, type); } 它前面的修饰是attribute((constructor)),这个导致machine_init或者type_init等会在main()之前就被执行。所有type_init(kvm_type_init）-&gt; kvm_accel_type -&gt; kvm_accel_class_init -&gt; kvm_init依次完成了函数注册，所有说module_call_init(MODULE_INIT_QOM)函数已经完成了kvm_init的执行，所有这样就清楚KVM调用关系了。如此就先去看kvm_init函数，前面主要干了一件事，填充KVMState *s结构体，然后通过kvm_ioctl(s, KVM_GET_API_VERSION, 0)判断内核KVM驱动和当前QEMU版本是否兼容，下面则是执行kvm_ioctl(s, KVM_CREATE_VM, type)进行虚拟机的创建活动，创建了KVM虚拟机，获取虚拟机句柄。具体KVM_CREATE_VM在内核态做了什么，ioctl的工作等另外再说，现在假定KVM_CREATE_VM所代表的虚拟机创建成功，下面通过检查kvm_check_extension结果填充KVMState，kvm_arch_init初始化KVMState，其中有IDENTITY_MAP_ADDR，TSS_ADDR，NR_MMU_PAGES等，cpu_register_phys_memory_client注册qemu对内存管理的函数集，kvm_create_irqchip创建kvm中断管理内容，通过kvm_vm_ioctl(s, KVM_CREATE_IRQCHIP)实现，具体内核态的工作内容后面分析。到此kvm_init的工作就完成了，最主要的工作就是创建的虚拟机。 这样绕了这么大圈，重新回到vl.c上面来，前面刚说了module_call_init(MODULE_INIT_MACHINE)本质就是把pc_init1赋值给了mc-&gt;init，然后machine_class = find_default_machine()，如此可以看到machine_class的init函数一定会执行pc_init1。 下面涉及对OPT入参的解析过程略过不提。 qemu准备模拟的机器的类型从下面语句获得: current_machine = MACHINE(object_new(object_class_get_name( OBJECT_CLASS(machine_class)))); machine_class则是通过入参传入的 case QEMU_OPTION_machine: olist = qemu_find_opts("machine"); opts = qemu_opts_parse_noisily(olist, optarg, true); if (!opts) { exit(1); } break; man qemu -machine [type=]name[,prop=value[,...]] Select the emulated machine by name. Use "-machine help" to list available machines 下面有cpu_exec_init_all就是执行了qemu的内存结构体的初始化而已，cpudef_init则提供了VCPU的不同型号的模拟，qemu_set_log设置日志输出，kvm对外的日志是从这里配置的。中间的乱七八糟的就忽略掉即可，然后直接到了machine_class-&gt;init(current_machine)函数，其实就是执行了pc_init1。暂且记下来，先看下面的，cpu_synchronize_all_post_init就是内核和qemu数据不一致同步一下。下面的函数没有重要的了，只有vm_start()函数需要记一下，后面会用到。 现在进入pc_init1函数： 在pc_init1中重点看两个函数，pc_cpus_init和pc_memory_init，顾名思义，CPU和内存的初始化，中断，vga等函数的初始化先忽略掉，先看这两个。pc_cpus_init入参是cpu_model，前面说过这是具体的CPU模型，所有X86的CPU模型都在builtin_x86_defs中定义，取其中一个看看 { .name = "SandyBridge", .level = 0xd, .vendor = CPUID_VENDOR_INTEL, .family = 6, .model = 42, .stepping = 1, .features[FEAT_1_EDX] = CPUID_VME | CPUID_SSE2 | CPUID_SSE | CPUID_FXSR | CPUID_MMX | CPUID_CLFLUSH | CPUID_PSE36 | CPUID_PAT | CPUID_CMOV | CPUID_MCA | CPUID_PGE | CPUID_MTRR | CPUID_SEP | CPUID_APIC | CPUID_CX8 | CPUID_MCE | CPUID_PAE | CPUID_MSR | CPUID_TSC | CPUID_PSE | CPUID_DE | CPUID_FP87, .features[FEAT_1_ECX] = CPUID_EXT_AVX | CPUID_EXT_XSAVE | CPUID_EXT_AES | CPUID_EXT_TSC_DEADLINE_TIMER | CPUID_EXT_POPCNT | CPUID_EXT_X2APIC | CPUID_EXT_SSE42 | CPUID_EXT_SSE41 | CPUID_EXT_CX16 | CPUID_EXT_SSSE3 | CPUID_EXT_PCLMULQDQ | CPUID_EXT_SSE3, .features[FEAT_8000_0001_EDX] = CPUID_EXT2_LM | CPUID_EXT2_RDTSCP | CPUID_EXT2_NX | CPUID_EXT2_SYSCALL, .features[FEAT_8000_0001_ECX] = CPUID_EXT3_LAHF_LM, .features[FEAT_XSAVE] = CPUID_XSAVE_XSAVEOPT, .features[FEAT_6_EAX] = CPUID_6_EAX_ARAT, .xlevel = 0x80000008, .model_id = "Intel Xeon E312xx (Sandy Bridge)", }, 你可以cat一个本地的/proc/cpuinfo，builtin_x86_defs定义的就是这些参数。然后是for循环中针对每个CPU初始化，即pc_new_cpu，直接进入cpu_x86_create函数，主要就是把CPUX86State填充了一下，涉及到CPUID和其他的feature。下面是x86_cpu_realize，即唤醒CPU，重点是qemu_init_vcpu，MCE忽略掉，走到qemu_kvm_start_vcpu，qemu创建VCPU，如下： //创建VPU对于的qemu线程，线程函数是qemu_kvm_cpu_thread_fn qemu_thread_create(cpu->thread, thread_name, qemu_kvm_cpu_thread_fn, cpu, QEMU_THREAD_JOINABLE); //如果线程没有创建成功，则一直在此处循环阻塞。说明多核vcpu的创建是顺序的 while (!cpu->created) { qemu_cond_wait(&qemu_cpu_cond, &qemu_global_mutex); } 线程创建完成，具体任务支线提，回到主流程上，qemu_init_vcpu执行完成后，下面就是cpu_reset，此处的作用是什么呢？答案是无用，本质是一个空函数，它的主要功能就是CPUClass的reset函数，reset在cpu_class_init里面注册的，注册的是cpu_common_reset，这是一个空函数，没有任何作用。cpu_class_init则是被cpu_type_info即TYPE_CPU使用，而cpu_type_info则由type_init(cpu_register_types)完成，type_init则是前面提到的和machine_init对应的注册关系。根据下句完成工作 \#define type_init(function) module_init(function, MODULE_INIT_QOM) 从上面看，pc_cpus_init函数过程已经理顺了，下面看一下，vcpu所在的线程对应的qemu_kvm_cpu_thread_fn中： //初始化VCPU r = kvm_init_vcpu(env); //初始化KVM中断 qemu_kvm_init_cpu_signals(env); //标志VCPU创建完成，和上面判断是对应的 cpu->created = true; qemu_cond_signal(&qemu_cpu_cond); while (1) { if (cpu_can_run(env)) { //CPU进入执行状态 r = kvm_cpu_exec(env); if (r == EXCP_DEBUG) { cpu_handle_guest_debug(env); } } qemu_kvm_wait_io_event(env); } CPU进入执行状态的时候我们看到其他的VCPU包括内存可能还没有初始化，关键是此处有一个开关，qemu_cpu_cond,打开这个开关才能进入到CPU执行状态，谁来打开这个开关，后面再说。先看kvm_init_vcpu，通过kvm_vm_ioctl，KVM_CREATE_VCPU创建VCPU，用KVM_GET_VCPU_MMAP_SIZE获取env-&gt;kvm_run对应的内存映射，kvm_arch_init_vcpu则填充对应的kvm_arch内容，具体内核部分，后面单独写。kvm_init_vcpu就是获取了vcpu，将相关内容填充了env。qemu_kvm_init_cpu_signals则是将中断组合掩码传递给kvm_set_signal_mask，最终给内核KVM_SET_SIGNAL_MASK。kvm_cpu_exec此时还在阻塞过程中，先挂起来，看内存的初始化。内存初始化函数是pc_memory_init,memory_region_init_ram传入了高端内存和低端内存的值，memory_region_init负责填充mr，重点在qemu_ram_alloc，即qemu_ram_alloc_from_ptr，首先有RAMBlock，ram_list，那就直接借助find_ram_offset函数一起看一下qemu的内存分布模型。 qemu模拟了普通内存分布模型，内存的线性也是分块被使用的，每个块称为RAMBlock，由ram_list统领，RAMBlock.offset则是区块的线性地址，即相对于开始的偏移位，RAMBlock.length(size)则是区块的大小，find_ram_offset则是在线性区间内找到没有使用的一段空间，可以完全容纳新申请的ramblock length大小，代码就是进行了所有区块的遍历，找到满足新申请length的最小区间，把ramblock安插进去即可，返回的offset即是新分配区间的开始地址。而RAMBlock的物理则是在RAMBlock.host,由kvm_vmalloc(size)分配真正物理内存，内部qemu_vmalloc使用qemu_memalign页对齐分配内存。后续的都是对RAMBlock的插入等处理。从上面看，memory_region_init_ram已经将qemu内存模型和实际的物理内存初始化了。vmstate_register_ram_global这个函数则是负责将前面提到的ramlist中的ramblock和memory region的初始地址对应一下，将mr-&gt;name填充到ramblock的idstr里面，就是让二者有确定的对应关系，如此mr就有了物理内存使用。后面则是subregion的处理，memory_region_init_alias初始化，其中将ram传递给mr-&gt;owner确定了隶属关系，memory_region_add_subregion则是大头，memory_region_add_subregion_common前面的判断忽略，QTAILQ_INSERT_TAIL(&amp;mr-&gt;subregions, subregion, subregions_link)就是插入了链表而已，主要内容在memory_region_transaction_commit。memory_region_transaction_commit中引入了新的结构address_spaces（AS），注释里面提到“AddressSpace: describes a mapping of addresses to #MemoryRegion objects”，就是内存地址的映射关系，因为内存有不同的应用类型，address_spaces以链表形式存在，commit函数则是对所有AS执行address_space_update_topology，先看AS在哪里注册的，就是前面提到的kvm_init里面，执行memory_listener_register，注册了address_space_memory和address_space_io两个，涉及的另外一个结构体则是MemoryListener，有kvm_memory_listener和kvm_io_listener，就是用于监控内存映射关系发生变化之后执行回调函数。下面进入到address_space_update_topology函数，FlatView则是“Flattened global view of current active memory hierarchy”，address_space_get_flatview直接获取当前的，generate_memory_topology则根据前面已经变化的mr重新生成FlatView,然后通过address_space_update_topology_pass比较，简单说address_space_update_topology_pass就是两个FlatView逐条的FlatRange进行对比，以后一个FlatView为准，如果前面FlatView的FlatRange和后面的不一样，则对前面的FlatView的这条FlatRange进行处理，差别就是3种情况，如代码： while (iold < old_view->nr || inew < new_view->nr) { if (iold < old_view->nr) { frold = &old_view->ranges[iold]; } else { frold = NULL; } if (inew < new_view->nr) { frnew = &new_view->ranges[inew]; } else { frnew = NULL; } if (frold && (!frnew || int128_lt(frold->addr.start, frnew->addr.start) || (int128_eq(frold->addr.start, frnew->addr.start) && !flatrange_equal(frold, frnew)))) { /* In old but not in new, or in both but attributes changed. */ if (!adding) { //这个判断代码添加的无用，可以直接删除, //address_space_update_topology里面的两个pass也可以删除一个 MEMORY_LISTENER_UPDATE_REGION(frold, as, Reverse, region_del); } ++iold; } else if (frold && frnew && flatrange_equal(frold, frnew)) { /* In both and unchanged (except logging may have changed) */ if (adding) { MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_nop); if (frold->dirty_log_mask && !frnew->dirty_log_mask) { MEMORY_LISTENER_UPDATE_REGION(frnew, as, Reverse, log_stop); } else if (frnew->dirty_log_mask && !frold->dirty_log_mask) { MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, log_start); } } ++iold; ++inew; } else { /* In new */ if (adding) { MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_add); } ++inew; } } 重点在MEMORY_LISTENER_UPDATE_REGION函数上，将变化的FlatRange构造一个MemoryRegionSection，然后遍历所有的memory_listeners，如果memory_listeners监控的内存区域和MemoryRegionSection一样，则执行第四个入参函数，如region_del函数，即kvm_region_del函数，这个是在kvm_init中初始化的。kvm_region_del主要是kvm_set_phys_mem函数，主要是将MemoryRegionSection有效值转换成KVMSlot形式，在kvm_set_user_memory_region中使用kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &amp;mem)传递给kernel。我们看内存初始化真正需要做的是什么？就是qemu申请内存，把申请物理地址传递给kernel进行映射，那我们直接就可以KVMSlot申请内存，然后传递给kvm_vm_ioctl，这样也是OK的，之所以有这么多代码，因为qemu本身是一个软件虚拟机，mr涉及的地址已经是vm的地址，对于KVM是多余的，只是方便函数复用而已。内存初始化之后还是pci等处理先跳过，如此pc_init就完成了，但是前面VM线程已经初始化成功，在qemu_kvm_cpu_thread_fn函数中等待运行： while (1) { if (cpu_can_run(cpu)) { r = kvm_cpu_exec(cpu); if (r == EXCP_DEBUG) { cpu_handle_guest_debug(cpu); } } qemu_kvm_wait_io_event(cpu); } 判断条件就是cpu_can_run函数，即cpu-&gt;stop &amp;&amp; cpu-&gt;stopped &amp;&amp; current_run_state ！= running 都是false，而这几个参数都是由vm_start函数决定的 void vm_start(void){ if (!runstate_is_running()) { cpu_enable_ticks(); runstate_set(RUN_STATE_RUNNING); vm_state_notify(1, RUN_STATE_RUNNING); resume_all_vcpus(); monitor_protocol_event(QEVENT_RESUME, NULL); } } 如此kvm_cpu_exec就真正进入执行阶段，即通过kvm_vcpu_ioctl传递KVM_RUN给内核。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM源代码分析1:基本工作原理]]></title>
    <url>%2F2014%2F12%2F11%2Fkvm-src-1%2F</url>
    <content type="text"><![CDATA[原文出自：http://oenhan.com/kvm-src-1文章写作以及技术水平远远在我之上，感觉自己无力写出如此精炼的文章，膜拜并转发 1.KVM模型结构为什么有OS虚拟化？随着CPU计算能力的提高，单独的OS已不能充分利用CPU的计算能力，1.很多应用的执行需要单独占用一个OS环境，如安全测试等；2.而IAAS云计算厂商也是以OS为范围销售计算能力。那么在所有虚拟化方案中，都是由hypervisor取代原生的OS去控制具体硬件资源，而同时hypervisor将资源分配具体的VM，VM中运行的是没有修改过的OS，如果让VM中的OS能正常运行，hypervisor的任务就是模拟具体的硬件资源，让OS不能识别出是真是假。 当然上面的模型是Xen示例，OS对应用而言是硬件资源管理中心，那么hypervisor就是具体VM的OS了，KVM是就利用了这一点，利用现有的kernel代码，构建了一个hypervisor，这个样子内存分配，进程调度等就无需重写代码，如此hypervisor就是所谓的host，VM中的OS就是guest。 guest OS保证具体运行场景中的程序正常执行，而KVM的代码则部署在HOST上，Userspace对应的是QEMU，Kernel对应的是KVM Driver，KVM Driver负责模拟虚拟机的CPU运行，内存管理，设备管理等；QEMU则模拟虚拟机的IO设备接口以及用户态控制接口。QEMU通过KVM等fd进行IOCTL控制KVM驱动的运行过程。 如上图所示，guest自身有自己的用户模式和内核模式；guest是在host中是作为一个用户态进程存在的，这个进程就是qemu，qemu本身就是一个虚拟化程序，只是纯软件虚拟化效率很低，它被KVM进行改造后，作为KVM的前端存在，用来进行创建进程或者IO交互等；而KVM Driver则是Linux内核模式，它提供KVM fd给qemu调用，用来进行cpu虚拟化，内存虚拟化等。QEMU通KVM提供的fd接口，通过ioctl系统调用创建和运行虚拟机。KVM Driver使得整个Linux成为一个虚拟机监控器，负责接收qemu模拟效率很低的命令。 2.KVM工作原理 上图是一个执行过程图，首先启动一个虚拟化管理软件qemu，开始启动一个虚拟机，通过ioctl等系统调用向内核中申请指定的资源，搭建好虚拟环境，启动虚拟机内的OS，执行 VMLAUCH 指令，即进入了guest代码执行过程。如果 Guest OS 发生外部中断或者影子页表缺页之类的事件，暂停 Guest OS 的执行，退出QEMU即guest VM-exit，进行一些必要的处理，然后重新进入客户模式，执行guest代码；这个时候如果是io请求，则提交给用户态下的qemu处理，qemu处理后再次通过IOCTL反馈给KVM驱动。 3.CPU虚拟化X86体系结构CPU虚拟化技术的称为 Intel VT-x 技术，引入了VMX，提供了两种处理器的工作环境。 VMCS 结构实现两种环境之间的切换。 VM Entry 使虚拟机进去guest模式，VM Exit 使虚拟机退出guest模式。 VMM调度guest执行时，qemu 通过 ioctl 系统调用进入内核模式，在 KVM Driver中获得当前物理 CPU的引用。之后将guest状态从VMCS中读出， 并装入物理CPU中。执行 VMLAUCH 指令使得物理处理器进入非根操作环境，运行guest OS代码。 当 guest OS 执行一些特权指令或者外部事件时， 比如I/O访问，对控制寄存器的操作，MSR的读写等， 都会导致物理CPU发生 VMExit， 停止运行 Guest OS，将 Guest OS保存到VMCS中， Host 状态装入物理处理器中， 处理器进入根操作环境，KVM取得控制权，通过读取 VMCS 中 VM_EXIT_REASON 字段得到引起 VM Exit 的原因。 从而调用kvm_exit_handler 处理函数。 如果由于 I/O 获得信号到达，则退出到userspace模式的 Qemu 处理。处理完毕后，重新进入guest模式运行虚拟 CPU。 4.Mem虚拟化OS对于物理内存主要有两点认识：1.物理地址从0开始；2.内存地址是连续的。VMM接管了所有内存，但guest OS的对内存的使用就存在这两点冲突了，除此之外，一个guest对内存的操作很有可能影响到另外一个guest乃至host的运行。VMM的内存虚拟化就要解决这些问题。 在OS代码中，应用也是占用所有的逻辑地址，同时不影响其他应用的关键点在于有线性地址这个中间层；解决方法则是添加了一个中间层：guest物理地址空间；guest看到是从0开始的guest物理地址空间（类比从0开始的线性地址），而且是连续的，虽然有些地址没有映射；同时guest物理地址映射到不同的host逻辑地址，如此保证了VM之间的安全性要求。 这样MEM虚拟化就是GVA-&gt;GPA-&gt;HPA的寻址过程，传统软件方法有影子页表，硬件虚拟化提供了EPT支持。 总体描述到此，后面代码里面见真相。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2014%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment workflow st=>start: 开始 op=>operation: My Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op &{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);]]></content>
  </entry>
</search>
