<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[KVM 虚拟化原理4--内存]]></title>
    <url>%2F2018%2F12%2F10%2Fkvm-memory%2F</url>
    <content type="text"><![CDATA[内存虚拟化简介前一章介绍了CPU虚拟化的内容，这一章介绍一下KVM的内存虚拟化原理。可以说内存是除了CPU外最重要的组件，Guest最终使用的还是宿主机的内存，所以内存虚拟化其实就是关于如何做Guest到宿主机物理内存之间的各种地址转换，如何转换会让转换效率更高呢，KVM经历了三代的内存虚拟化技术，大大加快了内存的访问速率。 传统的地址转换在保护模式下，普通的应用进程使用的都是自己的虚拟地址空间，一个64位的机器上的每一个进程都可以访问0到2^64的地址范围，实际上内存并没有这么多，也不会给你这么多。对于进程而言，他拥有所有的内存，对内核而言，只分配了一小段内存给进程，待进程需要更多的进程的时候再分配给进程。通常应用进程所使用的内存叫做虚拟地址，而内核所使用的是物理内存。内核负责为每个进程维护虚拟地址到物理内存的转换关系映射。首先，逻辑地址需要转换为线性地址，然后由线性地址转换为物理地址。 1逻辑地址 ==&gt; 线性地址 ==&gt; 物理地址 逻辑地址和线性地址之间通过简单的偏移来完成。 一个完整的逻辑地址 = [段选择符：段内偏移地址]，查找GDT或者LDT（通过寄存器gdtr，ldtr）找到描述符，通过段选择符(selector)前13位在段描述符做index，找到Base地址，Base+offset就是线性地址。 为什么要这么做？据说是Intel为了保证兼容性。 逻辑地址到线性地址的转换在虚拟化中没有太多的需要介绍的，这一层不存在实际的虚拟化操作，和传统方式一样，最重要的是线性地址到物理地址这一层的转换。 传统的线性地址到物理地址的转换由CPU的页式内存管理，页式内存管理。页式内存管理负责将线性地址转换到物理地址，一个线性地址被分五段描述，第一段为基地址，通过与当前CR3寄存器（CR3寄存器每个进程有一个，线程共享，当发生进程切换的时候，CR3被载入到对应的寄存器中，这也是各个进程的内存隔离的基础）做运算，得到页表的地址index，通过四次运算，最终得到一个大小为4K的页（有可能更大，比如设置了hugepages以后）。整个过程都是CPU完成，进程不需要参与其中，如果在查询中发现页已经存在，直接返回物理地址，如果页不存在，那么将产生一个缺页中断，内核负责处理缺页中断，并把页加载到页表中，中断返回后，CPU获取到页地址后继续进行运算。 KVM中的内存结构由于qemu-kvm进程在宿主机上作为一个普通进程，那对于Guest而言，需要的转换过程就是这样。 12345678910111213Guest虚拟内存地址(GVA) | Guest线性地址 | Guest物理地址(GPA) | Guest ------------------ | HV HV虚拟地址(HVA) | HV线性地址 | HV物理地址(HPA) What’s the fu*k ？这么多…别着急，Guest虚拟地址到HV线性地址之间的转换和HV虚拟地址到线性地址的转换过程可以省略，这样看起来就更清晰一点。 123456789Guest虚拟内存地址(GVA) | Guest物理地址(GPA) | Guest ------------------ | HV HV虚拟地址(HVA) | HV物理地址(HPA) 前面也说到KVM通过不断的改进转换过程，让KVM的内存虚拟化更加的高效，我们从最初的软件虚拟化的方式介绍。 软件虚拟化方式实现第一层转换，由GVA-&gt;GPA的转换和传统的转换关系一样，通过查找CR3然后进行页表查询，找到对应的GPA，GPA到HVA的关系由qemu-kvm负责维护，我们在第二章KVM启动过程的demo里面就有介绍到怎样给KVM映射内存，通过mmap的方式把HV的内存映射给Guest。 123456struct kvm_userspace_memory_region region = &#123; .slot = 0, .guest_phys_addr = 0x1000, .memory_size = 0x1000, .userspace_addr = (uint64_t)mem,&#125;; 可以看到，qemu-kvm的kvm_userspace_memory_region结构体描述了guest的物理地址起始位置和内存大小，然后描述了Guest的物理内存在HV的映射userspace_addr，通过多个slot，可以把不连续的HV的虚拟地址空间映射给Guest的连续的物理地址空间。 软件模拟的虚拟化方式由qemu-kvm来负责维护GPA-&gt;HVA的转换，然后再经过一次HVA-&gt;HPA的方式，从过程上来看，这样的访问是很低效的，特别是在当GVA到GPA转换时候产生缺页中断，这时候产生一个异常Guest退出，HV捕获异常后计算出物理地址（分配新的内存给Guest），然后重新Entry。这个过程会可能导致频繁的Guest退出，且转换过程过长。于是KVM使用了一种叫做影子页表的技术。 影子页表的虚拟化方式影子页表的出现，就是为了减少地址转换带来的开销，直接把GVA转换到HVP的技术。在软件虚拟化的内存转换中，GVA到GPA的转换通过查询CR3寄存器来完成，CR3保存了Guest中的页表基地址，然后载入MMU来做地址转换。在加入了影子页表的技术后，当访问到CR3寄存器的时候（可能是由于Guest进程后导致的），KVM捕获到这个操作，CPU虚拟化章节 EXIT_REASON_CR_ACCESS，qemu-kvm通过载入特俗的CR3和影子页表来欺骗Guest这个就是真实的CR3，后面的操作就和传统的访问内存的方式一致，当需要访问物理内存的时候，只会经过一层的影子页表的转换。 影子页表由qemu-kvm进程维护，实际上就是一个Guest的页表到宿主机页表的映射，每一级的页表的hash值对应到qemu-kvm中影子页表的一个目录。在初次GVA-&gt;HPA的转换时候，影子页表没有建立，此时Guest产生缺页中断，和传统的转换过程一样，经过两次转换(VA-&gt;PA)，然后影子页表记录GVA-&gt;GPA-&gt;HVA-&gt;HPA。这样产生GVA-&gt;GPA的直接关系，保存到影子页表中。 影子页表的引入，减少了GVA-&gt;HPA的转换过程，但是坏处在于qemu-kvm需要为Guest的每个进程维护一个影子页表，这将带来很大的内存开销，同时影子页表的建立是很耗时的，如果Guest进程过多，将导致频繁的影子页表的导入与导出，虽然用了cache技术，但是还是软件层面的，效率并不是最好，所以Intel和AMD在此基础上提供了硬件虚拟化技术。 EPT硬件加速的虚拟化方式EPT(extended page table)可以看做一个硬件的影子页表，在Guest中通过增加EPT寄存器，当Guest产生了CR3和页表的访问的时候，由于对CR3中的页表地址的访问是GPA，当地址为空时候，也就是Page fault后，产生缺页异常，如果在软件模拟或者影子页表的虚拟化方式中，此时会有VM退出，qemu-kvm进程接管并获取到此异常。但是在EPT的虚拟化方式中，qemu-kvm忽略此异常，Guest并不退出，而是按照传统的缺页中断处理，在缺页中断处理的过程中会产生EXIT_REASON_EPT_VIOLATION，Guest退出，qemu-kvm捕获到异常后，分配物理地址并建立GVA-&gt;HPA的映射，并保存到EPT中，将EPT载入到MMU，下次转换时候直接查询根据CR3查询EPT表来完成GVA-&gt;HPA的转换。以后的转换都由硬件直接完成，大大提高了效率，且不需要为每个进程维护一套页表，减少了内存开销。在笔者的测试中，Guest和HV的内存访问速率对比为3756MB/s对比4340MB/s。可以看到内存访问已经很接近宿主机的水平了。 总结KVM内存的虚拟化就是一个将虚拟机的虚拟内存转换为宿主机物理内存的过程，Guest使用的依然是宿主机的物理内存，只是在这个过程中怎样减少转换带来的开销成为优化的主要点。KVM经过软件模拟-&gt;影子页表-&gt;EPT的技术的进化，效率也越来越高。]]></content>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟化原理3--CPU]]></title>
    <url>%2F2018%2F12%2F10%2Fkvm-cpu%2F</url>
    <content type="text"><![CDATA[CPU 虚拟化简介上一篇文章笼统的介绍了一个虚拟机的诞生过程，从demo中也可以看到，运行一个虚拟机再也不需要像以前想象的那样，需要用软件来模拟硬件指令集了。虚拟机的指令集直接运行在宿主机物理CPU上，当虚拟机中的指令设计到IO操作或者一些特殊指令的时候，控制权转让给了宿主机（这里其实是转让给了vm monitor，下面检查VMM），也就是一个demo进程，他在宿主机上的表现形式也就是一个用户级进程。 用一张图来解释更为贴切。 VMM完成vCPU，内存的初始化后，通过ioctl调用KVM的接口，完成虚拟机的创建，并创建一个线程来运行VM，由于VM在前期初始化的时候会设置各种寄存器来帮助KVM查找到需要加载的指令的入口（main函数）。所以线程在调用了KVM接口后，物理CPU的控制权就交给了VM。VM运行在VMX non-root模式，这是Intel-V或者AMD-V提供的一种特殊的CPU执行模式。然后当VM执行了特殊指令的时候，CPU将当前VM的上下文保存到VMCS寄存器（这个寄存器是一个指针，保存了实际的上下文地址），然后执行权切换到VMM。VMM 获取 VM 返回原因，并做处理。如果是IO请求，VMM 可以直接读取VM的内存并将IO操作模拟出来，然后再调用VMRESUME指令，VM继续执行，此时在VM看来，IO操作的指令被CPU执行了。 Intel-V 技术Intel-V 技术是Intel为了支持虚拟化而提供的一套CPU特殊运行模式。 Intel-V虚拟化技术结构Intel-V 在IA-32处理器上扩展了处理器等级，原来的CPU支持ring0~ring3 4个等级，但是Linux只使用了其中的两个ring0,ring3。当CPU寄存器标示了当前CPU处于ring0级别的时候，表示此时CPU正在运行的是内核的代码。而当CPU处于ring3级别的时候，表示此时CPU正在运行的是用户级别的代码。当发生系统调用或者进程切换的时候，CPU会从ring3级别转到ring0级别。ring3级别是不允许执行硬件操作的，所有硬件操作都需要系统提供的API来完成。比如说一个IO操作： 1int nread = read(fd, buffer, 1024); 当执行到此段代码的时候，然后查找到系统调用号，保存到寄存器eax，然后会将对应的参数压栈后产生一个系统调用中断，对应的是 int $0x80。产生了系统调用中断后，此时CPU将切换到ring0模式，内核通过寄存器读取到参数，并完成最后的IO后续操作，操作完成后返回ring3模式。 12345movel $3,%eaxmovel fd,%ebxmovel buffer,%ecxmovel 1024,%edx int $0x80 Intel-V 在 ring0~ring3 的基础上，增加了VMX模式，VMX分为root和non-root。这里的VMX root模式是给VMM（前面有提到VM monitor)，在KVM体系中，就是qemu-kvm进程所运行的模式。VMX non-root模式就是运行的Guest，Guest也分ring0~ring3，不过他并不感知自己处于VMX non-root模式下。 Intel的虚拟架构基本上分两个部分: 虚拟机监视器 客户机（Guest VM) 虚拟机监视器（Virtual-machine monitors - VMM)虚拟机监视器在宿主机上表现为一个提供虚拟机CPU，内存以及一系列硬件虚拟的实体，这个实体在KVM体系中就是一个进程，如qemu-kvm。VMM负责管理虚拟机的资源，并拥有所有虚拟机资源的控制权，包括切换虚拟机的CPU上下文等。 Guest这个Guest在前面的Demo里面也提到，可能是一个操作系统（OS），也可能就是一个二进制程序，whatever，对于VMM来说，他就是一堆指令集，只需要知道入口（rip寄存器值）就可以加载。Guest运行需要虚拟CPU，当Guest代码运行的时候，处于VMX non-root模式，此模式下，该用什么指令还是用什么指令，该用寄存器该用cache还是用cache，但是在执行到特殊指令的时候（比如Demo中的out指令），把CPU控制权交给VMM，由VMM来处理特殊指令，完成硬件操作。 VMM 与 Guest 的切换 Guest与VMM之间的切换分两个部分：VM entry 和 VM exit。有几种情况会导致VM exit，比如说Guest执行了硬件访问操作，或者Guest调用了VMCALL指令或者调用了退出指令或者产生了一个page fault，或者访问了特殊设备的寄存器等。当Guest处于VMX模式的时候，没有提供获取是否处于此模式下的指令或者寄存器，也就是说，Guest不能判断当前CPU是否处于VMX模式。当产生VM exit的时候，CPU会将exit reason保存到MSRs（VMX模式的特殊寄存器组），对应到KVM就是vCPU-&gt;kvm_run-&gt;exit_reason。VMM根据exit_reason做相应的处理。 VMM 的生命周期如上图所示，VMM 开始于VMXON 指令，结束与VMXOFF指令。第一次启动Guest，通过VMLAUNCH指令加载Guest，这时候一切都是新的，比如说起始的rip寄存器等。后续Guest exit后再entry，是通过VMRESUME指令，此指令会将VMCS(后面会介绍到）所指向的内容加载到当前Guest的上下文，以便Guest继续执行。 VMCS （Virtual-Machine control structure)顾名思义，VMCS就是虚拟机控制结构，前面提到过很多次，Guest Exit的时候，会将当前Guest的上下文保存到VMCS中，Guest entry的时候把VMCS上下文恢复到VMM。VMCS是一个64位的指针，指向一个真实的内存地址，VMCS是以vCPU为单位的，就是说当前有多少个vCPU，就有多少个VMCS指针。VMCS的操作包括VMREAD，VMWRITE，VMCLEAR。 Guest exit Reason下面是qemu-kvm定义的exit reason。可以看到有很多可能会导致Guest转让控制权。选取几个解释一下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = &#123; [EXIT_REASON_EXCEPTION_NMI] = handle_exception, [EXIT_REASON_EXTERNAL_INTERRUPT] = handle_external_interrupt, [EXIT_REASON_TRIPLE_FAULT] = handle_triple_fault, [EXIT_REASON_NMI_WINDOW] = handle_nmi_window, // 访问了IO设备 [EXIT_REASON_IO_INSTRUCTION] = handle_io, // 访问了CR寄存器，地址寄存器，和DR寄存器（debug register)一样，用于调试 [EXIT_REASON_CR_ACCESS] = handle_cr, [EXIT_REASON_DR_ACCESS] = handle_dr, [EXIT_REASON_CPUID] = handle_cpuid, // 访问了MSR寄存器 [EXIT_REASON_MSR_READ] = handle_rdmsr, [EXIT_REASON_MSR_WRITE] = handle_wrmsr, [EXIT_REASON_PENDING_INTERRUPT] = handle_interrupt_window, // Guest执行了HLT指令，Demo开胃菜就是这个指令 [EXIT_REASON_HLT] = handle_halt, [EXIT_REASON_INVD] = handle_invd, [EXIT_REASON_INVLPG] = handle_invlpg, [EXIT_REASON_RDPMC] = handle_rdpmc, // 不太清楚以下VM系列的指令有什么用，猜测是递归VM（虚拟机里面运行虚拟机） [EXIT_REASON_VMCALL] = handle_vmcall, [EXIT_REASON_VMCLEAR] = handle_vmclear, [EXIT_REASON_VMLAUNCH] = handle_vmlaunch, [EXIT_REASON_VMPTRLD] = handle_vmptrld, [EXIT_REASON_VMPTRST] = handle_vmptrst, [EXIT_REASON_VMREAD] = handle_vmread, [EXIT_REASON_VMRESUME] = handle_vmresume, [EXIT_REASON_VMWRITE] = handle_vmwrite, [EXIT_REASON_VMOFF] = handle_vmoff, [EXIT_REASON_VMON] = handle_vmon, [EXIT_REASON_TPR_BELOW_THRESHOLD] = handle_tpr_below_threshold, // 访问了高级PCI设备 [EXIT_REASON_APIC_ACCESS] = handle_apic_access, [EXIT_REASON_APIC_WRITE] = handle_apic_write, [EXIT_REASON_EOI_INDUCED] = handle_apic_eoi_induced, [EXIT_REASON_WBINVD] = handle_wbinvd, [EXIT_REASON_XSETBV] = handle_xsetbv, // 进程切换 [EXIT_REASON_TASK_SWITCH] = handle_task_switch, [EXIT_REASON_MCE_DURING_VMENTRY] = handle_machine_check, // ept 是Intel的一个硬件内存虚拟化技术 [EXIT_REASON_EPT_VIOLATION] = handle_ept_violation, [EXIT_REASON_EPT_MISCONFIG] = handle_ept_misconfig, // 执行了暂停指令 [EXIT_REASON_PAUSE_INSTRUCTION] = handle_pause, [EXIT_REASON_MWAIT_INSTRUCTION] = handle_invalid_op, [EXIT_REASON_MONITOR_INSTRUCTION] = handle_invalid_op, [EXIT_REASON_INVEPT] = handle_invept,&#125;; 总结KVM的CPU虚拟化依托于Intel-V提供的虚拟化技术，将Guest运行于VMX模式，当执行了特殊操作的时候，将控制权返回给VMM。VMM处理完特殊操作后再把结果返回给Guest。CPU虚拟化可以说是KVM的最关键的核心，弄清楚了VM Exit和VM Entry。后续的IO虚拟化，内存虚拟化都是建立在此基础上。下一章介绍内存虚拟化。]]></content>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟化原理2— QEMU启动过程]]></title>
    <url>%2F2018%2F12%2F10%2Fkvm-boot%2F</url>
    <content type="text"><![CDATA[虚拟机启动过程1234567891011121314第一步，获取到kvm句柄kvmfd = open(&quot;/dev/kvm&quot;, O_RDWR);第二步，创建虚拟机，获取到虚拟机句柄。vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0);第三步，为虚拟机映射内存，还有其他的PCI，信号处理的初始化。ioctl(kvmfd, KVM_SET_USER_MEMORY_REGION, &amp;mem);第四步，将虚拟机镜像映射到内存，相当于物理机的boot过程，把镜像映射到内存。第五步，创建vCPU，并为vCPU分配内存空间。ioctl(kvmfd, KVM_CREATE_VCPU, vcpuid);vcpu-&gt;kvm_run_mmap_size = ioctl(kvm-&gt;dev_fd, KVM_GET_VCPU_MMAP_SIZE, 0);第五步，创建vCPU个数的线程并运行虚拟机。ioctl(kvm-&gt;vcpus-&gt;vcpu_fd, KVM_RUN, 0);第六步，线程进入循环，并捕获虚拟机退出原因，做相应的处理。这里的退出并不一定是虚拟机关机，虚拟机如果遇到IO操作，访问硬件设备，缺页中断等都会退出执行，退出执行可以理解为将CPU执行上下文返回到QEMU。 12345678910open(&quot;/dev/kvm&quot;)ioctl(KVM_CREATE_VM)ioctl(KVM_CREATE_VCPU)for (;;) &#123; ioctl(KVM_RUN) switch (exit_reason) &#123; case KVM_EXIT_IO: /* ... */ case KVM_EXIT_HLT: /* ... */ &#125;&#125; 关于KVM_CREATE_VM参数的描述，创建的VM是没有cpu和内存的，需要QEMU进程利用mmap系统调用映射一块内存给VM的描述符，其实也就是给VM创建内存的过程。 KVM ioctl接口文档 先来一个KVM API开胃菜下面是一个KVM的简单demo，其目的在于加载 code 并使用KVM运行起来.这是一个at&amp;t的8086汇编，.code16表示他是一个16位的，当然直接运行是运行不起来的，为了让他运行起来，我们可以用KVM提供的API，将这个程序看做一个最简单的操作系统，让其运行起来。这个汇编的作用是输出al寄存器的值到0x3f8端口。对于x86架构来说，通过IN/OUT指令访问。PC架构一共有65536个8bit的I/O端口，组成64KI/O地址空间，编号从0~0xFFFF。连续两个8bit的端口可以组成一个16bit的端口，连续4个组成一个32bit的端口。I/O地址空间和CPU的物理地址空间是两个不同的概念，例如I/O地址空间为64K，一个32bit的CPU物理地址空间是4G。最终程序理想的输出应该是，al，bl的值后面KVM初始化的时候有赋值。4\n (并不直接输出\n，而是换了一行），hlt 指令表示虚拟机退出 12345678910.globl _start .code16_start: mov $0x3f8, %dx add %bl, %al add $&apos;0&apos;, %al out %al, (%dx) mov $&apos;\n&apos;, %al out %al, (%dx) hlt 我们编译一下这个汇编，得到一个 Bin.bin 的二进制文件 12as -32 bin.S -o bin.old -m elf_i386 --oformat binary -N -e _start -Ttext 0x10000 -o Bin.bin bin.o 查看一下二进制格式 12345678910111213➜ demo1 hexdump -C bin.bin00000000 ba f8 03 00 d8 04 30 ee b0 0a ee f4 |......0.....|0000000c对应了下面的code数组，这样直接加载字节码就不需要再从文件加载了 const uint8_t code[] = &#123; 0xba, 0xf8, 0x03, /* mov $0x3f8, %dx */ 0x00, 0xd8, /* add %bl, %al */ 0x04, &apos;0&apos;, /* add $&apos;0&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xb0, &apos;\n&apos;, /* mov $&apos;\n&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xf4, /* hlt */ &#125;; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#include &lt;err.h&gt;#include &lt;fcntl.h&gt;#include &lt;linux/kvm.h&gt;#include &lt;stdint.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/ioctl.h&gt;#include &lt;sys/mman.h&gt;#include &lt;sys/stat.h&gt;#include &lt;sys/types.h&gt;int main(void)&#123; int kvm, vmfd, vcpufd, ret; const uint8_t code[] = &#123; 0xba, 0xf8, 0x03, /* mov $0x3f8, %dx */ 0x00, 0xd8, /* add %bl, %al */ 0x04, &apos;0&apos;, /* add $&apos;0&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xb0, &apos;\n&apos;, /* mov $&apos;\n&apos;, %al */ 0xee, /* out %al, (%dx) */ 0xf4, /* hlt */ &#125;; uint8_t *mem; struct kvm_sregs sregs; size_t mmap_size; struct kvm_run *run; // 获取 kvm 句柄 kvm = open(&quot;/dev/kvm&quot;, O_RDWR | O_CLOEXEC); if (kvm == -1) err(1, &quot;/dev/kvm&quot;); // 确保是正确的 API 版本 ret = ioctl(kvm, KVM_GET_API_VERSION, NULL); if (ret == -1) err(1, &quot;KVM_GET_API_VERSION&quot;); if (ret != 12) errx(1, &quot;KVM_GET_API_VERSION %d, expected 12&quot;, ret); // 创建一虚拟机 vmfd = ioctl(kvm, KVM_CREATE_VM, (unsigned long)0); if (vmfd == -1) err(1, &quot;KVM_CREATE_VM&quot;); // 为这个虚拟机申请内存，并将代码（镜像）加载到虚拟机内存中 mem = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0); if (!mem) err(1, &quot;allocating guest memory&quot;); memcpy(mem, code, sizeof(code)); // 为什么从 0x1000 开始呢，因为页表空间的前4K是留给页表目录 struct kvm_userspace_memory_region region = &#123; .slot = 0, .guest_phys_addr = 0x1000, .memory_size = 0x1000, .userspace_addr = (uint64_t)mem, &#125;; // 设置 KVM 的内存区域 ret = ioctl(vmfd, KVM_SET_USER_MEMORY_REGION, &amp;region); if (ret == -1) err(1, &quot;KVM_SET_USER_MEMORY_REGION&quot;); // 创建虚拟CPU vcpufd = ioctl(vmfd, KVM_CREATE_VCPU, (unsigned long)0); if (vcpufd == -1) err(1, &quot;KVM_CREATE_VCPU&quot;); // 获取 KVM 运行时结构的大小 ret = ioctl(kvm, KVM_GET_VCPU_MMAP_SIZE, NULL); if (ret == -1) err(1, &quot;KVM_GET_VCPU_MMAP_SIZE&quot;); mmap_size = ret; if (mmap_size &lt; sizeof(*run)) errx(1, &quot;KVM_GET_VCPU_MMAP_SIZE unexpectedly small&quot;); // 将 kvm run 与 vcpu 做关联，这样能够获取到kvm的运行时信息 run = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED, vcpufd, 0); if (!run) err(1, &quot;mmap vcpu&quot;); // 获取特殊寄存器 ret = ioctl(vcpufd, KVM_GET_SREGS, &amp;sregs); if (ret == -1) err(1, &quot;KVM_GET_SREGS&quot;); // 设置代码段为从地址0处开始，我们的代码被加载到了0x0000的起始位置 sregs.cs.base = 0; sregs.cs.selector = 0; // KVM_SET_SREGS 设置特殊寄存器 ret = ioctl(vcpufd, KVM_SET_SREGS, &amp;sregs); if (ret == -1) err(1, &quot;KVM_SET_SREGS&quot;); // 设置代码的入口地址，相当于32位main函数的地址，这里16位汇编都是由0x1000处开始。 // 如果是正式的镜像，那么rip的值应该是类似引导扇区加载进来的指令 struct kvm_regs regs = &#123; .rip = 0x1000, .rax = 2, // 设置 ax 寄存器初始值为 2 .rbx = 2, // 同理 .rflags = 0x2, // 初始化flags寄存器，x86架构下需要设置，否则会粗错 &#125;; ret = ioctl(vcpufd, KVM_SET_REGS, &amp;regs); if (ret == -1) err(1, &quot;KVM_SET_REGS&quot;); // 开始运行虚拟机，如果是qemu-kvm，会用一个线程来执行这个vCPU，并加载指令 while (1) &#123; // 开始运行虚拟机 ret = ioctl(vcpufd, KVM_RUN, NULL); if (ret == -1) err(1, &quot;KVM_RUN&quot;); // 获取虚拟机退出原因 switch (run-&gt;exit_reason) &#123; case KVM_EXIT_HLT: puts(&quot;KVM_EXIT_HLT&quot;); return 0; // 汇编调用了 out 指令，vmx 模式下不允许执行这个操作，所以 // 将操作权切换到了宿主机，切换的时候会将上下文保存到VMCS寄存器 // 后面CPU虚拟化会讲到这部分 // 因为虚拟机的内存宿主机能够直接读取到，所以直接在宿主机上获取到 // 虚拟机的输出（out指令），这也是后面PCI设备虚拟化的一个基础，DMA模式的PCI设备 case KVM_EXIT_IO: if (run-&gt;io.direction == KVM_EXIT_IO_OUT &amp;&amp; run-&gt;io.size == 1 &amp;&amp; run-&gt;io.port == 0x3f8 &amp;&amp; run-&gt;io.count == 1) putchar(*(((char *)run) + run-&gt;io.data_offset)); else errx(1, &quot;unhandled KVM_EXIT_IO&quot;); break; case KVM_EXIT_FAIL_ENTRY: errx(1, &quot;KVM_EXIT_FAIL_ENTRY: hardware_entry_failure_reason = 0x%llx&quot;, (unsigned long long)run-&gt;fail_entry.hardware_entry_failure_reason); case KVM_EXIT_INTERNAL_ERROR: errx(1, &quot;KVM_EXIT_INTERNAL_ERROR: suberror = 0x%x&quot;, run-&gt;internal.suberror); default: errx(1, &quot;exit_reason = 0x%x&quot;, run-&gt;exit_reason); &#125; &#125;&#125; 编译并运行这个demo 1234gcc -g demo.c -o demo➜ demo1 ./demo4KVM_EXIT_HLT 另外一个简单的QEMU emulator demoIBM的徐同学有做过介绍，在此基础上我再详细介绍一下qemu-kvm的启动过程。 123456789.globl _start .code16_start: xorw %ax, %ax # 将 ax 寄存器清零loop1: out %ax, $0x10 # 像 0x10 的端口输出 ax 的内容，at&amp;t汇编的操作数和Intel的相反。 inc %ax # ax 值加一 jmp loop1 # 继续循环 这个汇编的作用就是一直不停的向0x10端口输出一字节的值。 从main函数开始说起 12345678910111213141516171819202122232425262728293031int main(int argc, char **argv) &#123; int ret = 0; // 初始化kvm结构体 struct kvm *kvm = kvm_init(); if (kvm == NULL) &#123; fprintf(stderr, &quot;kvm init fauilt\n&quot;); return -1; &#125; // 创建VM，并分配内存空间 if (kvm_create_vm(kvm, RAM_SIZE) &lt; 0) &#123; fprintf(stderr, &quot;create vm fault\n&quot;); return -1; &#125; // 加载镜像 load_binary(kvm); // only support one vcpu now kvm-&gt;vcpu_number = 1; // 创建执行现场 kvm-&gt;vcpus = kvm_init_vcpu(kvm, 0, kvm_cpu_thread); // 启动虚拟机 kvm_run_vm(kvm); kvm_clean_vm(kvm); kvm_clean_vcpu(kvm-&gt;vcpus); kvm_clean(kvm);&#125; 第一步，调用kvm_init() 初始化了 kvm 结构体。先来看看怎么定义一个简单的kvm。 1234567891011121314struct kvm &#123; int dev_fd; // /dev/kvm 的句柄 int vm_fd; // GUEST 的句柄 __u64 ram_size; // GUEST 的内存大小 __u64 ram_start; // GUEST 的内存起始地址， // 这个地址是qemu emulator通过mmap映射的地址 int kvm_version; struct kvm_userspace_memory_region mem; // slot 内存结构，由用户空间填充、 // 允许对guest的地址做分段。将多个slot组成线性地址 struct vcpu *vcpus; // vcpu 数组 int vcpu_number; // vcpu 个数&#125;; 初始化 kvm 结构体。 12345678910111213struct kvm *kvm_init(void) &#123; struct kvm *kvm = malloc(sizeof(struct kvm)); kvm-&gt;dev_fd = open(KVM_DEVICE, O_RDWR); // 打开 /dev/kvm 获取 kvm 句柄 if (kvm-&gt;dev_fd &lt; 0) &#123; perror(&quot;open kvm device fault: &quot;); return NULL; &#125; kvm-&gt;kvm_version = ioctl(kvm-&gt;dev_fd, KVM_GET_API_VERSION, 0); // 获取 kvm API 版本 return kvm;&#125; 第二步+第三步，创建虚拟机，获取到虚拟机句柄，并为其分配内存。 12345678910111213141516171819202122232425262728293031323334353637383940int kvm_create_vm(struct kvm *kvm, int ram_size) &#123; int ret = 0; // 调用 KVM_CREATE_KVM 接口获取 vm 句柄 kvm-&gt;vm_fd = ioctl(kvm-&gt;dev_fd, KVM_CREATE_VM, 0); if (kvm-&gt;vm_fd &lt; 0) &#123; perror(&quot;can not create vm&quot;); return -1; &#125; // 为 kvm 分配内存。通过系统调用. kvm-&gt;ram_size = ram_size; kvm-&gt;ram_start = (__u64)mmap(NULL, kvm-&gt;ram_size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_NORESERVE, -1, 0); if ((void *)kvm-&gt;ram_start == MAP_FAILED) &#123; perror(&quot;can not mmap ram&quot;); return -1; &#125; // kvm-&gt;mem 结构需要初始化后传递给 KVM_SET_USER_MEMORY_REGION 接口 // 只有一个内存槽 kvm-&gt;mem.slot = 0; // guest 物理内存起始地址 kvm-&gt;mem.guest_phys_addr = 0; // 虚拟机内存大小 kvm-&gt;mem.memory_size = kvm-&gt;ram_size; // 虚拟机内存在host上的用户空间地址，这里就是绑定内存给guest kvm-&gt;mem.userspace_addr = kvm-&gt;ram_start; // 调用 KVM_SET_USER_MEMORY_REGION 为虚拟机分配内存。 ret = ioctl(kvm-&gt;vm_fd, KVM_SET_USER_MEMORY_REGION, &amp;(kvm-&gt;mem)); if (ret &lt; 0) &#123; perror(&quot;can not set user memory region&quot;); return ret; &#125; return ret;&#125; 接下来就是load_binary把二进制文件load到虚拟机的内存中来，在第一个demo中我们是直接把字节码放到了内存中，这里模拟镜像加载步骤，把二进制文件加载到内存中。 1234567891011121314151617181920void load_binary(struct kvm *kvm) &#123; int fd = open(BINARY_FILE, O_RDONLY); // 打开这个二进制文件(镜像） if (fd &lt; 0) &#123; fprintf(stderr, &quot;can not open binary file\n&quot;); exit(1); &#125; int ret = 0; char *p = (char *)kvm-&gt;ram_start; while(1) &#123; ret = read(fd, p, 4096); // 将镜像内容加载到虚拟机的内存中 if (ret &lt;= 0) &#123; break; &#125; printf(&quot;read size: %d&quot;, ret); p += ret; &#125;&#125; 加载完镜像后，需要初始化vCPU，以便能够运行镜像内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct vcpu &#123; int vcpu_id; // vCPU id，vCPU int vcpu_fd; // vCPU 句柄 pthread_t vcpu_thread; // vCPU 线程句柄 struct kvm_run *kvm_run; // KVM 运行时结构，也可以看做是上下文 int kvm_run_mmap_size; // 运行时结构大小 struct kvm_regs regs; // vCPU的寄存器 struct kvm_sregs sregs; // vCPU的特殊寄存器 void *(*vcpu_thread_func)(void *); // 线程执行函数&#125;;struct vcpu *kvm_init_vcpu(struct kvm *kvm, int vcpu_id, void *(*fn)(void *)) &#123; // 申请vcpu结构 struct vcpu *vcpu = malloc(sizeof(struct vcpu)); // 只有一个 vCPU，所以这里只初始化一个 vcpu-&gt;vcpu_id = 0; // 调用 KVM_CREATE_VCPU 获取 vCPU 句柄，并关联到kvm-&gt;vm_fd（由KVM_CREATE_VM返回） vcpu-&gt;vcpu_fd = ioctl(kvm-&gt;vm_fd, KVM_CREATE_VCPU, vcpu-&gt;vcpu_id); if (vcpu-&gt;vcpu_fd &lt; 0) &#123; perror(&quot;can not create vcpu&quot;); return NULL; &#125; // 获取KVM运行时结构大小 vcpu-&gt;kvm_run_mmap_size = ioctl(kvm-&gt;dev_fd, KVM_GET_VCPU_MMAP_SIZE, 0); if (vcpu-&gt;kvm_run_mmap_size &lt; 0) &#123; perror(&quot;can not get vcpu mmsize&quot;); return NULL; &#125; printf(&quot;%d\n&quot;, vcpu-&gt;kvm_run_mmap_size); // 将 vcpu_fd 的内存映射给 vcpu-&gt;kvm_run结构。相当于一个关联操作 // 以便能够在虚拟机退出的时候获取到vCPU的返回值等信息 vcpu-&gt;kvm_run = mmap(NULL, vcpu-&gt;kvm_run_mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED, vcpu-&gt;vcpu_fd, 0); if (vcpu-&gt;kvm_run == MAP_FAILED) &#123; perror(&quot;can not mmap kvm_run&quot;); return NULL; &#125; // 设置线程执行函数 vcpu-&gt;vcpu_thread_func = fn; return vcpu;&#125; 最后一步，以上工作就绪后，启动虚拟机。 12345678910111213void kvm_run_vm(struct kvm *kvm) &#123; int i = 0; for (i = 0; i &lt; kvm-&gt;vcpu_number; i++) &#123; // 启动线程执行 vcpu_thread_func 并将 kvm 结构作为参数传递给线程 if (pthread_create(&amp;(kvm-&gt;vcpus-&gt;vcpu_thread), (const pthread_attr_t *)NULL, kvm-&gt;vcpus[i].vcpu_thread_func, kvm) != 0) &#123; perror(&quot;can not create kvm thread&quot;); exit(1); &#125; &#125; pthread_join(kvm-&gt;vcpus-&gt;vcpu_thread, NULL);&#125; 启动虚拟机其实就是创建线程，并执行相应的线程回调函数。线程回调函数在kvm_init_vcpu的时候传入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114void *kvm_cpu_thread(void *data) &#123; // 获取参数 struct kvm *kvm = (struct kvm *)data; int ret = 0; // 设置KVM的参数 kvm_reset_vcpu(kvm-&gt;vcpus); while (1) &#123; printf(&quot;KVM start run\n&quot;); // 启动虚拟机，此时的虚拟机已经有内存和CPU了，可以运行起来了。 ret = ioctl(kvm-&gt;vcpus-&gt;vcpu_fd, KVM_RUN, 0); if (ret &lt; 0) &#123; fprintf(stderr, &quot;KVM_RUN failed\n&quot;); exit(1); &#125; // 前文 kvm_init_vcpu 函数中，将 kvm_run 关联了 vCPU 结构的内存 // 所以这里虚拟机退出的时候，可以获取到 exit_reason，虚拟机退出原因 switch (kvm-&gt;vcpus-&gt;kvm_run-&gt;exit_reason) &#123; case KVM_EXIT_UNKNOWN: printf(&quot;KVM_EXIT_UNKNOWN\n&quot;); break; case KVM_EXIT_DEBUG: printf(&quot;KVM_EXIT_DEBUG\n&quot;); break; // 虚拟机执行了IO操作，虚拟机模式下的CPU会暂停虚拟机并 // 把执行权交给emulator case KVM_EXIT_IO: printf(&quot;KVM_EXIT_IO\n&quot;); printf(&quot;out port: %d, data: %d\n&quot;, kvm-&gt;vcpus-&gt;kvm_run-&gt;io.port, *(int *)((char *)(kvm-&gt;vcpus-&gt;kvm_run) + kvm-&gt;vcpus-&gt;kvm_run-&gt;io.data_offset) ); sleep(1); break; // 虚拟机执行了memory map IO操作 case KVM_EXIT_MMIO: printf(&quot;KVM_EXIT_MMIO\n&quot;); break; case KVM_EXIT_INTR: printf(&quot;KVM_EXIT_INTR\n&quot;); break; case KVM_EXIT_SHUTDOWN: printf(&quot;KVM_EXIT_SHUTDOWN\n&quot;); goto exit_kvm; break; default: printf(&quot;KVM PANIC\n&quot;); goto exit_kvm; &#125; &#125;exit_kvm: return 0;&#125;void kvm_reset_vcpu (struct vcpu *vcpu) &#123; if (ioctl(vcpu-&gt;vcpu_fd, KVM_GET_SREGS, &amp;(vcpu-&gt;sregs)) &lt; 0) &#123; perror(&quot;can not get sregs\n&quot;); exit(1); &#125; // #define CODE_START 0x1000 /* sregs 结构体 x86 struct kvm_sregs &#123; struct kvm_segment cs, ds, es, fs, gs, ss; struct kvm_segment tr, ldt; struct kvm_dtable gdt, idt; __u64 cr0, cr2, cr3, cr4, cr8; __u64 efer; __u64 apic_base; __u64 interrupt_bitmap[(KVM_NR_INTERRUPTS + 63) / 64]; &#125;; */ // cs 为code start寄存器，存放了程序的起始地址 vcpu-&gt;sregs.cs.selector = CODE_START; vcpu-&gt;sregs.cs.base = CODE_START * 16; // ss 为堆栈寄存器，存放了堆栈的起始位置 vcpu-&gt;sregs.ss.selector = CODE_START; vcpu-&gt;sregs.ss.base = CODE_START * 16; // ds 为数据段寄存器，存放了数据开始地址 vcpu-&gt;sregs.ds.selector = CODE_START; vcpu-&gt;sregs.ds.base = CODE_START *16; // es 为附加段寄存器 vcpu-&gt;sregs.es.selector = CODE_START; vcpu-&gt;sregs.es.base = CODE_START * 16; // fs, gs 同样为段寄存器 vcpu-&gt;sregs.fs.selector = CODE_START; vcpu-&gt;sregs.fs.base = CODE_START * 16; vcpu-&gt;sregs.gs.selector = CODE_START; // 为vCPU设置以上寄存器的值 if (ioctl(vcpu-&gt;vcpu_fd, KVM_SET_SREGS, &amp;vcpu-&gt;sregs) &lt; 0) &#123; perror(&quot;can not set sregs&quot;); exit(1); &#125; // 设置寄存器标志位 vcpu-&gt;regs.rflags = 0x0000000000000002ULL; // rip 表示了程序的起始指针，地址为 0x0000000 // 在加载镜像的时候，我们直接将binary读取到了虚拟机的内存起始位 // 所以虚拟机开始的时候会直接运行binary vcpu-&gt;regs.rip = 0; // rsp 为堆栈顶 vcpu-&gt;regs.rsp = 0xffffffff; // rbp 为堆栈底部 vcpu-&gt;regs.rbp= 0; if (ioctl(vcpu-&gt;vcpu_fd, KVM_SET_REGS, &amp;(vcpu-&gt;regs)) &lt; 0) &#123; perror(&quot;KVM SET REGS\n&quot;); exit(1); &#125;&#125; 运行一下结果，可以看到当虚拟机执行了指令 out %ax, $0x10 的时候，会引起虚拟机的退出，这是CPU虚拟化里面将要介绍的特殊机制。宿主机获取到虚拟机退出的原因后，获取相应的输出。这里的步骤就类似于IO虚拟化，直接读取IO模块的内存，并输出结果。 123456789101112131415161718➜ kvmsample git:(master) ✗ ./kvmsampleread size: 712288KVM start runKVM_EXIT_IOout port: 16, data: 0KVM start runKVM_EXIT_IOout port: 16, data: 1KVM start runKVM_EXIT_IOout port: 16, data: 2KVM start runKVM_EXIT_IOout port: 16, data: 3KVM start runKVM_EXIT_IOout port: 16, data: 4... 总结虚拟机的启动过程基本上可以这么总结：创建kvm句柄-&gt;创建vm-&gt;分配内存-&gt;加载镜像到内存-&gt;启动线程执行KVM_RUN。从这个虚拟机的demo可以看出，虚拟机的内存是由宿主机通过mmap调用映射给虚拟机的，而vCPU是宿主机的一个线程，这个线程通过设置相应的vCPU的寄存器指定了虚拟机的程序加载地址后，开始运行虚拟机的指令，当虚拟机执行了IO操作后，CPU捕获到中断并把执行权又交回给宿主机。 当然真实的qemu-kvm比这个复杂的多，包括设置很多IO设备的MMIO，设置信号处理等。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟化原理1 -- 概述]]></title>
    <url>%2F2018%2F12%2F10%2Fkvm-overview%2F</url>
    <content type="text"><![CDATA[KVM虚拟化简介KVM 全称 kernel-based virtual machine，由Qumranet公司发起，2008年被RedHat收购。KVM实现主要基于Intel-V或者AMD-V提供的虚拟化平台，利用Linux进程模拟虚拟机CPU和内存等。KVM不提供硬件虚拟化操作，其IO操作等都借助QEMU来完成。 Qemu 是纯软件实现的虚拟化模拟器，几乎可以模拟任何硬件设备，我们最熟悉的就是能够模拟一台能够独立运行操作系统的虚拟机，虚拟机认为自己和硬件打交道，但其实是和 Qemu 模拟出来的硬件打交道，Qemu 将这些指令转译给真正的硬件。 正因为 Qemu 是纯软件实现的，所有的指令都要经 Qemu 过一手，性能非常低，所以，在生产环境中，大多数的做法都是配合 KVM 来完成虚拟化工作，因为 KVM 是硬件辅助的虚拟化技术，主要负责 比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化，两者合作各自发挥自身的优势，相得益彰。 KVM有如下特点： guest作为一个普通进程运行于宿主机 guest的CPU(vCPU)作为进程的线程存在，并受到宿主机内核的调度 KVM整体架构 虚拟CPU虚拟机所有用户级别(user)的指令集，都会直接由宿主机线程执行，此线程会调用KVM的ioctl方式提供的接口加载guest的指令并在特殊的CPU模式下运行，不需要经过CPU指令集的软件模拟转换，大大的减少了虚拟化成本，这也是KVM优于其他虚拟化方式的点之一。 KVM向外提供了一个虚拟设备/dev/kvm，通过ioctl(IO设备带外管理接口）来对KVM进行操作，包括虚拟机的初始化，分配内存，指令加载等等。 虚拟IO设备guest作为一个进程存在，当然他的内核的所有驱动等都存在，只是硬件被QEMU所模拟。guest的所有虚拟的硬件操作都会有QEMU来接管，那些由host passthrough给guest的设备除外，QEMU负责与真实的宿主机硬件打交道。 虚拟内存guest的内存在host上由emulator提供，对emulator来说，guest访问的内存就是他的虚拟地址空间，guest上需要经过一次虚拟地址到物理地址的转换，转换到guest的物理地址其实也就是emulator的虚拟地址，emulator再次经过一次转换，转换为host的物理地址。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMP 简介]]></title>
    <url>%2F2018%2F12%2F10%2Fqmp-introduction%2F</url>
    <content type="text"><![CDATA[什么是QMP协议QMP，即QEMU Machine Protocol，就是qemu虚拟机中的一种协议，是qemu的一部分。qmp是基于json格式的一整套协议，通过这套协议我们可以控制qemu虚拟机实例的整个生命周期，包括挂起、暂停、快照、查询、外设的热插拔等，以及最简单的查询，都可以通过qmp实现。 有多种方法使用qmp，这里简要介绍通过tcp和unix socket使用qmp。 QMP协议有哪些特征1）轻量、基于文本、指令格式易于解析，因为它是json格式的；2）支持异步消息，主要指通过qmp发送给虚拟机的指令支持异步；3）Capabilities Negotiation，主要指我们初次建立qmp连接时，进入了capabilities negotiation模式,这时我们不能发送任何指令，除了qmp_capabilities指令，发送了qmp_capabilitie指令，我们就退出了capabilities negotiation模式，进入了指令模式（command mode），这时我们可以发送qmp指令，如{ “execute”: “query-status” }，这样就可以查询虚拟机的状态。 QMP协议有哪些模式 有两种模式：Capabilities Negotiation模式和Command模式。 那么该如何建立qmp连接呢这里简要介绍通过tcp和unix socket使用qmp。 通过TCP使用QMP使用-qmp添加qmp相关参数： 1./qemu-system-x86_64 -m 2048 -hda /root/centos6.img -enable-kvm -qmp tcp:localhost:1234,server,nowait 新开一个终端使用telnet 链接localhost：1234 1telnet localhost 1234 之后就可以使用qmp的命令和虚拟机交互了 123456789[root@localhost ~]# telnet localhost 1234Trying ::1...Connected to localhost.Escape character is &apos;^]&apos;.&#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;micro&quot;: 0, &quot;minor&quot;: 6, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot;&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;&#123; &quot;execute&quot;: &quot;qmp_capabilities&quot; &#125;&#123;&quot;return&quot;: &#123;&#125;&#125;&#123; &quot;execute&quot;: &quot;query-status&quot; &#125;&#123;&quot;return&quot;: &#123;&quot;status&quot;: &quot;running&quot;, &quot;singlestep&quot;: false, &quot;running&quot;: true&#125;&#125; 通过unix socket使用QMP使用unix socket创建qmp： 1./qemu-system-x86_64 -m 2048 -hda /root/centos6.img -enable-kvm -qmp unix:/tmp/qmp-test,server,nowait 使用nc连接该socket: 1nc -U /tmp/qmp-test 之后就一样了。 123456[root@localhost qmp]# nc -U /tmp/qmp-test&#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;micro&quot;: 0, &quot;minor&quot;: 6, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot;&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;&#123; &quot;execute&quot;: &quot;qmp_capabilities&quot; &#125;&#123;&quot;return&quot;: &#123;&#125;&#125;&#123; &quot;execute&quot;: &quot;query-status&quot; &#125;&#123;&quot;return&quot;: &#123;&quot;status&quot;: &quot;running&quot;, &quot;singlestep&quot;: false, &quot;running&quot;: true&#125;&#125; QMP的详细命令格式可以在qemu的代码树主目录下面的qmp-commands.hx中找到。 自动批量发送QMP命令可以通过下面这个脚本给QEMU虚拟机发送命令。这对于测试虚拟机的一些功能是很有用的。试了一下，对于unix socket的方法能使用的，对于tcp连接的方法没有使用成功。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135# QEMU Monitor Protocol Python class## Copyright (C) 2009 Red Hat Inc.## This work is licensed under the terms of the GNU GPL, version 2. See# the COPYING file in the top-level directory.import socket, json, time, commandsfrom optparse import OptionParserclass QMPError(Exception): passclass QMPConnectError(QMPError): passclass QEMUMonitorProtocol: def connect(self): print self.filename self.sock.connect(self.filename) data = self.__json_read() if data == None: raise QMPConnectError if not data.has_key(&apos;QMP&apos;): raise QMPConnectError return data[&apos;QMP&apos;][&apos;capabilities&apos;] def close(self): self.sock.close() def send_raw(self, line): self.sock.send(str(line)) return self.__json_read() def send(self, cmdline, timeout=30, convert=True): end_time = time.time() + timeout if convert: cmd = self.__build_cmd(cmdline) else: cmd = cmdline print(&quot;*cmdline = %s&quot; % cmd) print cmd self.__json_send(cmd) while time.time() &lt; end_time: resp = self.__json_read() if resp == None: return (False, None) elif resp.has_key(&apos;error&apos;): return (False, resp[&apos;error&apos;]) elif resp.has_key(&apos;return&apos;): return (True, resp[&apos;return&apos;]) def read(self, timeout=30): o = &quot;&quot; end_time = time.time() + timeout while time.time() &lt; end_time: try: o += self.sock.recv(1024) if len(o) &gt; 0: break except: time.sleep(0.01) if len(o) &gt; 0: return json.loads(o) else: return None def __build_cmd(self, cmdline): cmdargs = cmdline.split() qmpcmd = &#123; &apos;execute&apos;: cmdargs[0], &apos;arguments&apos;: &#123;&#125; &#125; for arg in cmdargs[1:]: opt = arg.split(&apos;=&apos;) try: value = int(opt[1]) except ValueError: value = opt[1] qmpcmd[&apos;arguments&apos;][opt[0]] = value print(&quot;*cmdline = %s&quot; % cmdline) return qmpcmd def __json_send(self, cmd): # XXX: We have to send any additional char, otherwise # the Server won&apos;t read our input self.sock.send(json.dumps(cmd) + &apos; &apos;) def __json_read(self): try: return json.loads(self.sock.recv(1024)) except ValueError: return def __init__(self, filename, protocol=&quot;tcp&quot;): if protocol == &quot;tcp&quot;: self.filename = (&quot;localhost&quot;, int(filename)) self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) elif protocol == &quot;unix&quot;: self.filename = filename print self.filename self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) #self.sock.setblocking(0) self.sock.settimeout(5)if __name__ == &quot;__main__&quot;: parser = OptionParser() parser.add_option(&apos;-n&apos;, &apos;--num&apos;, dest=&apos;num&apos;, default=&apos;10&apos;, help=&apos;Times want to try&apos;) parser.add_option(&apos;-f&apos;, &apos;--file&apos;, dest=&apos;port&apos;, default=&apos;4444&apos;, help=&apos;QMP port/filename&apos;) parser.add_option(&apos;-p&apos;, &apos;--protocol&apos;, dest=&apos;protocol&apos;,default=&apos;tcp&apos;, help=&apos;QMP protocol&apos;) def usage(): parser.print_help() sys.exit(1) options, args = parser.parse_args() print options if len(args) &gt; 0: usage() num = int(options.num) qmp_filename = options.port qmp_protocol = options.protocol qmp_socket = QEMUMonitorProtocol(qmp_filename,qmp_protocol) qmp_socket.connect() qmp_socket.send(&quot;qmp_capabilities&quot;) qmp_socket.close()###########################################################Usage#Options:# -h, --help show this help message and exit# -n NUM, --num=NUM Times want to try# -f PORT, --file=PORT QMP port/filename# -p PROTOCOL, --protocol=PROTOCOL# QMP protocol# e.g: # python xxxxx.py -n $NUM -f $PORT########################################################## 参考文档关于QMP更详细的文档，可以参考其官方文档：https://wiki.qemu.org/Documentation/QMP]]></content>
      <categories>
        <category>QEMU</category>
      </categories>
      <tags>
        <tag>QEMU</tag>
        <tag>QMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo+next主题]]></title>
    <url>%2F2018%2F11%2F26%2Fhexo-next%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[这篇内容详细记述了我在使用hexo搭载博客的过程中走过的路和跌过的坑。另外，我搭建了一个新的博客作为自己的技术博客，地址是xuquan.site，欢迎来逛逛~ 从印象笔记到简书到Hexo我一直有收集资料的习惯，最开始把资料都放在印象笔记里，然后自己平时处理消化之后会添加一个Learning Card作为资料开头，方便自己复习和记忆。但是时间一久，资料就特别多，加上处理过和没处理过的都积攒在一起就显得特别臃肿，于是我就考虑将消化过的内容发布到简书上，给自己做一个记录，也算是自己的技术博客。 但使用了简书3个月之后，我就发现了一些问题： 首先，我是用Typora来写内容的，简书虽然支持Markdown，但是自带的编辑器功能不是特别完善，有些时候还得反复切换Markdown和富文本模式，很麻烦； 其次，直接复制Typora的内容到简书是无法同步图片的，因为Typora的图片是放在文件夹内的asset文件夹内的，复制到简书需要图片挨个重新上传，图片多的时候简直想放弃这一篇内容； 另外，简书无法添加标签，只能分笔记本来写不同的内容，而且也不能添加置顶，功能比较单一； 在综合考虑之后，我决定搭建一个自己的博客。正好看到有人推荐hexo搭建，而且大家搭建的博客都挺赏心悦目的，我就开始动手搭建自己的了。 Hexo部署hexo有中文的文档，这一点非常方便，但是在安装过程中还是很容易有疏忽的地方，导致安装失败。 安装前提安装Hexo之前，必须保证自己的电脑中已经安装好了Node.js和Git。因为这两个软件我之前都安装过，这里就不重复安装过程了，检验方式如下： image-20180809141924679安装Hexo安装好node.js和git后，可以通过npm来安装Hexo。 npm install -g hexo-cli建站之后就可以在电脑里新建一个文件夹来作为存放博客全部内容的大本营了。我们直接用hexo命令来初始化博客文件夹： hexo init cd npm install 就是文件夹的名字，我们可以自己随意取这个名字，我的经验是，现在初始化应该不需要后面npm install这个步骤了，在创建的时候 ，文件夹初始化已经把需要的内容都下载进去了。 文件夹开始初始化了站内内容新建好的文件夹目录如下： .├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes这里解释一下各个文件夹的作用： config.yml博客的配置文件，博客的名称、关键词、作者、语言、博客主题…设置都在里面。 package.json应用程序信息，新添加的插件内容也会出现在这里面，我们可以不修改这里的内容。 scaffoldsscaffolds就是脚手架的意思，这里放了三个模板文件，分别是新添加博客文章（posts）、新添加博客页（page）和新添加草稿（draft）的目标样式。 这部分可以修改的内容是，我们可以在模板上添加比如categories等自定义内容 sourcesource是放置我们博客内容的地方，里面初始只有两个文件夹，一个是drafts（草稿），一个posts（文章），但之后我们通过命令新建tags（标签）还有categories（分类）页后，这里会相应地增加文件夹。 themes放置主题文件包的地方。Hexo会根据这个文件来生成静态页面。 初始状态下只有landscape一个文件夹，后续我们可以添加自己喜欢的。 Hexo命令init新建一个网站。 hexo init new新建文章或页面。 hexo new “title”这里的对应我们要添加的内容，如果是posts就是添加新的文章，如果是page就是添加新的页面。 默认是添加posts。 然后我们就可以在对应的posts或drafts文件夹里找到我们新建的文件，然后在文件里用Markdown的格式来写作了。 generate生成静态页面 hexo generate也可以简写成 hexo gdeploy将内容部署到网站 hexo deploy也可以简写成 hexo -dpublish发布内容，实际上是将内容从drafts（草稿）文件夹移到posts（文章）文件夹。 hexo publish server启动服务器，默认情况下，访问网站为http://localhost:4000/ hexo server也可以简写成 hexo s根据我的经验，除了第一次部署的时候，我们会重点用到hexo init这个命令外，在平时写博客和发布过程中最常用的就是： hexo n 新建文章hexo s 启动服务器，在本地查看内容hexo g 生成静态页面hexo deploy 部署到网站以上四个步骤。 其实以上命令我觉得就足够了，文档里还有很多功能，但我在实际使用的过程中都还没有遇到。 搭建好后我们在localhost:4000就可以看到这样的博客内容： image-20180809152743968实际操作我在新建博客之后，做了以下改动： 创建“分类”页面新建分类页面 hexo new page categories给分类页面添加类型 我们在source文件夹中的categories文件夹下找到index.md文件，并在它的头部加上type属性。 title: 文章分类date: 2017-05-27 13:47:40 type: “categories” #这部分是新添加的给模板添加分类属性 现在我们打开scarffolds文件夹里的post.md文件，给它的头部加上categories:，这样我们创建的所有新的文章都会自带这个属性，我们只需要往里填分类，就可以自动在网站上形成分类了。 title: hexo+next主题date: 1543200599000categories:tags:给文章添加分类 现在我们可以找到一篇文章，然后尝试给它添加分类 layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: [node.js, express] 创建“标签”页面创建”标签”页的方式和创建“分类”一样。 新建“标签”页面 hexo new page tags给标签页面添加类型 我们在source文件夹中的tags文件夹下找到index.md文件，并在它的头部加上type属性。 title: tagsdate: 2018-08-06 22:48:29type: “tags” #新添加的内容给文章添加标签 有两种写法都可以，第一种是类似数组的写法，把标签放在中括号[]里，用英文逗号隔开 layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: [node.js, express]第二种写法是用-短划线列出来 layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: node.js express部署域名紧接着我们就可以把这些内容添加到Github页面上，然后生成我们自己的博客了。 部署Github首先你必须有一个github账号 然后新建一个仓库，这一有第一个坑，我之前用了hexoblog来作为项目名称，一直没能搭建成功，后来看到其他大牛的经验，才发现项目名一定要是用户名.github.io的形式(README.md可选可不选) image-20180809153134467然后在setting里添加生成页面的选项 image-20180809153304980image-20180809153343362这个时候github页面其实就生成好了，但是我们的内容还需要同步到github上，所以打开hexo文件夹里的配置文件config.yml，添加部署路径 image-20180809153610047这里注意两小点： 属性和内容之间一定要有一个空格，配置文件有自己的格式规范如果你之前没有用git关联过自己的github库，需要配置SSH等参数，否则无法成功，这部分搜git就有很多相关教程我们再用hexo g &amp;&amp; hexo deploy就能将内容推送到github上了，在github页面上也能看到自己的内容了 image-20180809153933270部署自己的域名首先我们需要获取一个域名，我是在阿里云上购买了，上面可以根据自己想要的内容搜，比如我用了自己的名字，推荐给你的域名根据后缀不同会有价格上的区别，我选了一个不太贵的； 购买域名之后需要实名认证，这是另一个坑，我之前不知道实名认证审核完成前域名无法用，一直以为自己搭建失败了； 认证成功后需要解析域名 image-20180809154942783image-20180809155013659记录类型选CNAME，记录值是自己github生成页面的地址。 在博客的页面添加CNAME文件，并在里面记录自己域名的地址，将这个文件放在public文件夹下 这里还有一个小坑，CNAME文件经常被覆盖，导致我们重新部署博客后，链接就不可用了，这里可以下载一个叫hexo-generator-cname的插件，这样它会自动搞定CNAME的问题，只需要第一次手动将域名添加到文件里即可 npm i hexo-generator-cname –save最后hexo g &amp;&amp; hexo deploy就可以了 NexT主题hexo有很多开源的主题，我选了NexT，开始只是觉得很简洁清爽，后来发现它的功能挺齐全的，提前解决了很多搭建过程中会遇到的问题。这里强烈推荐一下。 首先，NexT也有中文文档，然后我们就可以开始了。 安装我是用的git clone的方法，文档中还有其他方法 $ git clone https://github.com/iissnan/hexo-theme-next themes/next设置主题在hexo根目录下的配置文件config.yml里设置主题 theme: next配置主题接下来我们就可以来按需配置主题内容了，所有内容都在themes/next文件夹下的config.yml文件里修改。 官方文档里写的是有些配置需要将一部分代码添加到配置文件中，但其实不用，我们逐行看配置文件就会发现，有很多功能都已经放在配置文件里了，只是注释掉了，我们只需要取消注释，把需要的相关信息补全即可使用 菜单栏 menu原生菜单栏有主页、关于、分类、标签等数个选项，但是在配置文件中是注释掉的状态，这里我们自行修改注释就行 menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive schedule: /schedule/ || calendarsitemap: /sitemap.xml || sitemapcommonweal: /404/ || heartbeat注意点： 如果事先没有通过hexo new page 来创建页面的话，即使在配置文件中取消注释，页面也没法显示我们也可以添加自己想要添加的页面，不用局限在配置文件里提供的选择里||后面是fontAwesome里的文件对应的名称menu_icons记得选enable: true（默认应该是true）我在这部分添加了两个自定义的页面，后面在第三方插件部分我会再提到。 menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th 读书: /books || book 电影: /movies || film archives: /archives/ || archive schedule: /schedule/ || calendarsitemap: /sitemap.xml || sitemapcommonweal: /404/ || heartbeat主题风格 schemes主题提供了4个，我们把想要选择的取消注释，其他三个保持注释掉的状态即可。 Muse image-20180809164700600Mist image-20180809164749052Pisces image-20180809164925685Gemini image-20180809165023401选择主题后也可以自定义，不过我还没摸清楚有哪些地方可以自定义，等弄清楚了我再来更新。 底部建站时间和图标修改修改主题的配置文件： footer: Specify the date when the site was setup.If not defined, current year will be used. since: 2018 Icon between year and copyright info. icon: snowflake-o If not defined, will be used author from Hexo main config. copyright: ————————————————————-Hexo link (Powered by Hexo). powered: false theme: # Theme &amp; scheme info link (Theme - NexT.scheme). enable: false # Version info of NexT after scheme info (vX.X.X). # version: false 我在这部分做了这样几件事： 把用户的图标从小人user改成了雪花snowflake-ocopyright留空，显示成页面author即我的名字powered: false把hexo的授权图片取消了theme: enable:false 把主题的内容也取消了这样底部信息比较简单。 image-20180809172835606个人社交信息 social在social里我们可以自定义自己想要在个人信息部分展现的账号，同时给他们加上图标。 social: GitHub: https://github.com/XuQuan-nikkkki || github E-Mail: mailto:xuquan1225@hotmail.com || envelope #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook注意点： ||后面对应的名称是fontAwesome里图标的名称，如果我们选择的账号没有对应的图标（如豆瓣、知乎），我们可以在fontAwesome库里去选择自己喜欢的图标建议不要找太新的fontAwesome图标，主题关联的库版本没有那么新，很可能显示不了或者显示一个地球网站动画效果为了网站响应速度我们可以把网站的动画关掉 motion: enable: false但我觉得页面比较素，所以开了动画，选择了canvas-nest这一个，主题自带四种效果，可以选自己喜欢的。 motion: enable: true async: true Canvas-nestcanvas_nest: true three_wavesthree_waves: false canvas_linescanvas_lines: false canvas_spherecanvas_sphere: false评论系统NexT原生支持多说、Disqus、hypercomments等多种评论系统。我选择了Disqus。 方法也非常简单。直接去Disqus注册，注册完了在配置的时候会给你一个名为shortname的ID，将这个ID填在配置文件里即可。 Disqusdisqus: enable: true shortname: xuquan count: true统计文章字数和阅读时间post_wordcount: item_text: true wordcount: true # 文章字数 min2read: true # 阅读时间 totalcount: true # 总共字数 separated_meta: true统计阅读次数这里我用的是leancloud的服务，具体方法参考NexT上的教程,添加完之后效果如下： image-20180809175133462第三方插件Hexo-adminHexo-admin插件允许我们直接在本地页面上修改文章内容。 下载 npm i hexo-admin –save登录http://localhost:4000/admin即可看到我们所有的文章内容，并且在可视化界面中操作文章内容 Hexo-doubanhexo-douban插件可以在博客中添加豆瓣电影、读书和游戏页面，关联我们自己的账号。 下载 npm install hexo-douban –save配置 在hexo根目录下的config.yml文件中添加如下内容 douban: user: builtin: false book: title: ‘This is my book title’ quote: ‘This is my book quote’ movie: title: ‘This is my movie title’ quote: ‘This is my movie quote’ game: title: ‘This is my game title’ quote: ‘This is my game quote’ timeout: 10000title和quote后面的内容会分别作为电影/读书/游戏页面的标题和副标题（引言）呈现在博客里。 user就写我们豆瓣的id，可以在“我的豆瓣”页面中找到，builtin指是否将生成页面功能嵌入hexo s和hexo g中，建议选false，因为true会导致页面每次启动本地服务器都需要很长时间生成豆瓣页面，长到怀疑人生。 生成页面 hexo douban #生成读书、电影、游戏三个页面hexo douban -b #生成读书页面hexo douban -m #生成电影页面hexo douban -g #生成游戏页面在博客中生成页面 这里就需要用到我们前面提过的hexo new命令了。 hexo new page bookshexo new page movieshexo new page games在博客中添加页面 在menu部分添加我们需要添加的页面名称和相对路径 menu: Home: / Archives: /archives Books: /books #This is your books page Movies: /movies #This is your movies page Games: /games #This is your games page部署到博客 hexo g &amp;&amp; hexo deploy我踩过的坑iPic图片上传hexo博客发布Typora写好的内容也会出现图片无法同步的问题，网上有大佬给出的解决方案是使用hexo-asset-image插件，这样在创建博客时会有一个与.md文件同名的文件夹，将图片同步到文件夹内即可。 但时间下来还是比较麻烦，因为Typora并没有自定义图片路径的功能，它会放在与文件相关的asset文件夹内。 我找到的最终方案是使用Typora自带的一个功能：图片上传iPic图床。这样在添加图片的时候，图片链接就自动更换成了图床的地址，这时同步到博客就没有问题了。 评论系统因为多说已经停止服务了，最开始看到有人说Disqus得翻墙，就选了一个韩国的评论服务，叫来必力，但事实证明墙外就没有稳定的服务，在我挂VPN的情况下也要加载好半天，后来就还是换成了Disqus，具体配置方法看前文。]]></content>
  </entry>
</search>
